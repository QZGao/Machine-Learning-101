{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Unsupervised Learning: A Deep Dive into K-Means Clustering\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the difference between supervised and unsupervised learning.\n",
    "- Grasp the theory and intuition behind the K-Means clustering algorithm.\n",
    "- Implement K-Means from preprocessing to prediction using Scikit-learn.\n",
    "- Learn how to choose the optimal number of clusters (K) using the Elbow Method.\n",
    "- Apply K-Means to a real-world problem: image compression.\n",
    "- Recognize the strengths and limitations of the K-Means algorithm.\n",
    "\n",
    "**Prerequisites:** Python basics, NumPy fundamentals, Pandas for data handling, and Matplotlib for visualization.\n",
    "\n",
    "**Estimated Time:** ~60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the fascinating world of unsupervised learning! Unlike supervised learning, where we have labeled data to guide our model, unsupervised learning is about finding hidden patterns and structures in data all on its own. It's a form of machine learning that acts like a detective, uncovering insights without any prior knowledge of the outcomes.\n",
    "\n",
    "One of the most fundamental and widely used unsupervised algorithms is **K-Means Clustering**. This notebook will provide a comprehensive, professor-led tour of K-Means, from its theoretical underpinnings to its practical applications.\n",
    "\n",
    "**ðŸŽ¯ Success Indicators:** By the end of this notebook, you will be able to:\n",
    "- Explain the K-Means algorithm's objective and iterative process.\n",
    "- Use Scikit-learn to train a K-Means model and interpret its output.\n",
    "- Apply the Elbow Method to make a data-driven decision for choosing 'K'.\n",
    "- Build a complete pipeline to compress an image by reducing its color palette.\n",
    "\n",
    "**ðŸ’¡ Professor's Tips for Beginners:**\n",
    "- **Visualize Everything:** Clustering is a visual task. Plot your data before, during, and after to build strong intuition.\n",
    "- **Understand the 'Why':** Don't just run the code. Focus on *why* each step, like feature scaling, is critical for distance-based algorithms like K-Means.\n",
    "- **K is Key:** The choice of 'K' is the most important hyperparameter you'll set. The Elbow Method is a great starting point, but the best 'K' often depends on the specific problem you're trying to solve.\n",
    "\n",
    "Let's begin by setting up our environment."
   ],
   "id": "6870e07476f85e95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Essential imports for data manipulation, clustering, and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn modules for K-Means and dataset creation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set a consistent style for our plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set a random seed for reproducibility of our results\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ðŸš€ Libraries loaded and environment set up successfully!\")"
   ],
   "id": "ee9fc1668c5ca2ee"
  },
  {
   "cell_type": "markdown",
   "id": "65d6d4c8",
   "metadata": {},
   "source": [
    "## 1. What is Clustering?\n",
    "\n",
    "**Clustering** is the task of grouping a set of objects in such a way that objects in the same group (called a **cluster**) are more similar to each other than to those in other groups. Think of it as automatic categorization.\n",
    "\n",
    "It's a core task in **unsupervised learning** because we don't provide the algorithm with any labels. We simply give it the data and ask, \"What are the natural groups here?\"\n",
    "\n",
    "**Real-world Applications:**\n",
    "- **Customer Segmentation**: Grouping customers by purchasing behavior for targeted marketing campaigns.\n",
    "- **Image Segmentation**: Grouping pixels to identify objects in an image.\n",
    "- **Anomaly Detection**: Identifying outliers that don't belong to any group (e.g., fraud detection).\n",
    "- **Genomics**: Clustering genes with similar expression patterns to identify their functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eace0ea",
   "metadata": {},
   "source": [
    "## 2. The K-Means Algorithm: Theory and Intuition\n",
    "\n",
    "K-Means is a **centroid-based clustering** algorithm. This means it works by identifying cluster centers (centroids) and assigning each data point to the nearest center. The 'K' in K-Means simply refers to the **number of clusters** you want to find, which you have to specify in advance.\n",
    "\n",
    "### The Goal: Minimizing Inertia\n",
    "\n",
    "The core objective of K-Means is to minimize the **within-cluster sum of squares (WCSS)**, also known as **inertia** or **distortion**. This is the sum of the squared distances between each data point and its assigned cluster's centroid.\n",
    "\n",
    "The objective function, $J$, is defined as:\n",
    "$$ J = \\\\sum_{i=1}^{m} \\\\sum_{k=1}^{K} w_{ik} ||x^{(i)} - \\\\mu_k||^2 $$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of data points.\n",
    "- $K$ is the number of clusters.\n",
    "- $x^{(i)}$ is the $i$-th data point.\n",
    "- $\\\\mu_k$ is the centroid of the $k$-th cluster.\n",
    "- $w_{ik}$ is 1 if point $x^{(i)}$ is in cluster $k$, and 0 otherwise.\n",
    "\n",
    "In simple terms, K-Means tries to make the clusters as tight and dense as possible.\n",
    "\n",
    "### The Iterative Process\n",
    "\n",
    "K-Means finds the best centroids through a simple, iterative two-step process:\n",
    "\n",
    "1.  **Assignment Step**: Assign each data point $x^{(i)}$ to its *closest* centroid $\\mu_k$. \"Closest\" is measured by the squared Euclidean distance.\n",
    "2.  **Update Step**: Recalculate the centroid of each cluster by taking the *mean* of all data points assigned to it.\n",
    "\n",
    "This process is repeated until the cluster assignments no longer change, meaning the algorithm has **converged**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bde272f",
   "metadata": {},
   "source": [
    "## 3. K-Means in Practice with a Toy Dataset\n",
    "\n",
    "Theory is great, but let's see K-Means in action. We'll start with a simple 2D dataset that is easy to visualize. Scikit-learn's `make_blobs` function is perfect for this.\n",
    "\n",
    "### 3.1. Generating and Exploring the Dataset\n",
    "\n",
    "First, let's create our data. We'll generate 300 points grouped into 4 distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 2D data with 4 clusters\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=300, \n",
    "    centers=4, \n",
    "    cluster_std=0.80, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Shape of our feature matrix (X): {X.shape}\")\n",
    "print(f\"First 5 data points:\\n{X[:5]}\")\n",
    "\n",
    "# Visualize the generated data\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50, alpha=0.7)\n",
    "plt.title('Generated 2D Data for Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01418381",
   "metadata": {},
   "source": [
    "As we can see, the data is clearly grouped, making it a perfect candidate for K-Means.\n",
    "\n",
    "### 3.2. Applying the Scikit-learn K-Means Model\n",
    "\n",
    "Scikit-learn provides a consistent and easy-to-use API for all its models. The pattern is always:\n",
    "1. **Instantiate** the model, setting its hyperparameters.\n",
    "2. **Fit** the model to the data.\n",
    "3. **Predict** or inspect the results.\n",
    "\n",
    "Let's apply this to our data. We already know the ground truth is 4 clusters, so we'll set `n_clusters=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75293627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the KMeans model\n",
    "# PARAMETER EXPLANATION:\n",
    "# â€¢ n_clusters: The number of clusters (K) to form. This is the most important hyperparameter.\n",
    "# â€¢ n_init: The number of times the algorithm will be run with different initial centroid seeds. \n",
    "#   The final result will be the best output in terms of inertia. 'auto' is a good default.\n",
    "# â€¢ random_state: Ensures that the initialization of centroids is reproducible.\n",
    "kmeans = KMeans(n_clusters=4, n_init='auto', random_state=42)\n",
    "\n",
    "# 2. Fit the model to the data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 3. Get the cluster assignments and centroids\n",
    "y_kmeans = kmeans.predict(X) # Get cluster label for each point\n",
    "cluster_centers = kmeans.cluster_centers_ # Get the coordinates of the centroids\n",
    "inertia = kmeans.inertia_ # Get the final WCSS value\n",
    "\n",
    "print(f\"Cluster labels for the first 10 points: {y_kmeans[:10]}\")\n",
    "print(f\"Coordinates of the 4 cluster centers:\\n{cluster_centers}\")\n",
    "print(f\"Final Inertia (WCSS): {inertia:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53158a00",
   "metadata": {},
   "source": [
    "### 3.3. Visualizing the Clustering Results\n",
    "\n",
    "Now that the model has assigned each point to a cluster, let's visualize the result. We'll create a scatter plot where each point is colored according to its assigned cluster label, and we'll mark the final centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points, colored by their assigned cluster\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.7)\n",
    "\n",
    "# Plot the final centroids as red 'X' marks\n",
    "plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', s=200, marker='X', label='Centroids')\n",
    "\n",
    "plt.title('K-Means Clustering Results (K=4)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a46a9e",
   "metadata": {},
   "source": [
    "Success! The K-Means algorithm successfully identified the four distinct groups in our data.\n",
    "\n",
    "## 4. How to Choose the Right Number of Clusters (K)\n",
    "\n",
    "In our toy example, we knew to set `K=4`. But in the real world, you rarely know the optimal number of clusters beforehand. This is one of the main challenges of K-Means.\n",
    "\n",
    "A popular and intuitive technique to help find a good value for K is the **Elbow Method**.\n",
    "\n",
    "**The Elbow Method Logic:**\n",
    "1. Run K-Means for a range of different K values (e.g., K from 1 to 10).\n",
    "2. For each K, calculate the **inertia (WCSS)**.\n",
    "3. Plot K against the inertia.\n",
    "4. Look for an \"elbow\" in the plot. This is the point where the rate of decrease in inertia sharply slows down. This point suggests the last effective number of clusters.\n",
    "\n",
    "The intuition is that adding more clusters beyond the elbow point yields diminishing returns and is essentially just subdividing already tight clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inertia for a range of K values\n",
    "inertia_values = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method graph\n",
    "plt.plot(k_range, inertia_values, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
    "plt.title('The Elbow Method for Optimal K')\n",
    "plt.xticks(k_range)\n",
    "plt.annotate('Elbow Point', xy=(4, inertia_values[3]), xytext=(5, 1000), \n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1250e7e",
   "metadata": {},
   "source": [
    "The plot clearly shows an \"elbow\" at **K=4**. For K>4, the drop in inertia becomes much less significant. This confirms that 4 is a good choice for our dataset.\n",
    "\n",
    "## 5. Real-World Application: Image Compression\n",
    "\n",
    "Now, let's apply K-Means to a more complex and practical problem: **image compression**.\n",
    "\n",
    "A standard digital image (like a PNG) represents each pixel's color using RGB (Red, Green, Blue) values. Each of these values is an 8-bit integer from 0 to 255. This means a single image can contain thousands or even millions of distinct colors.\n",
    "\n",
    "We can use K-Means to reduce the number of colors in an image to a smaller palette of 'K' representative colors. This is a form of **quantization**.\n",
    "\n",
    "**How it works:**\n",
    "1.  **Treat each pixel as a data point** in a 3-dimensional (R, G, B) space.\n",
    "2.  Run K-Means with a chosen `K` (e.g., 16) to find the `K` most common or representative colors. These `K` colors will be our new palette (the cluster centroids).\n",
    "3.  **Re-color the image** by replacing each pixel's original color with the centroid color of the cluster it was assigned to.\n",
    "\n",
    "This compresses the image because instead of storing the full 24-bit RGB value for every pixel, we only need to store the small palette of K colors and a tiny index (e.g., 4 bits for 16 colors) for each pixel.\n",
    "\n",
    "### 5.1. Load and Prepare the Image Data\n",
    "We'll use a sample image of a bird. First, we load it and inspect its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "# Note: The 'bird_small.png' image should be in the same directory as this notebook.\n",
    "try:\n",
    "    original_img = plt.imread('bird_small.png')\n",
    "except FileNotFoundError:\n",
    "    # If the file is not found, we can use a placeholder from an online source\n",
    "    print(\"Local 'bird_small.png' not found. Using a placeholder image.\")\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "    url = 'https://raw.githubusercontent.com/dibgerge/ml-coursera-python-assignments/master/Exercise7/data/bird_small.png'\n",
    "    response = requests.get(url)\n",
    "    img_pil = Image.open(BytesIO(response.content))\n",
    "    original_img = np.array(img_pil)\n",
    "\n",
    "# Display the original image\n",
    "plt.imshow(original_img)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Inspect the image data\n",
    "print(f\"Original image shape: {original_img.shape}\")\n",
    "print(f\"Data type: {original_img.dtype}\")\n",
    "print(f\"Color value range: {original_img.min()} to {original_img.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd24fa7",
   "metadata": {},
   "source": [
    "The image is 128x128 pixels, and each pixel has 3 values (R, G, and B). For K-Means to work, we need a 2D array where each row is a data point and each column is a feature. In our case, a data point is a pixel and the features are its R, G, and B values.\n",
    "\n",
    "We will now:\n",
    "1.  **Normalize** the data by dividing by 255, so all values are between 0 and 1. This is good practice for many ML algorithms.\n",
    "2.  **Reshape** the `(128, 128, 3)` array into a `(16384, 3)` array, where 16384 = 128 * 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f23e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "img_normalized = original_img / 255.0\n",
    "\n",
    "# Reshape the image into a 2D array of pixels\n",
    "pixel_data = img_normalized.reshape(-1, 3)\n",
    "\n",
    "print(f\"Shape of the reshaped pixel data: {pixel_data.shape}\")\n",
    "print(f\"Number of pixels (data points): {pixel_data.shape[0]}\")\n",
    "print(f\"Number of features (R, G, B): {pixel_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d95b1",
   "metadata": {},
   "source": [
    "### 5.2. Run K-Means to Find the Dominant Colors\n",
    "\n",
    "Now we can run K-Means on our `pixel_data`. We'll choose `K=16` to create a 16-color palette. The `cluster_centers_` found by the algorithm will be our new colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc8b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of colors for our new palette\n",
    "K_colors = 16\n",
    "\n",
    "print(f\"Running K-Means to find the {K_colors} most representative colors...\")\n",
    "\n",
    "# Instantiate and fit the KMeans model\n",
    "kmeans_img = KMeans(n_clusters=K_colors, n_init='auto', random_state=42)\n",
    "kmeans_img.fit(pixel_data)\n",
    "\n",
    "# The new color palette is the set of cluster centroids\n",
    "new_palette = kmeans_img.cluster_centers_\n",
    "\n",
    "# The labels tell us which palette color each original pixel belongs to\n",
    "pixel_labels = kmeans_img.labels_\n",
    "\n",
    "print(f\"Successfully found a new {K_colors}-color palette.\")\n",
    "print(f\"Shape of the new palette: {new_palette.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8fd63",
   "metadata": {},
   "source": [
    "### 5.3. Reconstruct the Image and Compare\n",
    "\n",
    "The final step is to create the compressed image. We can do this easily by creating a new array of colors based on the labels assigned by K-Means. `new_palette[pixel_labels]` will map each pixel's label back to its assigned centroid color.\n",
    "\n",
    "Then, we reshape this array back to the original image dimensions `(128, 128, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the image using the new palette and pixel labels\n",
    "compressed_pixel_data = new_palette[pixel_labels]\n",
    "\n",
    "# Reshape the data back to the original image dimensions\n",
    "compressed_img = compressed_pixel_data.reshape(original_img.shape)\n",
    "\n",
    "# Display the original and compressed images side-by-side\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].imshow(original_img)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(compressed_img)\n",
    "ax[1].set_title(f'Compressed Image ({K_colors} Colors)')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸŽ¯ Notice how the compressed image retains the main features while using only 16 colors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f9d72",
   "metadata": {},
   "source": [
    "## 6. K-Means: Strengths and Weaknesses\n",
    "\n",
    "K-Means is a powerful and intuitive algorithm, but it's important to understand its limitations.\n",
    "\n",
    "**Strengths of K-Means:**\n",
    "- **Simplicity and Speed**: It's easy to understand and computationally efficient, making it great for large datasets.\n",
    "- **Scalability**: It scales well to a large number of samples.\n",
    "- **Guaranteed Convergence**: The algorithm is guaranteed to converge to a solution (though it might be a local minimum).\n",
    "\n",
    "**Weaknesses of K-Means:**\n",
    "- **Need to Specify K**: The biggest drawback is having to choose the number of clusters, K, in advance.\n",
    "- **Sensitivity to Initialization**: The final clusters can depend on the initial placement of centroids. Scikit-learn mitigates this by running the algorithm multiple times with different initializations (`n_init`).\n",
    "- **Assumes Spherical Clusters**: K-Means works best when clusters are spherical and roughly equal in size. It struggles with clusters of irregular shapes or varying densities.\n",
    "- **Sensitivity to Outliers**: Since centroids are based on the mean, outliers can significantly pull them off-center.\n",
    "\n",
    "Here's a visual example where K-Means would fail to identify non-convex clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moon-shaped data where K-Means struggles\n",
    "from sklearn.datasets import make_moons\n",
    "X_moons, y_moons = make_moons(200, noise=.05, random_state=42)\n",
    "\n",
    "# Apply K-Means with K=2\n",
    "kmeans_moons = KMeans(n_clusters=2, n_init='auto', random_state=42)\n",
    "labels_moons = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "# Visualize the incorrect clustering\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, s=50, cmap='viridis', alpha=0.7)\n",
    "plt.title('K-Means Failing on Non-Convex Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ For data like this, density-based algorithms like DBSCAN are a better choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5ef22",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully taken a deep dive into the K-Means clustering algorithm.\n",
    "\n",
    "**What We've Covered:**\n",
    "- âœ… The fundamentals of **unsupervised learning** and **clustering**.\n",
    "- âœ… The **theory and intuition** behind K-Means, including its objective function (inertia) and the assign-update iterative process.\n",
    "- âœ… A hands-on implementation of K-Means on a **toy dataset** using Scikit-learn.\n",
    "- âœ… A practical method for choosing the optimal number of clusters, K, using the **Elbow Method**.\n",
    "- âœ… A detailed, real-world application of K-Means for **image compression**.\n",
    "- âœ… A balanced look at the **strengths and weaknesses** of the algorithm.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- K-Means is a fast and effective algorithm for identifying spherical, evenly-sized clusters.\n",
    "- Choosing the right 'K' is a critical, and often subjective, part of the process.\n",
    "- For distance-based algorithms like K-Means, feature scaling is crucial.\n",
    "- Always be aware of the algorithm's limitations and consider alternatives (like DBSCAN) for complex cluster shapes.\n",
    "\n",
    "**Next Steps in Your Journey:**\n",
    "- **Explore other clustering algorithms**: Look into `DBSCAN` for density-based clustering and `Hierarchical Clustering` for creating nested clusters.\n",
    "- **Dimensionality Reduction**: Before clustering, you can often improve results by using techniques like `Principal Component Analysis (PCA)` to reduce the number of features.\n",
    "- **Cluster Evaluation Metrics**: Dive deeper into metrics like the `Silhouette Score` to evaluate the quality of your clusters quantitatively."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
