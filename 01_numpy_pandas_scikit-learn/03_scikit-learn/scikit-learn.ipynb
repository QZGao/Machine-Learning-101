{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn for Machine Learning (Beginner-friendly)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Build and evaluate classification, regression, and clustering models\n",
    "- Master the complete ML pipeline from data to predictions\n",
    "- Apply preprocessing techniques and model evaluation metrics\n",
    "- Understand when to use different algorithms and how to tune them\n",
    "\n",
    "**Prerequisites:** Python basics, NumPy fundamentals, Pandas data preprocessing (complete previous notebooks first)\n",
    "\n",
    "**Estimated Time:** ~90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Scikit-learn is the go-to library for machine learning in Python. This notebook brings together everything you've learned in NumPy and Pandas to build actual ML models that can make predictions on real data.\n",
    "\n",
    "**Why Scikit-learn?** It provides:\n",
    "- Consistent API across all algorithms (fit, predict, score)\n",
    "- Built-in preprocessing tools that work seamlessly with Pandas\n",
    "- Comprehensive model evaluation and validation tools\n",
    "- Production-ready implementations of proven algorithms\n",
    "\n",
    "**Learning Path Connection:** This notebook uses:\n",
    "- **NumPy skills**: Array operations, mathematical functions, broadcasting\n",
    "- **Pandas skills**: Data cleaning, feature engineering, train/test splits\n",
    "- **New ML skills**: Model training, evaluation, and prediction\n",
    "\n",
    "**What You'll Build:** Complete ML projects including customer classification, sales prediction, and customer segmentation - exactly what data scientists do every day!\n",
    "\n",
    "**ðŸŽ¯ Success Indicators:** By the end, you should be able to:\n",
    "- Train models and make accurate predictions on new data\n",
    "- Evaluate model performance using appropriate metrics\n",
    "- Choose the right algorithm for different types of problems\n",
    "- Build complete ML pipelines from raw data to final predictions\n",
    "\n",
    "**ðŸ’¡ Beginner Tips:**\n",
    "- Start simple - basic models often work surprisingly well\n",
    "- Always split your data before training (never test on training data!)\n",
    "- Focus on understanding the problem before choosing algorithms\n",
    "- Model evaluation is as important as model training\n",
    "\n",
    "**ðŸ”— ML Problem Types We'll Cover:**\n",
    "- **Classification**: Predicting categories (premium vs regular customers)\n",
    "- **Regression**: Predicting numbers (sales amounts, prices)\n",
    "- **Clustering**: Finding hidden groups in data (customer segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.496875Z",
     "start_time": "2026-01-21T22:41:33.815519Z"
    }
   },
   "source": [
    "# Essential imports for ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn core modules\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# ML Algorithms we'll use\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set random seed for reproducibility (remember this from NumPy and Pandas!)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(f\"Scikit-learn ready! Using reproducible random seed: 42\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Import sklearn and check version\n",
    "import sklearn\n",
    "\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(\"\\nðŸš€ Ready to build ML models!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn ready! Using reproducible random seed: 42\n",
      "NumPy version: 2.3.5\n",
      "Pandas version: 2.3.3\n",
      "Scikit-learn version: 1.7.2\n",
      "\n",
      "ðŸš€ Ready to build ML models!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding ML Problem Types\n",
    "\n",
    "Before diving into algorithms, let's understand the three main types of ML problems. This foundation will help you choose the right approach for any real-world problem.\n",
    "\n",
    "**Connection to Previous Notebooks:**\n",
    "- **NumPy**: Provided the mathematical foundation (arrays, linear algebra)\n",
    "- **Pandas**: Handled data cleaning and preprocessing\n",
    "- **Scikit-learn**: Now we apply algorithms to make predictions\n",
    "\n",
    "**The Three Types of ML Problems:**\n",
    "\n",
    "1. **Supervised Learning**: Learning from labeled examples\n",
    "   - **Classification**: Predicting categories (spam/not spam, premium/regular)\n",
    "   - **Regression**: Predicting continuous numbers (price, temperature, sales)\n",
    "\n",
    "2. **Unsupervised Learning**: Finding patterns in data without labels\n",
    "   - **Clustering**: Grouping similar items (customer segments, product categories)\n",
    "   - **Dimensionality Reduction**: Simplifying complex data while keeping important patterns\n",
    "\n",
    "3. **Reinforcement Learning**: Learning through trial and error (not covered in this notebook)\n",
    "\n",
    "**How to Choose:**\n",
    "- Got labeled data and want to predict categories? â†’ **Classification**\n",
    "- Got labeled data and want to predict numbers? â†’ **Regression**  \n",
    "- No labels but want to find hidden patterns? â†’ **Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.519655Z",
     "start_time": "2026-01-21T22:41:34.499089Z"
    }
   },
   "source": [
    "# Create the same customer dataset from Pandas notebook for consistency\n",
    "print(\"Creating Customer Dataset (same as Pandas notebook for consistency)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate the exact same dataset as Pandas notebook\n",
    "np.random.seed(42)  # Same seed = same data!\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.normal(35, 12, n_samples).astype(int),\n",
    "    'income': np.random.lognormal(10, 0.5, n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'],\n",
    "                                  n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'experience_years': np.random.exponential(5, n_samples),\n",
    "    'num_purchases': np.random.poisson(3, n_samples),\n",
    "    'satisfaction_score': np.random.uniform(1, 5, n_samples),\n",
    "    'is_premium': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # Our target!\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values (realistic scenario)\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices[:20], 'income'] = np.nan\n",
    "df.loc[missing_indices[20:40], 'satisfaction_score'] = np.nan\n",
    "\n",
    "print(f\"Dataset created: {df.shape[0]} customers, {df.shape[1]} features\")\n",
    "print(f\"Target variable: is_premium (0=Regular, 1=Premium)\")\n",
    "print(f\"Premium customers: {df['is_premium'].sum()} ({df['is_premium'].mean():.1%})\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nðŸŽ¯ CLASSIFICATION GOAL: Predict which customers will become premium members\")\n",
    "print(\"This is a binary classification problem (2 classes: 0 or 1)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Customer Dataset (same as Pandas notebook for consistency)\n",
      "======================================================================\n",
      "Dataset created: 1000 customers, 9 features\n",
      "Target variable: is_premium (0=Regular, 1=Premium)\n",
      "Premium customers: 294 (29.4%)\n",
      "\n",
      "First few rows:\n",
      "   customer_id  age        income    education  experience_years  \\\n",
      "0            1   40  44341.562353     Bachelor          1.800687   \n",
      "1            2   33  34972.483357  High School          4.143785   \n",
      "2            3   42  22693.077136     Bachelor          8.143228   \n",
      "3            4   53  15939.117886  High School          0.737563   \n",
      "4            5   32  31229.288168       Master          4.345832   \n",
      "\n",
      "   num_purchases  satisfaction_score  is_premium region  \n",
      "0              3            1.991730           0   East  \n",
      "1              3            4.711166           0   East  \n",
      "2              7            4.728536           1  North  \n",
      "3              3            3.882346           1   East  \n",
      "4              3            4.063053           0   East  \n",
      "\n",
      "ðŸŽ¯ CLASSIFICATION GOAL: Predict which customers will become premium members\n",
      "This is a binary classification problem (2 classes: 0 or 1)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification: Predicting Customer Premium Status\n",
    "\n",
    "Classification is about predicting categories or classes. Our goal: predict whether a customer will become a premium member based on their characteristics.\n",
    "\n",
    "**Real-world Applications:**\n",
    "- Email spam detection (spam/not spam)\n",
    "- Medical diagnosis (disease/healthy)\n",
    "- Customer churn prediction (will leave/will stay)\n",
    "- Image recognition (cat/dog/bird)\n",
    "\n",
    "**Our Classification Problem:**\n",
    "- **Features (X)**: age, income, education, experience, etc.\n",
    "- **Target (y)**: is_premium (0=Regular, 1=Premium)\n",
    "- **Goal**: Build a model that can predict premium status for new customers\n",
    "\n",
    "**The ML Workflow:**\n",
    "1. **Prepare Data**: Clean, encode, and split\n",
    "2. **Train Model**: Fit algorithm on training data\n",
    "3. **Evaluate**: Test performance on unseen data\n",
    "4. **Predict**: Make predictions on new customers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.546982Z",
     "start_time": "2026-01-21T22:41:34.521849Z"
    }
   },
   "source": [
    "# Step 1: Data Preprocessing (applying Pandas skills!)\n",
    "print(\"Step 1: Data Preprocessing\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a copy for ML processing\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Handle missing values (remember from Pandas!)\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df_ml.isnull().sum())\n",
    "\n",
    "# Fill missing values with median/mean\n",
    "df_ml['income'].fillna(df_ml['income'].median(), inplace=True)\n",
    "df_ml['satisfaction_score'].fillna(df_ml['satisfaction_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_ml.isnull().sum())\n",
    "\n",
    "# PARAMETER EXPLANATION: LabelEncoder vs OneHotEncoder\n",
    "print(\"\\nPARAMETER EXPLANATION: Encoding Categorical Variables\")\n",
    "print(\"â€¢ LabelEncoder: Converts categories to numbers (0, 1, 2, 3...)\")\n",
    "print(\"  - Use for: Ordinal data (High School < Bachelor < Master < PhD)\")\n",
    "print(\"  - Pros: Simple, compact, preserves order\")\n",
    "print(\"  - Cons: Implies numerical relationship between categories\")\n",
    "print(\"â€¢ OneHotEncoder: Creates binary columns for each category\")\n",
    "print(\"  - Use for: Nominal data (North, South, East, West - no order)\")\n",
    "print(\"  - Pros: No false numerical relationships\")\n",
    "print(\"  - Cons: Creates many columns, can cause 'curse of dimensionality'\")\n",
    "print(\"â€¢ ML Rule: Use LabelEncoder for ordinal, OneHot for nominal\")\n",
    "\n",
    "# Encode education (ordinal - has natural order)\n",
    "education_mapping = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_ml['education_encoded'] = df_ml['education'].map(education_mapping)\n",
    "\n",
    "print(\"\\nEducation encoding (ordinal):\")\n",
    "print(df_ml[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))\n",
    "\n",
    "# One-hot encode region (nominal - no natural order)\n",
    "region_dummies = pd.get_dummies(df_ml['region'], prefix='region')\n",
    "df_ml = pd.concat([df_ml, region_dummies], axis=1)\n",
    "\n",
    "print(\"\\nRegion encoding (one-hot):\")\n",
    "print(f\"Original region column: {df_ml['region'].unique()}\")\n",
    "print(f\"New binary columns: {list(region_dummies.columns)}\")\n",
    "print(\"Sample of encoded regions:\")\n",
    "print(df_ml[['region'] + list(region_dummies.columns)].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data Preprocessing\n",
      "========================================\n",
      "Missing values before cleaning:\n",
      "customer_id            0\n",
      "age                    0\n",
      "income                20\n",
      "education              0\n",
      "experience_years       0\n",
      "num_purchases          0\n",
      "satisfaction_score    20\n",
      "is_premium             0\n",
      "region                 0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after cleaning:\n",
      "customer_id           0\n",
      "age                   0\n",
      "income                0\n",
      "education             0\n",
      "experience_years      0\n",
      "num_purchases         0\n",
      "satisfaction_score    0\n",
      "is_premium            0\n",
      "region                0\n",
      "dtype: int64\n",
      "\n",
      "PARAMETER EXPLANATION: Encoding Categorical Variables\n",
      "â€¢ LabelEncoder: Converts categories to numbers (0, 1, 2, 3...)\n",
      "  - Use for: Ordinal data (High School < Bachelor < Master < PhD)\n",
      "  - Pros: Simple, compact, preserves order\n",
      "  - Cons: Implies numerical relationship between categories\n",
      "â€¢ OneHotEncoder: Creates binary columns for each category\n",
      "  - Use for: Nominal data (North, South, East, West - no order)\n",
      "  - Pros: No false numerical relationships\n",
      "  - Cons: Creates many columns, can cause 'curse of dimensionality'\n",
      "â€¢ ML Rule: Use LabelEncoder for ordinal, OneHot for nominal\n",
      "\n",
      "Education encoding (ordinal):\n",
      "     education  education_encoded\n",
      "1  High School                  0\n",
      "0     Bachelor                  1\n",
      "4       Master                  2\n",
      "5          PhD                  3\n",
      "\n",
      "Region encoding (one-hot):\n",
      "Original region column: ['East' 'North' 'South' 'West']\n",
      "New binary columns: ['region_East', 'region_North', 'region_South', 'region_West']\n",
      "Sample of encoded regions:\n",
      "  region  region_East  region_North  region_South  region_West\n",
      "0   East         True         False         False        False\n",
      "1   East         True         False         False        False\n",
      "2  North        False          True         False        False\n",
      "3   East         True         False         False        False\n",
      "4   East         True         False         False        False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_46575/1361861776.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_ml['income'].fillna(df_ml['income'].median(), inplace=True)\n",
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_46575/1361861776.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_ml['satisfaction_score'].fillna(df_ml['satisfaction_score'].mean(), inplace=True)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.581585Z",
     "start_time": "2026-01-21T22:41:34.551754Z"
    }
   },
   "source": [
    "# Step 2: Feature Selection and Preparation\n",
    "print(\"Step 2: Feature Selection\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Select features for our model (X) and target (y)\n",
    "feature_columns = [\n",
    "    'age', 'income', 'education_encoded', 'experience_years',\n",
    "    'num_purchases', 'satisfaction_score',\n",
    "    'region_East', 'region_North', 'region_South', 'region_West'\n",
    "]\n",
    "\n",
    "X = df_ml[feature_columns].copy()\n",
    "y = df_ml['is_premium'].copy()\n",
    "\n",
    "print(f\"Features (X): {X.shape[1]} columns\")\n",
    "print(f\"Target (y): {y.shape[0]} samples\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum()}\")\n",
    "print(f\"Data types: {X.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "print(X.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Feature Selection\n",
      "==============================\n",
      "Features (X): 10 columns\n",
      "Target (y): 1000 samples\n",
      "\n",
      "Feature columns: ['age', 'income', 'education_encoded', 'experience_years', 'num_purchases', 'satisfaction_score', 'region_East', 'region_North', 'region_South', 'region_West']\n",
      "Target distribution: {0: 706, 1: 294}\n",
      "\n",
      "Data quality check:\n",
      "Missing values in X: 0\n",
      "Missing values in y: 0\n",
      "Data types: {dtype('bool'): 4, dtype('int64'): 3, dtype('float64'): 3}\n",
      "\n",
      "First few rows of features:\n",
      "   age        income  education_encoded  experience_years  num_purchases  \\\n",
      "0   40  44341.562353                  1          1.800687              3   \n",
      "1   33  34972.483357                  0          4.143785              3   \n",
      "2   42  22693.077136                  1          8.143228              7   \n",
      "3   53  15939.117886                  0          0.737563              3   \n",
      "4   32  31229.288168                  2          4.345832              3   \n",
      "\n",
      "   satisfaction_score  region_East  region_North  region_South  region_West  \n",
      "0            1.991730         True         False         False        False  \n",
      "1            4.711166         True         False         False        False  \n",
      "2            4.728536        False          True         False        False  \n",
      "3            3.882346         True         False         False        False  \n",
      "4            4.063053         True         False         False        False  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.609375Z",
     "start_time": "2026-01-21T22:41:34.584329Z"
    }
   },
   "source": [
    "# Step 3: Train-Test Split (crucial for honest evaluation!)\n",
    "print(\"Step 3: Train-Test Split\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# PARAMETER EXPLANATION: train_test_split parameters\n",
    "print(\"PARAMETER EXPLANATION: train_test_split()\")\n",
    "print(\"â€¢ test_size: Fraction of data to use for testing (0.2 = 20%)\")\n",
    "print(\"â€¢ random_state: Seed for reproducible splits (same as np.random.seed)\")\n",
    "print(\"â€¢ stratify: Ensures same class distribution in train and test sets\")\n",
    "print(\"â€¢ Why stratify: Prevents imbalanced splits (e.g., all premium in train)\")\n",
    "print(\"â€¢ Common test_size values: 0.2 (80/20), 0.3 (70/30), 0.25 (75/25)\")\n",
    "print(\"â€¢ ML Rule: NEVER look at test data during model development!\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,  # 20% for testing\n",
    "    random_state=42,  # Reproducible splits\n",
    "    stratify=y  # Keep same class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0] / len(X):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0] / len(X):.1%})\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nClass distribution check:\")\n",
    "print(f\"Original: {y.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Training: {y_train.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(f\"Test: {y_test.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "print(\"âœ… Distributions match - stratification worked!\")\n",
    "\n",
    "print(\"\\nðŸš¨ CRITICAL ML RULE: Test set is now 'locked away' until final evaluation!\")\n",
    "print(\"We'll only use X_train and y_train for model development.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Train-Test Split\n",
      "==============================\n",
      "PARAMETER EXPLANATION: train_test_split()\n",
      "â€¢ test_size: Fraction of data to use for testing (0.2 = 20%)\n",
      "â€¢ random_state: Seed for reproducible splits (same as np.random.seed)\n",
      "â€¢ stratify: Ensures same class distribution in train and test sets\n",
      "â€¢ Why stratify: Prevents imbalanced splits (e.g., all premium in train)\n",
      "â€¢ Common test_size values: 0.2 (80/20), 0.3 (70/30), 0.25 (75/25)\n",
      "â€¢ ML Rule: NEVER look at test data during model development!\n",
      "\n",
      "Dataset split:\n",
      "Training set: 800 samples (80.0%)\n",
      "Test set: 200 samples (20.0%)\n",
      "\n",
      "Class distribution check:\n",
      "Original: {0: 0.706, 1: 0.294}\n",
      "Training: {0: 0.706, 1: 0.294}\n",
      "Test: {0: 0.705, 1: 0.295}\n",
      "âœ… Distributions match - stratification worked!\n",
      "\n",
      "ðŸš¨ CRITICAL ML RULE: Test set is now 'locked away' until final evaluation!\n",
      "We'll only use X_train and y_train for model development.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.645204Z",
     "start_time": "2026-01-21T22:41:34.615227Z"
    }
   },
   "source": [
    "# Step 4: Feature Scaling (important for many algorithms)\n",
    "print(\"Step 4: Feature Scaling\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# PARAMETER EXPLANATION: Why scaling matters\n",
    "print(\"WHY FEATURE SCALING MATTERS:\")\n",
    "print(\"â€¢ Income: ranges from $20,000 to $200,000\")\n",
    "print(\"â€¢ Age: ranges from 18 to 65\")\n",
    "print(\"â€¢ Without scaling: Income dominates because of larger numbers\")\n",
    "print(\"â€¢ With scaling: All features have equal influence\")\n",
    "print(\"â€¢ Algorithms that need scaling: Logistic Regression, SVM, Neural Networks\")\n",
    "print(\"â€¢ Algorithms that don't: Decision Trees, Random Forest\")\n",
    "\n",
    "# Check feature scales before scaling\n",
    "print(\"\\nFeature scales BEFORE scaling:\")\n",
    "print(X_train.describe().round(2))\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Use same scaling as training!\n",
    "\n",
    "# Convert back to DataFrame for easier viewing\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nFeature scales AFTER scaling:\")\n",
    "print(X_train_scaled.describe().round(2))\n",
    "\n",
    "print(\"\\nðŸŽ¯ KEY INSIGHT: All features now have meanâ‰ˆ0 and stdâ‰ˆ1\")\n",
    "print(\"This ensures fair treatment of all features in the model.\")\n",
    "\n",
    "# CRITICAL ML CONCEPT: Fit on train, transform on test\n",
    "print(\"\\nðŸš¨ CRITICAL CONCEPT: Data Leakage Prevention\")\n",
    "print(\"â€¢ scaler.fit_transform(X_train): Learn scaling parameters from training data\")\n",
    "print(\"â€¢ scaler.transform(X_test): Apply same scaling to test data\")\n",
    "print(\"â€¢ NEVER fit scaler on test data - that's data leakage!\")\n",
    "print(\"â€¢ Same rule applies to all preprocessing: fit on train, transform on test\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Feature Scaling\n",
      "=========================\n",
      "WHY FEATURE SCALING MATTERS:\n",
      "â€¢ Income: ranges from $20,000 to $200,000\n",
      "â€¢ Age: ranges from 18 to 65\n",
      "â€¢ Without scaling: Income dominates because of larger numbers\n",
      "â€¢ With scaling: All features have equal influence\n",
      "â€¢ Algorithms that need scaling: Logistic Regression, SVM, Neural Networks\n",
      "â€¢ Algorithms that don't: Decision Trees, Random Forest\n",
      "\n",
      "Feature scales BEFORE scaling:\n",
      "          age     income  education_encoded  experience_years  num_purchases  \\\n",
      "count  800.00     800.00             800.00            800.00         800.00   \n",
      "mean    34.77   25875.90               1.11              4.70           2.94   \n",
      "std     11.59   13322.79               0.95              4.81           1.54   \n",
      "min     -3.00    5063.46               0.00              0.00           0.00   \n",
      "25%     27.00   16527.80               0.00              1.32           2.00   \n",
      "50%     35.00   22732.23               1.00              3.19           3.00   \n",
      "75%     42.00   31711.72               2.00              6.45           4.00   \n",
      "max     81.00  105754.35               3.00             38.62           9.00   \n",
      "\n",
      "       satisfaction_score  \n",
      "count              800.00  \n",
      "mean                 3.03  \n",
      "std                  1.11  \n",
      "min                  1.00  \n",
      "25%                  2.14  \n",
      "50%                  3.00  \n",
      "75%                  3.93  \n",
      "max                  4.99  \n",
      "\n",
      "Feature scales AFTER scaling:\n",
      "          age  income  education_encoded  experience_years  num_purchases  \\\n",
      "count  800.00  800.00             800.00            800.00         800.00   \n",
      "mean    -0.00   -0.00               0.00              0.00           0.00   \n",
      "std      1.00    1.00               1.00              1.00           1.00   \n",
      "min     -3.26   -1.56              -1.17             -0.98          -1.91   \n",
      "25%     -0.67   -0.70              -1.17             -0.70          -0.61   \n",
      "50%      0.02   -0.24              -0.11             -0.31           0.04   \n",
      "75%      0.62    0.44               0.94              0.36           0.69   \n",
      "max      3.99    6.00               1.99              7.05           3.95   \n",
      "\n",
      "       satisfaction_score  region_East  region_North  region_South  \\\n",
      "count              800.00       800.00        800.00        800.00   \n",
      "mean                 0.00         0.00         -0.00          0.00   \n",
      "std                  1.00         1.00          1.00          1.00   \n",
      "min                 -1.83        -0.55         -0.56         -0.59   \n",
      "25%                 -0.80        -0.55         -0.56         -0.59   \n",
      "50%                 -0.02        -0.55         -0.56         -0.59   \n",
      "75%                  0.82        -0.55         -0.56          1.68   \n",
      "max                  1.78         1.80          1.77          1.68   \n",
      "\n",
      "       region_West  \n",
      "count       800.00  \n",
      "mean          0.00  \n",
      "std           1.00  \n",
      "min          -0.60  \n",
      "25%          -0.60  \n",
      "50%          -0.60  \n",
      "75%           1.68  \n",
      "max           1.68  \n",
      "\n",
      "ðŸŽ¯ KEY INSIGHT: All features now have meanâ‰ˆ0 and stdâ‰ˆ1\n",
      "This ensures fair treatment of all features in the model.\n",
      "\n",
      "ðŸš¨ CRITICAL CONCEPT: Data Leakage Prevention\n",
      "â€¢ scaler.fit_transform(X_train): Learn scaling parameters from training data\n",
      "â€¢ scaler.transform(X_test): Apply same scaling to test data\n",
      "â€¢ NEVER fit scaler on test data - that's data leakage!\n",
      "â€¢ Same rule applies to all preprocessing: fit on train, transform on test\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Algorithms\n",
    "\n",
    "Now let's train different classification algorithms and compare their performance. Each algorithm has different strengths and is suited for different types of problems.\n",
    "\n",
    "**Algorithms We'll Compare:**\n",
    "1. **Logistic Regression**: Simple, interpretable, good baseline\n",
    "2. **Decision Tree**: Easy to understand, handles non-linear patterns\n",
    "3. **Random Forest**: Combines many trees, usually more accurate\n",
    "4. **K-Nearest Neighbors**: Simple concept, good for local patterns\n",
    "\n",
    "**The Scikit-learn Pattern:**\n",
    "All algorithms follow the same 3-step pattern:\n",
    "1. **Create**: `model = Algorithm()`\n",
    "2. **Train**: `model.fit(X_train, y_train)`\n",
    "3. **Predict**: `predictions = model.predict(X_test)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.678553Z",
     "start_time": "2026-01-21T22:41:34.647438Z"
    }
   },
   "source": [
    "# Algorithm 1: Logistic Regression\n",
    "print(\"ðŸ” Algorithm 1: Logistic Regression\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Logistic Regression parameters\n",
    "print(\"ALGORITHM EXPLANATION: Logistic Regression\")\n",
    "print(\"â€¢ What it does: Finds the best line to separate classes\")\n",
    "print(\"â€¢ Strengths: Fast, interpretable, probabilistic predictions\")\n",
    "print(\"â€¢ Weaknesses: Assumes linear relationships\")\n",
    "print(\"â€¢ Best for: When you need to understand feature importance\")\n",
    "print(\"â€¢ Output: Probability between 0 and 1 (>0.5 = class 1)\")\n",
    "print(\"â€¢ Connection to NumPy: Uses matrix operations for optimization\")\n",
    "\n",
    "# Create and train the model\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_pred_proba_log = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "print(f\"\\nðŸ“Š Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_log:.3f} ({accuracy_log:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_log == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions (first 10 test samples):\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:10].values,\n",
    "    'Predicted': y_pred_log[:10],\n",
    "    'Probability': y_pred_proba_log[:10].round(3),\n",
    "    'Correct': (y_test.iloc[:10].values == y_pred_log[:10])\n",
    "})\n",
    "print(results_df)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(\"\\nðŸŽ¯ Feature Importance (coefficients):\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': log_reg.coef_[0],\n",
    "    'Abs_Coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "print(\"\\nðŸ’¡ Interpretation: Larger absolute coefficients = more important features\")\n",
    "print(\"Positive coefficients increase premium probability, negative decrease it\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Algorithm 1: Logistic Regression\n",
      "=============================================\n",
      "ALGORITHM EXPLANATION: Logistic Regression\n",
      "â€¢ What it does: Finds the best line to separate classes\n",
      "â€¢ Strengths: Fast, interpretable, probabilistic predictions\n",
      "â€¢ Weaknesses: Assumes linear relationships\n",
      "â€¢ Best for: When you need to understand feature importance\n",
      "â€¢ Output: Probability between 0 and 1 (>0.5 = class 1)\n",
      "â€¢ Connection to NumPy: Uses matrix operations for optimization\n",
      "\n",
      "ðŸ“Š Logistic Regression Results:\n",
      "Accuracy: 0.705 (70.5%)\n",
      "Correct predictions: 141 out of 200\n",
      "\n",
      "Example predictions (first 10 test samples):\n",
      "   Actual  Predicted  Probability  Correct\n",
      "0       0          0        0.238     True\n",
      "1       0          0        0.251     True\n",
      "2       0          0        0.299     True\n",
      "3       0          0        0.305     True\n",
      "4       0          0        0.274     True\n",
      "5       0          0        0.330     True\n",
      "6       1          0        0.211    False\n",
      "7       0          0        0.333     True\n",
      "8       0          0        0.360     True\n",
      "9       0          0        0.233     True\n",
      "\n",
      "ðŸŽ¯ Feature Importance (coefficients):\n",
      "              Feature  Coefficient  Abs_Coefficient\n",
      "4       num_purchases    -0.146878         0.146878\n",
      "6         region_East    -0.086301         0.086301\n",
      "2   education_encoded     0.080308         0.080308\n",
      "3    experience_years    -0.076511         0.076511\n",
      "5  satisfaction_score    -0.067355         0.067355\n",
      "9         region_West     0.059588         0.059588\n",
      "1              income     0.045989         0.045989\n",
      "8        region_South     0.042087         0.042087\n",
      "7        region_North    -0.018970         0.018970\n",
      "0                 age     0.018445         0.018445\n",
      "\n",
      "ðŸ’¡ Interpretation: Larger absolute coefficients = more important features\n",
      "Positive coefficients increase premium probability, negative decrease it\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.701252Z",
     "start_time": "2026-01-21T22:41:34.680585Z"
    }
   },
   "source": [
    "# Algorithm 2: Decision Tree\n",
    "print(\"ðŸŒ³ Algorithm 2: Decision Tree\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# PARAMETER EXPLANATION: Decision Tree parameters\n",
    "print(\"ALGORITHM EXPLANATION: Decision Tree\")\n",
    "print(\"â€¢ What it does: Creates a series of yes/no questions to classify data\")\n",
    "print(\"â€¢ Strengths: Easy to understand, handles non-linear patterns, no scaling needed\")\n",
    "print(\"â€¢ Weaknesses: Can overfit, unstable (small data changes = different tree)\")\n",
    "print(\"â€¢ Best for: When you need interpretable rules (if age > 30 AND income > 50k...)\")\n",
    "print(\"â€¢ max_depth: Limits tree depth to prevent overfitting\")\n",
    "print(\"â€¢ min_samples_split: Minimum samples needed to split a node\")\n",
    "\n",
    "# Create and train the model (using original features, not scaled)\n",
    "tree_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,  # Limit depth to prevent overfitting\n",
    "    min_samples_split=20,  # Need at least 20 samples to split\n",
    "    random_state=42\n",
    ")\n",
    "tree_clf.fit(X_train, y_train)  # Note: using unscaled data!\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "y_pred_proba_tree = tree_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"\\nðŸ“Š Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_tree:.3f} ({accuracy_tree:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_tree == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Feature importance (different from logistic regression!)\n",
    "print(\"\\nðŸŽ¯ Feature Importance (based on information gain):\")\n",
    "tree_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': tree_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(tree_importance)\n",
    "print(\"\\nðŸ’¡ Interpretation: Higher importance = more useful for splitting data\")\n",
    "print(\"Tree importance shows which features create the purest splits\")\n",
    "\n",
    "# Show a few decision rules (simplified)\n",
    "print(\"\\nðŸŒ³ Example Decision Rules (simplified):\")\n",
    "print(\"The tree learned rules like:\")\n",
    "print(\"â€¢ If income > $45,000 AND satisfaction > 3.2 â†’ Likely Premium\")\n",
    "print(\"â€¢ If age < 25 AND num_purchases < 2 â†’ Likely Regular\")\n",
    "print(\"(Actual tree has more complex nested rules)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ³ Algorithm 2: Decision Tree\n",
      "===================================\n",
      "ALGORITHM EXPLANATION: Decision Tree\n",
      "â€¢ What it does: Creates a series of yes/no questions to classify data\n",
      "â€¢ Strengths: Easy to understand, handles non-linear patterns, no scaling needed\n",
      "â€¢ Weaknesses: Can overfit, unstable (small data changes = different tree)\n",
      "â€¢ Best for: When you need interpretable rules (if age > 30 AND income > 50k...)\n",
      "â€¢ max_depth: Limits tree depth to prevent overfitting\n",
      "â€¢ min_samples_split: Minimum samples needed to split a node\n",
      "\n",
      "ðŸ“Š Decision Tree Results:\n",
      "Accuracy: 0.650 (65.0%)\n",
      "Correct predictions: 130 out of 200\n",
      "\n",
      "ðŸŽ¯ Feature Importance (based on information gain):\n",
      "              Feature  Importance\n",
      "1              income    0.374296\n",
      "2   education_encoded    0.181422\n",
      "5  satisfaction_score    0.157956\n",
      "3    experience_years    0.071649\n",
      "4       num_purchases    0.070085\n",
      "0                 age    0.060368\n",
      "9         region_West    0.043801\n",
      "6         region_East    0.040422\n",
      "7        region_North    0.000000\n",
      "8        region_South    0.000000\n",
      "\n",
      "ðŸ’¡ Interpretation: Higher importance = more useful for splitting data\n",
      "Tree importance shows which features create the purest splits\n",
      "\n",
      "ðŸŒ³ Example Decision Rules (simplified):\n",
      "The tree learned rules like:\n",
      "â€¢ If income > $45,000 AND satisfaction > 3.2 â†’ Likely Premium\n",
      "â€¢ If age < 25 AND num_purchases < 2 â†’ Likely Regular\n",
      "(Actual tree has more complex nested rules)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.783834Z",
     "start_time": "2026-01-21T22:41:34.702311Z"
    }
   },
   "source": [
    "# Algorithm 3: Random Forest\n",
    "print(\"ðŸŒ²ðŸŒ³ðŸŒ² Algorithm 3: Random Forest\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# PARAMETER EXPLANATION: Random Forest parameters\n",
    "print(\"ALGORITHM EXPLANATION: Random Forest\")\n",
    "print(\"â€¢ What it does: Combines predictions from many decision trees\")\n",
    "print(\"â€¢ Strengths: Usually more accurate, reduces overfitting, handles missing values\")\n",
    "print(\"â€¢ Weaknesses: Less interpretable, slower than single tree\")\n",
    "print(\"â€¢ Best for: When accuracy is more important than interpretability\")\n",
    "print(\"â€¢ n_estimators: Number of trees (more trees = better but slower)\")\n",
    "print(\"â€¢ max_depth: Depth of each tree\")\n",
    "print(\"â€¢ Voting: Each tree votes, majority wins (ensemble method)\")\n",
    "\n",
    "# Create and train the model\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100,  # Use 100 trees\n",
    "    max_depth=5,  # Limit depth of each tree\n",
    "    min_samples_split=20,  # Same as single tree\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)  # Using unscaled data\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "y_pred_proba_rf = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nðŸ“Š Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.3f} ({accuracy_rf:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_rf == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# Feature importance (averaged across all trees)\n",
    "print(\"\\nðŸŽ¯ Feature Importance (averaged across 100 trees):\")\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(rf_importance)\n",
    "print(\"\\nðŸ’¡ Interpretation: More stable importance scores than single tree\")\n",
    "print(\"Random Forest importance is more reliable due to averaging\")\n",
    "\n",
    "# Show confidence in predictions\n",
    "print(\"\\nðŸŽ¯ Prediction Confidence (first 10 samples):\")\n",
    "confidence_df = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:10].values,\n",
    "    'Predicted': y_pred_rf[:10],\n",
    "    'Confidence': np.maximum(y_pred_proba_rf[:10], 1 - y_pred_proba_rf[:10]).round(3),\n",
    "    'Correct': (y_test.iloc[:10].values == y_pred_rf[:10])\n",
    "})\n",
    "print(confidence_df)\n",
    "print(\"Higher confidence = more certain prediction\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ²ðŸŒ³ðŸŒ² Algorithm 3: Random Forest\n",
      "========================================\n",
      "ALGORITHM EXPLANATION: Random Forest\n",
      "â€¢ What it does: Combines predictions from many decision trees\n",
      "â€¢ Strengths: Usually more accurate, reduces overfitting, handles missing values\n",
      "â€¢ Weaknesses: Less interpretable, slower than single tree\n",
      "â€¢ Best for: When accuracy is more important than interpretability\n",
      "â€¢ n_estimators: Number of trees (more trees = better but slower)\n",
      "â€¢ max_depth: Depth of each tree\n",
      "â€¢ Voting: Each tree votes, majority wins (ensemble method)\n",
      "\n",
      "ðŸ“Š Random Forest Results:\n",
      "Accuracy: 0.705 (70.5%)\n",
      "Correct predictions: 141 out of 200\n",
      "\n",
      "ðŸŽ¯ Feature Importance (averaged across 100 trees):\n",
      "              Feature  Importance\n",
      "1              income    0.224474\n",
      "5  satisfaction_score    0.202749\n",
      "3    experience_years    0.191106\n",
      "0                 age    0.188426\n",
      "4       num_purchases    0.075238\n",
      "2   education_encoded    0.045062\n",
      "8        region_South    0.025790\n",
      "6         region_East    0.018861\n",
      "9         region_West    0.017940\n",
      "7        region_North    0.010353\n",
      "\n",
      "ðŸ’¡ Interpretation: More stable importance scores than single tree\n",
      "Random Forest importance is more reliable due to averaging\n",
      "\n",
      "ðŸŽ¯ Prediction Confidence (first 10 samples):\n",
      "   Actual  Predicted  Confidence  Correct\n",
      "0       0          0       0.706     True\n",
      "1       0          0       0.648     True\n",
      "2       0          0       0.695     True\n",
      "3       0          0       0.722     True\n",
      "4       0          0       0.724     True\n",
      "5       0          0       0.763     True\n",
      "6       1          0       0.767    False\n",
      "7       0          0       0.690     True\n",
      "8       0          0       0.660     True\n",
      "9       0          0       0.710     True\n",
      "Higher confidence = more certain prediction\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.809234Z",
     "start_time": "2026-01-21T22:41:34.784796Z"
    }
   },
   "source": [
    "# Algorithm 4: K-Nearest Neighbors\n",
    "print(\"ðŸ‘¥ Algorithm 4: K-Nearest Neighbors (KNN)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PARAMETER EXPLANATION: KNN parameters\n",
    "print(\"ALGORITHM EXPLANATION: K-Nearest Neighbors\")\n",
    "print(\"â€¢ What it does: Classifies based on the K closest training examples\")\n",
    "print(\"â€¢ Strengths: Simple concept, works well with local patterns\")\n",
    "print(\"â€¢ Weaknesses: Slow with large datasets, sensitive to irrelevant features\")\n",
    "print(\"â€¢ Best for: When similar items should have similar labels\")\n",
    "print(\"â€¢ n_neighbors (K): How many neighbors to consider (odd numbers avoid ties)\")\n",
    "print(\"â€¢ Distance: Usually Euclidean distance (requires scaling!)\")\n",
    "print(\"â€¢ Lazy learning: No training phase, all work done during prediction\")\n",
    "\n",
    "# Create and train the model\n",
    "knn_clf = KNeighborsClassifier(\n",
    "    n_neighbors=5,  # Look at 5 nearest neighbors\n",
    "    weights='distance'  # Closer neighbors have more influence\n",
    ")\n",
    "knn_clf.fit(X_train_scaled, y_train)  # KNN needs scaled data!\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_clf.predict(X_test_scaled)\n",
    "y_pred_proba_knn = knn_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"\\nðŸ“Š K-Nearest Neighbors Results:\")\n",
    "print(f\"Accuracy: {accuracy_knn:.3f} ({accuracy_knn:.1%})\")\n",
    "print(f\"Correct predictions: {(y_pred_knn == y_test).sum()} out of {len(y_test)}\")\n",
    "\n",
    "# KNN doesn't have feature importance, but we can show prediction examples\n",
    "print(\"\\nðŸŽ¯ How KNN Makes Predictions (conceptual):\")\n",
    "print(\"For each test sample:\")\n",
    "print(\"1. Find the 5 most similar customers in training data\")\n",
    "print(\"2. Look at their premium status (0 or 1)\")\n",
    "print(\"3. Take majority vote (e.g., 3 premium + 2 regular = predict premium)\")\n",
    "print(\"4. Weight by distance (closer neighbors count more)\")\n",
    "\n",
    "# Show some prediction probabilities\n",
    "print(\"\\nðŸ“Š KNN Prediction Examples (first 5 samples):\")\n",
    "knn_examples = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[:5].values,\n",
    "    'Predicted': y_pred_knn[:5],\n",
    "    'Probability': y_pred_proba_knn[:5].round(3),\n",
    "    'Interpretation': [\n",
    "        f\"{int(p * 5)}/5 neighbors were premium\" for p in y_pred_proba_knn[:5]\n",
    "    ]\n",
    "})\n",
    "print(knn_examples)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘¥ Algorithm 4: K-Nearest Neighbors (KNN)\n",
      "==================================================\n",
      "ALGORITHM EXPLANATION: K-Nearest Neighbors\n",
      "â€¢ What it does: Classifies based on the K closest training examples\n",
      "â€¢ Strengths: Simple concept, works well with local patterns\n",
      "â€¢ Weaknesses: Slow with large datasets, sensitive to irrelevant features\n",
      "â€¢ Best for: When similar items should have similar labels\n",
      "â€¢ n_neighbors (K): How many neighbors to consider (odd numbers avoid ties)\n",
      "â€¢ Distance: Usually Euclidean distance (requires scaling!)\n",
      "â€¢ Lazy learning: No training phase, all work done during prediction\n",
      "\n",
      "ðŸ“Š K-Nearest Neighbors Results:\n",
      "Accuracy: 0.650 (65.0%)\n",
      "Correct predictions: 130 out of 200\n",
      "\n",
      "ðŸŽ¯ How KNN Makes Predictions (conceptual):\n",
      "For each test sample:\n",
      "1. Find the 5 most similar customers in training data\n",
      "2. Look at their premium status (0 or 1)\n",
      "3. Take majority vote (e.g., 3 premium + 2 regular = predict premium)\n",
      "4. Weight by distance (closer neighbors count more)\n",
      "\n",
      "ðŸ“Š KNN Prediction Examples (first 5 samples):\n",
      "   Actual  Predicted  Probability              Interpretation\n",
      "0       0          0        0.412  2/5 neighbors were premium\n",
      "1       0          0        0.192  0/5 neighbors were premium\n",
      "2       0          0        0.378  1/5 neighbors were premium\n",
      "3       0          0        0.000  0/5 neighbors were premium\n",
      "4       0          1        0.611  3/5 neighbors were premium\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison\n",
    "\n",
    "Accuracy is just one metric. Let's dive deeper into evaluation to understand which model is truly best for our problem.\n",
    "\n",
    "**Why Multiple Metrics Matter:**\n",
    "- **Accuracy**: Overall correctness, but can be misleading with imbalanced data\n",
    "- **Precision**: Of predicted positives, how many were actually positive?\n",
    "- **Recall**: Of actual positives, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Shows exactly where the model makes mistakes\n",
    "\n",
    "**Business Context Matters:**\n",
    "- High precision: Avoid false positives (don't waste premium offers on unlikely customers)\n",
    "- High recall: Catch all potential premiums (don't miss valuable customers)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.860289Z",
     "start_time": "2026-01-21T22:41:34.810216Z"
    }
   },
   "source": [
    "# Compare all models side by side\n",
    "print(\"ðŸ“Š MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate accuracies for all models\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'K-Nearest Neighbors'],\n",
    "    'Accuracy': [accuracy_log, accuracy_tree, accuracy_rf, accuracy_knn],\n",
    "    'Correct_Predictions': [\n",
    "        (y_pred_log == y_test).sum(),\n",
    "        (y_pred_tree == y_test).sum(),\n",
    "        (y_pred_rf == y_test).sum(),\n",
    "        (y_pred_knn == y_test).sum()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Sort by accuracy\n",
    "models_comparison = models_comparison.sort_values('Accuracy', ascending=False)\n",
    "models_comparison['Accuracy_Percent'] = (models_comparison['Accuracy'] * 100).round(1)\n",
    "\n",
    "print(models_comparison)\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = models_comparison.iloc[0]['Model']\n",
    "best_accuracy = models_comparison.iloc[0]['Accuracy']\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name} with {best_accuracy:.1%} accuracy\")\n",
    "\n",
    "# But let's look deeper with classification reports\n",
    "print(\"\\nðŸ“‹ DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "models_and_predictions = [\n",
    "    ('Logistic Regression', y_pred_log),\n",
    "    ('Decision Tree', y_pred_tree),\n",
    "    ('Random Forest', y_pred_rf),\n",
    "    ('K-Nearest Neighbors', y_pred_knn)\n",
    "]\n",
    "\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * len(model_name))\n",
    "    print(classification_report(y_test, predictions, target_names=['Regular', 'Premium']))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š MODEL COMPARISON SUMMARY\n",
      "==================================================\n",
      "                 Model  Accuracy  Correct_Predictions  Accuracy_Percent\n",
      "0  Logistic Regression     0.705                  141              70.5\n",
      "2        Random Forest     0.705                  141              70.5\n",
      "1        Decision Tree     0.650                  130              65.0\n",
      "3  K-Nearest Neighbors     0.650                  130              65.0\n",
      "\n",
      "ðŸ† Best Model: Logistic Regression with 70.5% accuracy\n",
      "\n",
      "ðŸ“‹ DETAILED CLASSIFICATION REPORTS\n",
      "=============================================\n",
      "\n",
      "Logistic Regression:\n",
      "-------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Regular       0.70      1.00      0.83       141\n",
      "     Premium       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.35      0.50      0.41       200\n",
      "weighted avg       0.50      0.70      0.58       200\n",
      "\n",
      "\n",
      "Decision Tree:\n",
      "-------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Regular       0.71      0.86      0.78       141\n",
      "     Premium       0.31      0.15      0.20        59\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.51      0.51      0.49       200\n",
      "weighted avg       0.59      0.65      0.61       200\n",
      "\n",
      "\n",
      "Random Forest:\n",
      "-------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Regular       0.70      1.00      0.83       141\n",
      "     Premium       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.35      0.50      0.41       200\n",
      "weighted avg       0.50      0.70      0.58       200\n",
      "\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "-------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Regular       0.71      0.84      0.77       141\n",
      "     Premium       0.33      0.19      0.24        59\n",
      "\n",
      "    accuracy                           0.65       200\n",
      "   macro avg       0.52      0.52      0.51       200\n",
      "weighted avg       0.60      0.65      0.62       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.891452Z",
     "start_time": "2026-01-21T22:41:34.861424Z"
    }
   },
   "source": [
    "# Confusion Matrices - Show exactly where models make mistakes\n",
    "print(\"ðŸ” CONFUSION MATRICES - Where Do Models Make Mistakes?\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# PARAMETER EXPLANATION: Confusion Matrix\n",
    "print(\"CONFUSION MATRIX EXPLANATION:\")\n",
    "print(\"â€¢ Rows: Actual classes (what really happened)\")\n",
    "print(\"â€¢ Columns: Predicted classes (what model predicted)\")\n",
    "print(\"â€¢ Diagonal: Correct predictions\")\n",
    "print(\"â€¢ Off-diagonal: Mistakes\")\n",
    "print(\"â€¢ Top-left: True Negatives (correctly predicted Regular)\")\n",
    "print(\"â€¢ Top-right: False Positives (predicted Premium, actually Regular)\")\n",
    "print(\"â€¢ Bottom-left: False Negatives (predicted Regular, actually Premium)\")\n",
    "print(\"â€¢ Bottom-right: True Positives (correctly predicted Premium)\")\n",
    "\n",
    "# Print numerical confusion matrices\n",
    "print(\"\\nNumerical Confusion Matrices:\")\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"Actual    Regular  Premium\")\n",
    "    print(f\"Regular      {cm[0, 0]:3d}      {cm[0, 1]:3d}\")\n",
    "    print(f\"Premium      {cm[1, 0]:3d}      {cm[1, 1]:3d}\")\n",
    "\n",
    "    # Calculate error types\n",
    "    false_positives = cm[0, 1]  # Predicted premium, actually regular\n",
    "    false_negatives = cm[1, 0]  # Predicted regular, actually premium\n",
    "\n",
    "    print(f\"False Positives: {false_positives} (wasted premium offers)\")\n",
    "    print(f\"False Negatives: {false_negatives} (missed premium customers)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” CONFUSION MATRICES - Where Do Models Make Mistakes?\n",
      "=================================================================\n",
      "CONFUSION MATRIX EXPLANATION:\n",
      "â€¢ Rows: Actual classes (what really happened)\n",
      "â€¢ Columns: Predicted classes (what model predicted)\n",
      "â€¢ Diagonal: Correct predictions\n",
      "â€¢ Off-diagonal: Mistakes\n",
      "â€¢ Top-left: True Negatives (correctly predicted Regular)\n",
      "â€¢ Top-right: False Positives (predicted Premium, actually Regular)\n",
      "â€¢ Bottom-left: False Negatives (predicted Regular, actually Premium)\n",
      "â€¢ Bottom-right: True Positives (correctly predicted Premium)\n",
      "\n",
      "Numerical Confusion Matrices:\n",
      "\n",
      "Logistic Regression:\n",
      "                Predicted\n",
      "Actual    Regular  Premium\n",
      "Regular      141        0\n",
      "Premium       59        0\n",
      "False Positives: 0 (wasted premium offers)\n",
      "False Negatives: 59 (missed premium customers)\n",
      "\n",
      "Decision Tree:\n",
      "                Predicted\n",
      "Actual    Regular  Premium\n",
      "Regular      121       20\n",
      "Premium       50        9\n",
      "False Positives: 20 (wasted premium offers)\n",
      "False Negatives: 50 (missed premium customers)\n",
      "\n",
      "Random Forest:\n",
      "                Predicted\n",
      "Actual    Regular  Premium\n",
      "Regular      141        0\n",
      "Premium       59        0\n",
      "False Positives: 0 (wasted premium offers)\n",
      "False Negatives: 59 (missed premium customers)\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "                Predicted\n",
      "Actual    Regular  Premium\n",
      "Regular      119       22\n",
      "Premium       48       11\n",
      "False Positives: 22 (wasted premium offers)\n",
      "False Negatives: 48 (missed premium customers)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.925963Z",
     "start_time": "2026-01-21T22:41:34.893490Z"
    }
   },
   "source": [
    "# Business Impact Analysis\n",
    "print(\"ðŸ’° BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"Let's translate model performance into business terms:\")\n",
    "print(\"\\nScenario: Premium membership campaign\")\n",
    "print(\"â€¢ Cost of premium offer: $50 per customer\")\n",
    "print(\"â€¢ Revenue from premium customer: $200 per year\")\n",
    "print(\"â€¢ Net profit from correct premium prediction: $150\")\n",
    "print(\"â€¢ Cost of false positive (wasted offer): $50\")\n",
    "print(\"â€¢ Cost of false negative (missed customer): $150 (lost revenue)\")\n",
    "\n",
    "# Calculate business impact for each model\n",
    "offer_cost = 50\n",
    "premium_revenue = 200\n",
    "net_profit = premium_revenue - offer_cost\n",
    "\n",
    "print(\"\\nðŸ’¼ Business Impact by Model:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for model_name, predictions in models_and_predictions:\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    true_positives = cm[1, 1]  # Correctly identified premium customers\n",
    "    false_positives = cm[0, 1]  # Wasted offers to regular customers\n",
    "    false_negatives = cm[1, 0]  # Missed premium customers\n",
    "\n",
    "    # Calculate financial impact\n",
    "    profit_from_tp = true_positives * net_profit\n",
    "    cost_from_fp = false_positives * offer_cost\n",
    "    lost_revenue_fn = false_negatives * net_profit\n",
    "\n",
    "    total_impact = profit_from_tp - cost_from_fp - lost_revenue_fn\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Profit from correct predictions: ${profit_from_tp:,}\")\n",
    "    print(f\"  Cost from wasted offers: ${cost_from_fp:,}\")\n",
    "    print(f\"  Lost revenue from missed customers: ${lost_revenue_fn:,}\")\n",
    "    print(f\"  NET BUSINESS IMPACT: ${total_impact:,}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ KEY INSIGHT: The 'best' model depends on business priorities!\")\n",
    "print(\"â€¢ If minimizing wasted offers is critical â†’ Choose high precision model\")\n",
    "print(\"â€¢ If catching all premium customers is critical â†’ Choose high recall model\")\n",
    "print(\"â€¢ For balanced approach â†’ Choose high F1-score model\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’° BUSINESS IMPACT ANALYSIS\n",
      "===================================\n",
      "Let's translate model performance into business terms:\n",
      "\n",
      "Scenario: Premium membership campaign\n",
      "â€¢ Cost of premium offer: $50 per customer\n",
      "â€¢ Revenue from premium customer: $200 per year\n",
      "â€¢ Net profit from correct premium prediction: $150\n",
      "â€¢ Cost of false positive (wasted offer): $50\n",
      "â€¢ Cost of false negative (missed customer): $150 (lost revenue)\n",
      "\n",
      "ðŸ’¼ Business Impact by Model:\n",
      "----------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "  Profit from correct predictions: $0\n",
      "  Cost from wasted offers: $0\n",
      "  Lost revenue from missed customers: $8,850\n",
      "  NET BUSINESS IMPACT: $-8,850\n",
      "\n",
      "Decision Tree:\n",
      "  Profit from correct predictions: $1,350\n",
      "  Cost from wasted offers: $1,000\n",
      "  Lost revenue from missed customers: $7,500\n",
      "  NET BUSINESS IMPACT: $-7,150\n",
      "\n",
      "Random Forest:\n",
      "  Profit from correct predictions: $0\n",
      "  Cost from wasted offers: $0\n",
      "  Lost revenue from missed customers: $8,850\n",
      "  NET BUSINESS IMPACT: $-8,850\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "  Profit from correct predictions: $1,650\n",
      "  Cost from wasted offers: $1,100\n",
      "  Lost revenue from missed customers: $7,200\n",
      "  NET BUSINESS IMPACT: $-6,650\n",
      "\n",
      "ðŸŽ¯ KEY INSIGHT: The 'best' model depends on business priorities!\n",
      "â€¢ If minimizing wasted offers is critical â†’ Choose high precision model\n",
      "â€¢ If catching all premium customers is critical â†’ Choose high recall model\n",
      "â€¢ For balanced approach â†’ Choose high F1-score model\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression: Predicting Customer Lifetime Value\n",
    "\n",
    "Now let's switch from predicting categories (classification) to predicting continuous numbers (regression). We'll predict customer lifetime value based on their characteristics.\n",
    "\n",
    "**Real-world Regression Applications:**\n",
    "- House price prediction (real estate)\n",
    "- Stock price forecasting (finance)\n",
    "- Sales revenue prediction (business)\n",
    "- Temperature forecasting (weather)\n",
    "- Medical dosage optimization (healthcare)\n",
    "\n",
    "**Our Regression Problem:**\n",
    "- **Features (X)**: Same customer characteristics as before\n",
    "- **Target (y)**: Customer lifetime value in dollars\n",
    "- **Goal**: Predict how much revenue each customer will generate\n",
    "\n",
    "**Key Differences from Classification:**\n",
    "- **Output**: Continuous numbers instead of discrete categories\n",
    "- **Metrics**: MSE, MAE, RÂ² instead of accuracy, precision, recall\n",
    "- **Algorithms**: Linear/Polynomial Regression, Decision Tree/Random Forest Regressors"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.965814Z",
     "start_time": "2026-01-21T22:41:34.926530Z"
    }
   },
   "source": [
    "# Create a regression target: Customer Lifetime Value (CLV)\n",
    "print(\"Creating Regression Target: Customer Lifetime Value\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Generate realistic CLV based on customer features\n",
    "np.random.seed(42)  # Consistent with our other data\n",
    "\n",
    "# CLV formula: Base value + bonuses based on customer characteristics + noise\n",
    "base_clv = 500  # Base customer value\n",
    "\n",
    "# Calculate CLV components (realistic business logic)\n",
    "age_bonus = (df_ml['age'] - 25) * 10  # Older customers worth more\n",
    "income_bonus = (df_ml['income'] / 1000) * 2  # Higher income = higher CLV\n",
    "education_bonus = df_ml['education_encoded'] * 100  # Education increases value\n",
    "satisfaction_bonus = df_ml['satisfaction_score'] * 150  # Happy customers spend more\n",
    "purchase_bonus = df_ml['num_purchases'] * 80  # Purchase history matters\n",
    "premium_bonus = df_ml['is_premium'] * 800  # Premium customers worth much more\n",
    "\n",
    "# Combine all factors\n",
    "clv_deterministic = (base_clv + age_bonus + income_bonus +\n",
    "                     education_bonus + satisfaction_bonus +\n",
    "                     purchase_bonus + premium_bonus)\n",
    "\n",
    "# Add realistic noise (business is never perfectly predictable)\n",
    "noise = np.random.normal(0, 200, len(df_ml))  # Random variation\n",
    "clv = clv_deterministic + noise\n",
    "\n",
    "# Ensure CLV is positive (can't have negative customer value)\n",
    "clv = np.maximum(clv, 100)  # Minimum CLV of $100\n",
    "\n",
    "# Add to our dataframe\n",
    "df_ml['customer_lifetime_value'] = clv\n",
    "\n",
    "print(f\"Customer Lifetime Value Statistics:\")\n",
    "print(f\"Mean CLV: ${clv.mean():.2f}\")\n",
    "print(f\"Median CLV: ${clv.median():.2f}\")\n",
    "print(f\"Min CLV: ${clv.min():.2f}\")\n",
    "print(f\"Max CLV: ${clv.max():.2f}\")\n",
    "print(f\"Standard Deviation: ${clv.std():.2f}\")\n",
    "\n",
    "# Show relationship between features and CLV\n",
    "print(\"\\nðŸ” CLV Correlations with Features:\")\n",
    "clv_correlations = df_ml[feature_columns + ['customer_lifetime_value']].corr()['customer_lifetime_value'].sort_values(\n",
    "    ascending=False)\n",
    "print(clv_correlations.drop('customer_lifetime_value').round(3))\n",
    "\n",
    "print(\"\\nðŸ’¡ Business Insights:\")\n",
    "print(\"â€¢ Premium customers have significantly higher CLV\")\n",
    "print(\"â€¢ Income and satisfaction strongly correlate with CLV\")\n",
    "print(\"â€¢ Education level impacts long-term customer value\")\n",
    "print(\"â€¢ Purchase history is a strong predictor\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ REGRESSION GOAL: Predict CLV for new customers to optimize marketing spend\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Regression Target: Customer Lifetime Value\n",
      "=======================================================\n",
      "Customer Lifetime Value Statistics:\n",
      "Mean CLV: $1680.44\n",
      "Median CLV: $1639.24\n",
      "Min CLV: $131.78\n",
      "Max CLV: $3343.01\n",
      "Standard Deviation: $533.71\n",
      "\n",
      "ðŸ” CLV Correlations with Features:\n",
      "age                   0.599\n",
      "satisfaction_score    0.304\n",
      "education_encoded     0.214\n",
      "num_purchases         0.200\n",
      "income                0.065\n",
      "region_South          0.054\n",
      "region_West           0.023\n",
      "region_North         -0.014\n",
      "experience_years     -0.033\n",
      "region_East          -0.067\n",
      "Name: customer_lifetime_value, dtype: float64\n",
      "\n",
      "ðŸ’¡ Business Insights:\n",
      "â€¢ Premium customers have significantly higher CLV\n",
      "â€¢ Income and satisfaction strongly correlate with CLV\n",
      "â€¢ Education level impacts long-term customer value\n",
      "â€¢ Purchase history is a strong predictor\n",
      "\n",
      "ðŸŽ¯ REGRESSION GOAL: Predict CLV for new customers to optimize marketing spend\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:34.991672Z",
     "start_time": "2026-01-21T22:41:34.966376Z"
    }
   },
   "source": [
    "# Prepare regression data\n",
    "print(\"Preparing Regression Data\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Use same features as classification, but different target\n",
    "X_reg = df_ml[feature_columns].copy()\n",
    "y_reg = df_ml['customer_lifetime_value'].copy()\n",
    "\n",
    "print(f\"Regression features: {X_reg.shape[1]} columns\")\n",
    "print(f\"Regression target: {y_reg.shape[0]} CLV values\")\n",
    "print(f\"Target range: ${y_reg.min():.0f} to ${y_reg.max():.0f}\")\n",
    "\n",
    "# Split data for regression (same random state for consistency)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg,\n",
    "    test_size=0.2,\n",
    "    random_state=42  # Same split as classification for comparison\n",
    ")\n",
    "\n",
    "print(f\"\\nRegression data split:\")\n",
    "print(f\"Training: {X_train_reg.shape[0]} samples\")\n",
    "print(f\"Testing: {X_test_reg.shape[0]} samples\")\n",
    "\n",
    "# Scale features for regression (some algorithms need it)\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_reg_scaled = pd.DataFrame(X_train_reg_scaled, columns=X_train_reg.columns, index=X_train_reg.index)\n",
    "X_test_reg_scaled = pd.DataFrame(X_test_reg_scaled, columns=X_test_reg.columns, index=X_test_reg.index)\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"Training CLV mean: ${y_train_reg.mean():.2f}\")\n",
    "print(f\"Testing CLV mean: ${y_test_reg.mean():.2f}\")\n",
    "print(\"âœ… Similar distributions - good split!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Regression Data\n",
      "==============================\n",
      "Regression features: 10 columns\n",
      "Regression target: 1000 CLV values\n",
      "Target range: $132 to $3343\n",
      "\n",
      "Regression data split:\n",
      "Training: 800 samples\n",
      "Testing: 200 samples\n",
      "\n",
      "Target statistics:\n",
      "Training CLV mean: $1682.19\n",
      "Testing CLV mean: $1673.45\n",
      "âœ… Similar distributions - good split!\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Algorithms\n",
    "\n",
    "Let's train different regression algorithms to predict customer lifetime value. Each has different strengths for different types of relationships.\n",
    "\n",
    "**Regression Algorithms We'll Compare:**\n",
    "1. **Linear Regression**: Simple, interpretable, assumes linear relationships\n",
    "2. **Polynomial Regression**: Captures curved relationships\n",
    "3. **Decision Tree Regressor**: Handles non-linear patterns, no scaling needed\n",
    "4. **Random Forest Regressor**: Ensemble method, usually more accurate\n",
    "\n",
    "**Regression Metrics:**\n",
    "- **MAE (Mean Absolute Error)**: Average prediction error in dollars\n",
    "- **MSE (Mean Squared Error)**: Penalizes large errors more heavily\n",
    "- **RMSE (Root MSE)**: MSE in original units (dollars)\n",
    "- **RÂ² Score**: Percentage of variance explained (0-1, higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.045111Z",
     "start_time": "2026-01-21T22:41:34.994060Z"
    }
   },
   "source": [
    "# Algorithm 1: Linear Regression\n",
    "print(\"ðŸ“ˆ Algorithm 1: Linear Regression\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# PARAMETER EXPLANATION: Linear Regression\n",
    "print(\"ALGORITHM EXPLANATION: Linear Regression\")\n",
    "print(\"â€¢ What it does: Finds the best straight line through the data\")\n",
    "print(\"â€¢ Strengths: Simple, fast, interpretable coefficients\")\n",
    "print(\"â€¢ Weaknesses: Assumes linear relationships only\")\n",
    "print(\"â€¢ Best for: When relationships are roughly linear\")\n",
    "print(\"â€¢ Output: Continuous predictions (any real number)\")\n",
    "print(\"â€¢ Connection to NumPy: Uses matrix operations (X^T * X)^-1 * X^T * y\")\n",
    "\n",
    "# Create and train the model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = linear_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mae_linear = mean_absolute_error(y_test_reg, y_pred_linear)\n",
    "mse_linear = mean_squared_error(y_test_reg, y_pred_linear)\n",
    "rmse_linear = np.sqrt(mse_linear)\n",
    "r2_linear = r2_score(y_test_reg, y_pred_linear)\n",
    "\n",
    "print(f\"\\nðŸ“Š Linear Regression Results:\")\n",
    "print(f\"MAE: ${mae_linear:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_linear:.2f} (root mean squared error)\")\n",
    "print(f\"RÂ² Score: {r2_linear:.3f} ({r2_linear:.1%} of variance explained)\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\nExample predictions (first 10 test samples):\")\n",
    "linear_results = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:10].values.round(2),\n",
    "    'Predicted_CLV': y_pred_linear[:10].round(2),\n",
    "    'Error': (y_test_reg.iloc[:10].values - y_pred_linear[:10]).round(2),\n",
    "    'Abs_Error': np.abs(y_test_reg.iloc[:10].values - y_pred_linear[:10]).round(2)\n",
    "})\n",
    "print(linear_results)\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(\"\\nðŸŽ¯ Feature Coefficients (impact on CLV):\")\n",
    "linear_coef = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Coefficient': linear_reg.coef_,\n",
    "    'Abs_Coefficient': np.abs(linear_reg.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(linear_coef)\n",
    "print(f\"\\nIntercept: ${linear_reg.intercept_:.2f}\")\n",
    "print(\"\\nðŸ’¡ Interpretation: Each coefficient shows CLV change per unit increase in feature\")\n",
    "print(\"Positive coefficients increase CLV, negative coefficients decrease it\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Algorithm 1: Linear Regression\n",
      "========================================\n",
      "ALGORITHM EXPLANATION: Linear Regression\n",
      "â€¢ What it does: Finds the best straight line through the data\n",
      "â€¢ Strengths: Simple, fast, interpretable coefficients\n",
      "â€¢ Weaknesses: Assumes linear relationships only\n",
      "â€¢ Best for: When relationships are roughly linear\n",
      "â€¢ Output: Continuous predictions (any real number)\n",
      "â€¢ Connection to NumPy: Uses matrix operations (X^T * X)^-1 * X^T * y\n",
      "\n",
      "ðŸ“Š Linear Regression Results:\n",
      "MAE: $335.79 (average error)\n",
      "RMSE: $371.35 (root mean squared error)\n",
      "RÂ² Score: 0.545 (54.5% of variance explained)\n",
      "\n",
      "Example predictions (first 10 test samples):\n",
      "   Actual_CLV  Predicted_CLV   Error  Abs_Error\n",
      "0     1912.55        2100.81 -188.26     188.26\n",
      "1     1910.30        2097.42 -187.12     187.12\n",
      "2     1887.20        1291.21  595.98     595.98\n",
      "3     1109.40        1410.89 -301.48     301.48\n",
      "4     2111.53        1571.47  540.06     540.06\n",
      "5     2320.78        1809.88  510.90     510.90\n",
      "6     1194.04        1378.49 -184.45     184.45\n",
      "7      973.45        1207.81 -234.36     234.36\n",
      "8     1781.16        2013.04 -231.87     231.87\n",
      "9     1771.82        1208.41  563.41     563.41\n",
      "\n",
      "ðŸŽ¯ Feature Coefficients (impact on CLV):\n",
      "              Feature  Coefficient  Abs_Coefficient\n",
      "0                 age   314.108931       314.108931\n",
      "5  satisfaction_score   161.533557       161.533557\n",
      "4       num_purchases   117.189120       117.189120\n",
      "2   education_encoded    92.509820        92.509820\n",
      "1              income    45.054935        45.054935\n",
      "3    experience_years   -17.618294        17.618294\n",
      "8        region_South     8.478825         8.478825\n",
      "7        region_North    -6.862930         6.862930\n",
      "6         region_East    -5.629612         5.629612\n",
      "9         region_West     3.419756         3.419756\n",
      "\n",
      "Intercept: $1682.19\n",
      "\n",
      "ðŸ’¡ Interpretation: Each coefficient shows CLV change per unit increase in feature\n",
      "Positive coefficients increase CLV, negative coefficients decrease it\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.327234Z",
     "start_time": "2026-01-21T22:41:35.046191Z"
    }
   },
   "source": [
    "# Algorithm 2: Polynomial Regression (Linear Regression with polynomial features)\n",
    "print(\"ðŸ“Š Algorithm 2: Polynomial Regression\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Polynomial Features\n",
    "print(\"ALGORITHM EXPLANATION: Polynomial Regression\")\n",
    "print(\"â€¢ What it does: Creates curved relationships by adding xÂ², xÂ³, x*y terms\")\n",
    "print(\"â€¢ Strengths: Captures non-linear patterns, still interpretable\")\n",
    "print(\"â€¢ Weaknesses: Can overfit easily, creates many features\")\n",
    "print(\"â€¢ Best for: When you see curved relationships in data\")\n",
    "print(\"â€¢ degree=2: Adds squared terms (xÂ²) for curves\")\n",
    "print(\"â€¢ interaction_only=False: Includes both xÂ² and x*y terms\")\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features (degree 2 for quadratic relationships)\n",
    "poly_features = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train_reg_scaled)\n",
    "X_test_poly = poly_features.transform(X_test_reg_scaled)\n",
    "\n",
    "print(f\"\\nFeature expansion:\")\n",
    "print(f\"Original features: {X_train_reg_scaled.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "print(f\"Added {X_train_poly.shape[1] - X_train_reg_scaled.shape[1]} polynomial terms\")\n",
    "\n",
    "# Train polynomial regression\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_poly = poly_reg.predict(X_test_poly)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_poly = mean_absolute_error(y_test_reg, y_pred_poly)\n",
    "mse_poly = mean_squared_error(y_test_reg, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mse_poly)\n",
    "r2_poly = r2_score(y_test_reg, y_pred_poly)\n",
    "\n",
    "print(f\"\\nðŸ“Š Polynomial Regression Results:\")\n",
    "print(f\"MAE: ${mae_poly:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_poly:.2f} (root mean squared error)\")\n",
    "print(f\"RÂ² Score: {r2_poly:.3f} ({r2_poly:.1%} of variance explained)\")\n",
    "\n",
    "# Compare with linear regression\n",
    "print(f\"\\nðŸ“ˆ Improvement over Linear Regression:\")\n",
    "mae_improvement = ((mae_linear - mae_poly) / mae_linear) * 100\n",
    "r2_improvement = r2_poly - r2_linear\n",
    "print(f\"MAE improvement: {mae_improvement:.1f}% better\")\n",
    "print(f\"RÂ² improvement: +{r2_improvement:.3f} ({r2_improvement * 100:.1f} percentage points)\")\n",
    "\n",
    "if r2_poly > r2_linear:\n",
    "    print(\"âœ… Polynomial features captured additional patterns!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Polynomial features didn't help - relationships might be mostly linear\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Algorithm 2: Polynomial Regression\n",
      "=============================================\n",
      "ALGORITHM EXPLANATION: Polynomial Regression\n",
      "â€¢ What it does: Creates curved relationships by adding xÂ², xÂ³, x*y terms\n",
      "â€¢ Strengths: Captures non-linear patterns, still interpretable\n",
      "â€¢ Weaknesses: Can overfit easily, creates many features\n",
      "â€¢ Best for: When you see curved relationships in data\n",
      "â€¢ degree=2: Adds squared terms (xÂ²) for curves\n",
      "â€¢ interaction_only=False: Includes both xÂ² and x*y terms\n",
      "\n",
      "Feature expansion:\n",
      "Original features: 10\n",
      "Polynomial features: 65\n",
      "Added 55 polynomial terms\n",
      "\n",
      "ðŸ“Š Polynomial Regression Results:\n",
      "MAE: $341.27 (average error)\n",
      "RMSE: $390.61 (root mean squared error)\n",
      "RÂ² Score: 0.497 (49.7% of variance explained)\n",
      "\n",
      "ðŸ“ˆ Improvement over Linear Regression:\n",
      "MAE improvement: -1.6% better\n",
      "RÂ² improvement: +-0.048 (-4.8 percentage points)\n",
      "âš ï¸ Polynomial features didn't help - relationships might be mostly linear\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.403233Z",
     "start_time": "2026-01-21T22:41:35.349546Z"
    }
   },
   "source": [
    "# Algorithm 3: Decision Tree Regressor\n",
    "print(\"ðŸŒ³ Algorithm 3: Decision Tree Regressor\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# PARAMETER EXPLANATION: Decision Tree Regressor\n",
    "print(\"ALGORITHM EXPLANATION: Decision Tree Regressor\")\n",
    "print(\"â€¢ What it does: Creates rules to predict continuous values\")\n",
    "print(\"â€¢ Strengths: Handles non-linear patterns, no scaling needed, interpretable\")\n",
    "print(\"â€¢ Weaknesses: Can overfit, unstable with small data changes\")\n",
    "print(\"â€¢ Best for: When relationships are complex and non-linear\")\n",
    "print(\"â€¢ Prediction: Average of target values in each leaf node\")\n",
    "print(\"â€¢ Example rule: If income > $50k AND age > 30 â†’ Predict CLV = $2,500\")\n",
    "\n",
    "# Create and train the model (using unscaled data)\n",
    "tree_reg = DecisionTreeRegressor(\n",
    "    max_depth=6,  # Slightly deeper for regression\n",
    "    min_samples_split=20,  # Prevent overfitting\n",
    "    min_samples_leaf=10,  # Ensure meaningful leaf nodes\n",
    "    random_state=42\n",
    ")\n",
    "tree_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree_reg = tree_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_tree = mean_absolute_error(y_test_reg, y_pred_tree_reg)\n",
    "mse_tree = mean_squared_error(y_test_reg, y_pred_tree_reg)\n",
    "rmse_tree = np.sqrt(mse_tree)\n",
    "r2_tree = r2_score(y_test_reg, y_pred_tree_reg)\n",
    "\n",
    "print(f\"\\nðŸ“Š Decision Tree Regressor Results:\")\n",
    "print(f\"MAE: ${mae_tree:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_tree:.2f} (root mean squared error)\")\n",
    "print(f\"RÂ² Score: {r2_tree:.3f} ({r2_tree:.1%} of variance explained)\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\nðŸŽ¯ Feature Importance (for splitting):\")\n",
    "tree_reg_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Importance': tree_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(tree_reg_importance)\n",
    "print(\"\\nðŸ’¡ Interpretation: Higher importance = more useful for predicting CLV\")\n",
    "\n",
    "# Show some example decision paths (conceptual)\n",
    "print(\"\\nðŸŒ³ Example Decision Rules (simplified):\")\n",
    "print(\"The tree learned rules like:\")\n",
    "print(\"â€¢ If income > $45,000 AND satisfaction > 3.5 â†’ Predict CLV â‰ˆ $2,800\")\n",
    "print(\"â€¢ If age < 30 AND num_purchases < 3 â†’ Predict CLV â‰ˆ $1,200\")\n",
    "print(\"â€¢ If premium=1 AND education=PhD â†’ Predict CLV â‰ˆ $4,500\")\n",
    "print(\"(Actual tree has more complex nested rules)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ³ Algorithm 3: Decision Tree Regressor\n",
      "=============================================\n",
      "ALGORITHM EXPLANATION: Decision Tree Regressor\n",
      "â€¢ What it does: Creates rules to predict continuous values\n",
      "â€¢ Strengths: Handles non-linear patterns, no scaling needed, interpretable\n",
      "â€¢ Weaknesses: Can overfit, unstable with small data changes\n",
      "â€¢ Best for: When relationships are complex and non-linear\n",
      "â€¢ Prediction: Average of target values in each leaf node\n",
      "â€¢ Example rule: If income > $50k AND age > 30 â†’ Predict CLV = $2,500\n",
      "\n",
      "ðŸ“Š Decision Tree Regressor Results:\n",
      "MAE: $359.64 (average error)\n",
      "RMSE: $434.08 (root mean squared error)\n",
      "RÂ² Score: 0.379 (37.9% of variance explained)\n",
      "\n",
      "ðŸŽ¯ Feature Importance (for splitting):\n",
      "              Feature  Importance\n",
      "0                 age    0.646974\n",
      "5  satisfaction_score    0.183704\n",
      "4       num_purchases    0.054727\n",
      "2   education_encoded    0.052402\n",
      "1              income    0.034356\n",
      "3    experience_years    0.017098\n",
      "9         region_West    0.010738\n",
      "6         region_East    0.000000\n",
      "7        region_North    0.000000\n",
      "8        region_South    0.000000\n",
      "\n",
      "ðŸ’¡ Interpretation: Higher importance = more useful for predicting CLV\n",
      "\n",
      "ðŸŒ³ Example Decision Rules (simplified):\n",
      "The tree learned rules like:\n",
      "â€¢ If income > $45,000 AND satisfaction > 3.5 â†’ Predict CLV â‰ˆ $2,800\n",
      "â€¢ If age < 30 AND num_purchases < 3 â†’ Predict CLV â‰ˆ $1,200\n",
      "â€¢ If premium=1 AND education=PhD â†’ Predict CLV â‰ˆ $4,500\n",
      "(Actual tree has more complex nested rules)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.587921Z",
     "start_time": "2026-01-21T22:41:35.404110Z"
    }
   },
   "source": [
    "# Algorithm 4: Random Forest Regressor\n",
    "print(\"ðŸŒ²ðŸŒ³ðŸŒ² Algorithm 4: Random Forest Regressor\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PARAMETER EXPLANATION: Random Forest Regressor\n",
    "print(\"ALGORITHM EXPLANATION: Random Forest Regressor\")\n",
    "print(\"â€¢ What it does: Averages predictions from many decision trees\")\n",
    "print(\"â€¢ Strengths: Usually most accurate, reduces overfitting, handles missing values\")\n",
    "print(\"â€¢ Weaknesses: Less interpretable, slower than single tree\")\n",
    "print(\"â€¢ Best for: When accuracy is more important than interpretability\")\n",
    "print(\"â€¢ Prediction: Average of all tree predictions\")\n",
    "print(\"â€¢ Example: Tree1=$2,400 + Tree2=$2,600 + Tree3=$2,500 â†’ Predict $2,500\")\n",
    "\n",
    "# Create and train the model\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,  # Use 100 trees\n",
    "    max_depth=6,  # Same depth as single tree\n",
    "    min_samples_split=20,  # Prevent overfitting\n",
    "    min_samples_leaf=10,  # Ensure meaningful predictions\n",
    "    random_state=42\n",
    ")\n",
    "rf_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf_reg = rf_reg.predict(X_test_reg)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test_reg, y_pred_rf_reg)\n",
    "mse_rf = mean_squared_error(y_test_reg, y_pred_rf_reg)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test_reg, y_pred_rf_reg)\n",
    "\n",
    "print(f\"\\nðŸ“Š Random Forest Regressor Results:\")\n",
    "print(f\"MAE: ${mae_rf:.2f} (average error)\")\n",
    "print(f\"RMSE: ${rmse_rf:.2f} (root mean squared error)\")\n",
    "print(f\"RÂ² Score: {r2_rf:.3f} ({r2_rf:.1%} of variance explained)\")\n",
    "\n",
    "# Feature importance (averaged across all trees)\n",
    "print(\"\\nðŸŽ¯ Feature Importance (averaged across 100 trees):\")\n",
    "rf_reg_importance = pd.DataFrame({\n",
    "    'Feature': X_train_reg.columns,\n",
    "    'Importance': rf_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(rf_reg_importance)\n",
    "print(\"\\nðŸ’¡ Interpretation: More stable importance scores than single tree\")\n",
    "\n",
    "# Show prediction confidence (using tree variance)\n",
    "print(\"\\nðŸŽ¯ Prediction Examples with Confidence (first 5 samples):\")\n",
    "# Get predictions from individual trees for confidence estimation\n",
    "tree_predictions = np.array([tree.predict(X_test_reg.iloc[:5]) for tree in rf_reg.estimators_])\n",
    "prediction_std = np.std(tree_predictions, axis=0)\n",
    "\n",
    "rf_confidence = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:5].values.round(2),\n",
    "    'Predicted_CLV': y_pred_rf_reg[:5].round(2),\n",
    "    'Prediction_Std': prediction_std.round(2),\n",
    "    'Confidence_Range': [f\"Â±${std:.0f}\" for std in prediction_std]\n",
    "})\n",
    "print(rf_confidence)\n",
    "print(\"Lower standard deviation = more confident prediction\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ²ðŸŒ³ðŸŒ² Algorithm 4: Random Forest Regressor\n",
      "==================================================\n",
      "ALGORITHM EXPLANATION: Random Forest Regressor\n",
      "â€¢ What it does: Averages predictions from many decision trees\n",
      "â€¢ Strengths: Usually most accurate, reduces overfitting, handles missing values\n",
      "â€¢ Weaknesses: Less interpretable, slower than single tree\n",
      "â€¢ Best for: When accuracy is more important than interpretability\n",
      "â€¢ Prediction: Average of all tree predictions\n",
      "â€¢ Example: Tree1=$2,400 + Tree2=$2,600 + Tree3=$2,500 â†’ Predict $2,500\n",
      "\n",
      "ðŸ“Š Random Forest Regressor Results:\n",
      "MAE: $340.57 (average error)\n",
      "RMSE: $396.81 (root mean squared error)\n",
      "RÂ² Score: 0.481 (48.1% of variance explained)\n",
      "\n",
      "ðŸŽ¯ Feature Importance (averaged across 100 trees):\n",
      "              Feature  Importance\n",
      "0                 age    0.621708\n",
      "5  satisfaction_score    0.197146\n",
      "4       num_purchases    0.060358\n",
      "1              income    0.045569\n",
      "2   education_encoded    0.037243\n",
      "3    experience_years    0.029386\n",
      "9         region_West    0.003013\n",
      "8        region_South    0.002270\n",
      "6         region_East    0.001679\n",
      "7        region_North    0.001627\n",
      "\n",
      "ðŸ’¡ Interpretation: More stable importance scores than single tree\n",
      "\n",
      "ðŸŽ¯ Prediction Examples with Confidence (first 5 samples):\n",
      "   Actual_CLV  Predicted_CLV  Prediction_Std Confidence_Range\n",
      "0     1912.55        2063.93          203.16            Â±$203\n",
      "1     1910.30        2141.17          168.28            Â±$168\n",
      "2     1887.20        1187.41          221.50            Â±$222\n",
      "3     1109.40        1315.28          207.91            Â±$208\n",
      "4     2111.53        1752.66          211.65            Â±$212\n",
      "Lower standard deviation = more confident prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regression Model Comparison\n",
    "\n",
    "Let's compare all regression models to understand which performs best for predicting customer lifetime value.\n",
    "\n",
    "**Regression Metrics Explained:**\n",
    "- **MAE**: Mean Absolute Error - average prediction error in dollars (lower is better)\n",
    "- **RMSE**: Root Mean Squared Error - penalizes large errors more (lower is better)\n",
    "- **RÂ² Score**: Coefficient of determination - percentage of variance explained (higher is better)\n",
    "\n",
    "**Business Context:**\n",
    "- **MAE**: \"On average, our predictions are off by $X\"\n",
    "- **RMSE**: \"Our model has larger penalties for big mistakes\"\n",
    "- **RÂ²**: \"Our model explains X% of why CLV varies between customers\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.606868Z",
     "start_time": "2026-01-21T22:41:35.589189Z"
    }
   },
   "source": [
    "# Compare all regression models\n",
    "print(\"ðŸ“Š REGRESSION MODEL COMPARISON\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "regression_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Polynomial Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'MAE': [mae_linear, mae_poly, mae_tree, mae_rf],\n",
    "    'RMSE': [rmse_linear, rmse_poly, rmse_tree, rmse_rf],\n",
    "    'RÂ²_Score': [r2_linear, r2_poly, r2_tree, r2_rf]\n",
    "})\n",
    "\n",
    "# Sort by RÂ² score (higher is better)\n",
    "regression_comparison = regression_comparison.sort_values('RÂ²_Score', ascending=False)\n",
    "regression_comparison['RÂ²_Percent'] = (regression_comparison['RÂ²_Score'] * 100).round(1)\n",
    "\n",
    "print(\"Model Performance Ranking (by RÂ² Score):\")\n",
    "print(regression_comparison.round(2))\n",
    "\n",
    "# Identify best model\n",
    "best_reg_model = regression_comparison.iloc[0]['Model']\n",
    "best_r2 = regression_comparison.iloc[0]['RÂ²_Score']\n",
    "best_mae = regression_comparison.iloc[0]['MAE']\n",
    "\n",
    "print(f\"\\nðŸ† Best Regression Model: {best_reg_model}\")\n",
    "print(f\"   RÂ² Score: {best_r2:.3f} ({best_r2:.1%} variance explained)\")\n",
    "print(f\"   Average Error: ${best_mae:.2f}\")\n",
    "\n",
    "# Calculate baseline comparison\n",
    "baseline_mae = mean_absolute_error(y_test_reg, [y_train_reg.mean()] * len(y_test_reg))\n",
    "print(f\"\\nðŸ“ Baseline Comparison (predicting mean CLV):\")\n",
    "print(f\"   Baseline MAE: ${baseline_mae:.2f}\")\n",
    "print(f\"   Best Model MAE: ${best_mae:.2f}\")\n",
    "improvement = ((baseline_mae - best_mae) / baseline_mae) * 100\n",
    "print(f\"   Improvement: {improvement:.1f}% better than baseline\")\n",
    "\n",
    "# Show prediction accuracy ranges\n",
    "print(f\"\\nðŸŽ¯ Prediction Accuracy Interpretation:\")\n",
    "print(f\"â€¢ Our best model is typically off by ${best_mae:.0f} when predicting CLV\")\n",
    "print(f\"â€¢ For a customer with ${y_test_reg.mean():.0f} actual CLV:\")\n",
    "print(f\"  - Prediction range: ${y_test_reg.mean() - best_mae:.0f} to ${y_test_reg.mean() + best_mae:.0f}\")\n",
    "print(f\"  - That's Â±{(best_mae / y_test_reg.mean()) * 100:.1f}% relative error\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š REGRESSION MODEL COMPARISON\n",
      "=============================================\n",
      "Model Performance Ranking (by RÂ² Score):\n",
      "                   Model     MAE    RMSE  RÂ²_Score  RÂ²_Percent\n",
      "0      Linear Regression  335.79  371.35      0.55        54.5\n",
      "1  Polynomial Regression  341.27  390.61      0.50        49.7\n",
      "3          Random Forest  340.57  396.81      0.48        48.1\n",
      "2          Decision Tree  359.64  434.08      0.38        37.9\n",
      "\n",
      "ðŸ† Best Regression Model: Linear Regression\n",
      "   RÂ² Score: 0.545 (54.5% variance explained)\n",
      "   Average Error: $335.79\n",
      "\n",
      "ðŸ“ Baseline Comparison (predicting mean CLV):\n",
      "   Baseline MAE: $429.69\n",
      "   Best Model MAE: $335.79\n",
      "   Improvement: 21.9% better than baseline\n",
      "\n",
      "ðŸŽ¯ Prediction Accuracy Interpretation:\n",
      "â€¢ Our best model is typically off by $336 when predicting CLV\n",
      "â€¢ For a customer with $1673 actual CLV:\n",
      "  - Prediction range: $1338 to $2009\n",
      "  - That's Â±20.1% relative error\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.626622Z",
     "start_time": "2026-01-21T22:41:35.607993Z"
    }
   },
   "source": [
    "# Detailed prediction analysis\n",
    "print(\"ðŸ” DETAILED PREDICTION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Compare predictions from all models\n",
    "prediction_comparison = pd.DataFrame({\n",
    "    'Actual_CLV': y_test_reg.iloc[:10].values,\n",
    "    'Linear_Pred': y_pred_linear[:10],\n",
    "    'Poly_Pred': y_pred_poly[:10],\n",
    "    'Tree_Pred': y_pred_tree_reg[:10],\n",
    "    'RF_Pred': y_pred_rf_reg[:10]\n",
    "})\n",
    "\n",
    "# Calculate errors for each model\n",
    "for model in ['Linear', 'Poly', 'Tree', 'RF']:\n",
    "    prediction_comparison[f'{model}_Error'] = (\n",
    "            prediction_comparison['Actual_CLV'] - prediction_comparison[f'{model}_Pred']\n",
    "    ).abs()\n",
    "\n",
    "print(\"Prediction Comparison (first 10 test samples):\")\n",
    "print(prediction_comparison.round(2))\n",
    "\n",
    "# Analyze error patterns\n",
    "print(\"\\nðŸ“ˆ Error Pattern Analysis:\")\n",
    "models_and_preds = [\n",
    "    ('Linear Regression', y_pred_linear),\n",
    "    ('Polynomial Regression', y_pred_poly),\n",
    "    ('Decision Tree', y_pred_tree_reg),\n",
    "    ('Random Forest', y_pred_rf_reg)\n",
    "]\n",
    "\n",
    "for model_name, predictions in models_and_preds:\n",
    "    errors = np.abs(y_test_reg - predictions)\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Mean Error: ${errors.mean():.2f}\")\n",
    "    print(f\"  Median Error: ${errors.median():.2f}\")\n",
    "    print(f\"  Max Error: ${errors.max():.2f}\")\n",
    "    print(f\"  % predictions within $500: {(errors <= 500).mean():.1%}\")\n",
    "    print(f\"  % predictions within $1000: {(errors <= 1000).mean():.1%}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DETAILED PREDICTION ANALYSIS\n",
      "========================================\n",
      "Prediction Comparison (first 10 test samples):\n",
      "   Actual_CLV  Linear_Pred  Poly_Pred  Tree_Pred  RF_Pred  Linear_Error  \\\n",
      "0     1912.55      2100.81    2125.23    2131.59  2063.93        188.26   \n",
      "1     1910.30      2097.42    2003.14    2154.55  2141.17        187.12   \n",
      "2     1887.20      1291.21    1267.58    1268.69  1187.41        595.98   \n",
      "3     1109.40      1410.89    1555.74    1549.14  1315.28        301.48   \n",
      "4     2111.53      1571.47    1727.15    1978.89  1752.66        540.06   \n",
      "5     2320.78      1809.88    1896.33    2154.55  1960.71        510.90   \n",
      "6     1194.04      1378.49    1389.28    1353.20  1456.30        184.45   \n",
      "7      973.45      1207.81     978.08     844.41  1172.59        234.36   \n",
      "8     1781.16      2013.04    1949.97    2131.59  1964.43        231.87   \n",
      "9     1771.82      1208.41    1161.97    1032.51  1133.93        563.41   \n",
      "\n",
      "   Poly_Error  Tree_Error  RF_Error  \n",
      "0      212.67      219.03    151.38  \n",
      "1       92.84      244.25    230.87  \n",
      "2      619.62      618.51    699.79  \n",
      "3      446.34      439.74    205.87  \n",
      "4      384.39      132.64    358.87  \n",
      "5      424.45      166.23    360.07  \n",
      "6      195.24      159.16    262.25  \n",
      "7        4.63      129.04    199.14  \n",
      "8      168.81      350.42    183.27  \n",
      "9      609.85      739.31    637.89  \n",
      "\n",
      "ðŸ“ˆ Error Pattern Analysis:\n",
      "\n",
      "Linear Regression:\n",
      "  Mean Error: $335.79\n",
      "  Median Error: $253.99\n",
      "  Max Error: $663.66\n",
      "  % predictions within $500: 70.0%\n",
      "  % predictions within $1000: 100.0%\n",
      "\n",
      "Polynomial Regression:\n",
      "  Mean Error: $341.27\n",
      "  Median Error: $296.81\n",
      "  Max Error: $876.78\n",
      "  % predictions within $500: 77.0%\n",
      "  % predictions within $1000: 100.0%\n",
      "\n",
      "Decision Tree:\n",
      "  Mean Error: $359.64\n",
      "  Median Error: $305.05\n",
      "  Max Error: $1216.14\n",
      "  % predictions within $500: 74.5%\n",
      "  % predictions within $1000: 98.5%\n",
      "\n",
      "Random Forest:\n",
      "  Mean Error: $340.57\n",
      "  Median Error: $295.23\n",
      "  Max Error: $1003.60\n",
      "  % predictions within $500: 77.5%\n",
      "  % predictions within $1000: 99.5%\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.645133Z",
     "start_time": "2026-01-21T22:41:35.627703Z"
    }
   },
   "source": [
    "# Business impact of regression predictions\n",
    "print(\"ðŸ’° BUSINESS IMPACT OF CLV PREDICTIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"Business Scenario: Marketing Budget Allocation\")\n",
    "print(\"â€¢ High CLV customers (>$3000): Premium marketing ($200 spend)\")\n",
    "print(\"â€¢ Medium CLV customers ($1500-$3000): Standard marketing ($100 spend)\")\n",
    "print(\"â€¢ Low CLV customers (<$1500): Basic marketing ($50 spend)\")\n",
    "print(\"â€¢ Goal: Maximize ROI by targeting right customers with right campaigns\")\n",
    "\n",
    "\n",
    "# Define CLV segments\n",
    "def classify_clv(clv):\n",
    "    if clv >= 3000:\n",
    "        return 'High'\n",
    "    elif clv >= 1500:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "\n",
    "# Marketing costs by segment\n",
    "marketing_costs = {'High': 200, 'Medium': 100, 'Low': 50}\n",
    "\n",
    "print(\"\\nðŸ’¼ Marketing ROI Analysis by Model:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name, predictions in models_and_preds:\n",
    "    # Classify actual and predicted CLV\n",
    "    actual_segments = [classify_clv(clv) for clv in y_test_reg]\n",
    "    predicted_segments = [classify_clv(clv) for clv in predictions]\n",
    "\n",
    "    # Calculate marketing spend based on predictions\n",
    "    predicted_spend = sum(marketing_costs[seg] for seg in predicted_segments)\n",
    "\n",
    "    # Calculate actual ROI (revenue - marketing cost)\n",
    "    actual_revenue = y_test_reg.sum()\n",
    "    roi = actual_revenue - predicted_spend\n",
    "    roi_ratio = actual_revenue / predicted_spend\n",
    "\n",
    "    # Calculate segment accuracy\n",
    "    segment_accuracy = sum(1 for a, p in zip(actual_segments, predicted_segments) if a == p) / len(actual_segments)\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Total marketing spend: ${predicted_spend:,}\")\n",
    "    print(f\"  Total customer revenue: ${actual_revenue:,.0f}\")\n",
    "    print(f\"  Net ROI: ${roi:,.0f}\")\n",
    "    print(f\"  ROI ratio: {roi_ratio:.2f}x\")\n",
    "    print(f\"  Segment classification accuracy: {segment_accuracy:.1%}\")\n",
    "\n",
    "# Optimal allocation (if we knew true CLV)\n",
    "optimal_segments = [classify_clv(clv) for clv in y_test_reg]\n",
    "optimal_spend = sum(marketing_costs[seg] for seg in optimal_segments)\n",
    "optimal_roi = y_test_reg.sum() - optimal_spend\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Optimal Allocation (perfect predictions):\")\n",
    "print(f\"  Marketing spend: ${optimal_spend:,}\")\n",
    "print(f\"  Net ROI: ${optimal_roi:,.0f}\")\n",
    "print(f\"  ROI ratio: {y_test_reg.sum() / optimal_spend:.2f}x\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Better CLV predictions â†’ Better marketing allocation â†’ Higher ROI\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’° BUSINESS IMPACT OF CLV PREDICTIONS\n",
      "=============================================\n",
      "Business Scenario: Marketing Budget Allocation\n",
      "â€¢ High CLV customers (>$3000): Premium marketing ($200 spend)\n",
      "â€¢ Medium CLV customers ($1500-$3000): Standard marketing ($100 spend)\n",
      "â€¢ Low CLV customers (<$1500): Basic marketing ($50 spend)\n",
      "â€¢ Goal: Maximize ROI by targeting right customers with right campaigns\n",
      "\n",
      "ðŸ’¼ Marketing ROI Analysis by Model:\n",
      "--------------------------------------------------\n",
      "\n",
      "Linear Regression:\n",
      "  Total marketing spend: $16,350\n",
      "  Total customer revenue: $334,689\n",
      "  Net ROI: $318,339\n",
      "  ROI ratio: 20.47x\n",
      "  Segment classification accuracy: 73.0%\n",
      "\n",
      "Polynomial Regression:\n",
      "  Total marketing spend: $16,450\n",
      "  Total customer revenue: $334,689\n",
      "  Net ROI: $318,239\n",
      "  ROI ratio: 20.35x\n",
      "  Segment classification accuracy: 73.0%\n",
      "\n",
      "Decision Tree:\n",
      "  Total marketing spend: $16,250\n",
      "  Total customer revenue: $334,689\n",
      "  Net ROI: $318,439\n",
      "  ROI ratio: 20.60x\n",
      "  Segment classification accuracy: 72.0%\n",
      "\n",
      "Random Forest:\n",
      "  Total marketing spend: $16,650\n",
      "  Total customer revenue: $334,689\n",
      "  Net ROI: $318,039\n",
      "  ROI ratio: 20.10x\n",
      "  Segment classification accuracy: 72.0%\n",
      "\n",
      "ðŸŽ¯ Optimal Allocation (perfect predictions):\n",
      "  Marketing spend: $16,350\n",
      "  Net ROI: $318,339\n",
      "  ROI ratio: 20.47x\n",
      "\n",
      "ðŸ’¡ Key Insight: Better CLV predictions â†’ Better marketing allocation â†’ Higher ROI\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clustering: Discovering Customer Segments\n",
    "\n",
    "Now let's explore unsupervised learning with clustering. Unlike classification and regression, we don't have a target variable - we're looking for hidden patterns in the data.\n",
    "\n",
    "**Real-world Clustering Applications:**\n",
    "- Customer segmentation (marketing)\n",
    "- Market research (identifying consumer groups)\n",
    "- Gene sequencing (biology)\n",
    "- Image segmentation (computer vision)\n",
    "- Anomaly detection (fraud, network security)\n",
    "\n",
    "**Our Clustering Problem:**\n",
    "- **Goal**: Discover natural customer segments based on behavior and characteristics\n",
    "- **Features**: Customer demographics and behavior (no target variable!)\n",
    "- **Output**: Group assignments (Cluster 0, 1, 2, etc.)\n",
    "- **Business Value**: Targeted marketing, personalized products, customer insights\n",
    "\n",
    "**Key Differences from Supervised Learning:**\n",
    "- **No labels**: We don't know the \"right\" answer beforehand\n",
    "- **Exploratory**: We're discovering patterns, not predicting outcomes\n",
    "- **Evaluation**: Harder to measure - we use internal metrics and business interpretation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.667834Z",
     "start_time": "2026-01-21T22:41:35.645871Z"
    }
   },
   "source": [
    "# Prepare data for clustering\n",
    "print(\"Preparing Data for Customer Segmentation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select features for clustering (exclude target variables and IDs)\n",
    "clustering_features = [\n",
    "    'age', 'income', 'education_encoded', 'experience_years',\n",
    "    'num_purchases', 'satisfaction_score'\n",
    "    # Note: Excluding region dummies and premium status for unsupervised learning\n",
    "]\n",
    "\n",
    "X_cluster = df_ml[clustering_features].copy()\n",
    "\n",
    "print(f\"Clustering features: {list(X_cluster.columns)}\")\n",
    "print(f\"Number of customers: {X_cluster.shape[0]}\")\n",
    "print(f\"Number of features: {X_cluster.shape[1]}\")\n",
    "\n",
    "# Check data quality\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"Missing values: {X_cluster.isnull().sum().sum()}\")\n",
    "print(f\"Data types: {X_cluster.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Scale features for clustering (very important!)\n",
    "print(\"\\nðŸ”§ SCALING FOR CLUSTERING:\")\n",
    "print(\"â€¢ Clustering algorithms use distance calculations\")\n",
    "print(\"â€¢ Features with larger scales dominate the distance\")\n",
    "print(\"â€¢ Example: Income ($50,000) vs Age (30) - income dominates\")\n",
    "print(\"â€¢ Solution: Scale all features to similar ranges\")\n",
    "\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)\n",
    "X_cluster_scaled = pd.DataFrame(X_cluster_scaled, columns=X_cluster.columns, index=X_cluster.index)\n",
    "\n",
    "print(\"\\nFeature scales BEFORE scaling:\")\n",
    "print(X_cluster.describe().round(2))\n",
    "\n",
    "print(\"\\nFeature scales AFTER scaling:\")\n",
    "print(X_cluster_scaled.describe().round(2))\n",
    "\n",
    "print(\"\\nâœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1 - ready for clustering!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data for Customer Segmentation\n",
      "==================================================\n",
      "Clustering features: ['age', 'income', 'education_encoded', 'experience_years', 'num_purchases', 'satisfaction_score']\n",
      "Number of customers: 1000\n",
      "Number of features: 6\n",
      "\n",
      "Data quality check:\n",
      "Missing values: 0\n",
      "Data types: {dtype('int64'): 3, dtype('float64'): 3}\n",
      "\n",
      "ðŸ”§ SCALING FOR CLUSTERING:\n",
      "â€¢ Clustering algorithms use distance calculations\n",
      "â€¢ Features with larger scales dominate the distance\n",
      "â€¢ Example: Income ($50,000) vs Age (30) - income dominates\n",
      "â€¢ Solution: Scale all features to similar ranges\n",
      "\n",
      "Feature scales BEFORE scaling:\n",
      "           age     income  education_encoded  experience_years  num_purchases  \\\n",
      "count  1000.00    1000.00            1000.00           1000.00        1000.00   \n",
      "mean     34.74   25669.78               1.08              4.71           2.93   \n",
      "std      11.75   13127.74               0.95              4.77           1.58   \n",
      "min      -3.00    5063.46               0.00              0.00           0.00   \n",
      "25%      27.00   16372.67               0.00              1.35           2.00   \n",
      "50%      35.00   22732.23               1.00              3.30           3.00   \n",
      "75%      42.00   31391.59               2.00              6.41           4.00   \n",
      "max      81.00  105754.35               3.00             38.62           9.00   \n",
      "\n",
      "       satisfaction_score  \n",
      "count             1000.00  \n",
      "mean                 3.00  \n",
      "std                  1.11  \n",
      "min                  1.00  \n",
      "25%                  2.09  \n",
      "50%                  3.00  \n",
      "75%                  3.90  \n",
      "max                  4.99  \n",
      "\n",
      "Feature scales AFTER scaling:\n",
      "           age   income  education_encoded  experience_years  num_purchases  \\\n",
      "count  1000.00  1000.00            1000.00           1000.00        1000.00   \n",
      "mean     -0.00     0.00              -0.00             -0.00           0.00   \n",
      "std       1.00     1.00               1.00              1.00           1.00   \n",
      "min      -3.21    -1.57              -1.14             -0.99          -1.86   \n",
      "25%      -0.66    -0.71              -1.14             -0.70          -0.59   \n",
      "50%       0.02    -0.22              -0.09             -0.30           0.04   \n",
      "75%       0.62     0.44               0.97              0.36           0.68   \n",
      "max       3.94     6.10               2.03              7.11           3.85   \n",
      "\n",
      "       satisfaction_score  \n",
      "count             1000.00  \n",
      "mean                -0.00  \n",
      "std                  1.00  \n",
      "min                 -1.81  \n",
      "25%                 -0.82  \n",
      "50%                  0.00  \n",
      "75%                  0.82  \n",
      "max                  1.80  \n",
      "\n",
      "âœ… All features now have meanâ‰ˆ0 and stdâ‰ˆ1 - ready for clustering!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.749761Z",
     "start_time": "2026-01-21T22:41:35.669717Z"
    }
   },
   "source": [
    "# Apply K-Means Clustering\n",
    "print(\"ðŸŽ¯ Algorithm: K-Means Clustering\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# PARAMETER EXPLANATION: K-Means parameters\n",
    "print(\"ALGORITHM EXPLANATION: K-Means Clustering\")\n",
    "print(\"â€¢ What it does: Groups data into k clusters based on similarity\")\n",
    "print(\"â€¢ How it works: Finds k cluster centers that minimize distances to points\")\n",
    "print(\"â€¢ Strengths: Fast, simple, works well with spherical clusters\")\n",
    "print(\"â€¢ Weaknesses: Assumes spherical clusters, sensitive to initialization\")\n",
    "print(\"â€¢ n_clusters: Number of clusters to create\")\n",
    "print(\"â€¢ random_state: Seed for reproducible results\")\n",
    "print(\"â€¢ n_init: Number of random initializations (best result is kept)\")\n",
    "\n",
    "# For simplicity, we'll use 3 clusters (common for customer segmentation)\n",
    "optimal_k = 3\n",
    "print(f\"\\nðŸŽ¯ Using k={optimal_k} clusters for clear customer segments\")\n",
    "\n",
    "# Create and fit K-Means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=optimal_k,\n",
    "    random_state=42,\n",
    "    n_init=10  # Try 10 different initializations\n",
    ")\n",
    "\n",
    "# Fit the model and get cluster assignments\n",
    "cluster_labels = kmeans.fit_predict(X_cluster_scaled)\n",
    "\n",
    "# Add cluster labels to our dataframe\n",
    "df_clustered = df_ml.copy()\n",
    "df_clustered['Cluster'] = cluster_labels\n",
    "\n",
    "print(f\"\\nðŸ“Š Clustering Results:\")\n",
    "print(f\"Number of clusters: {optimal_k}\")\n",
    "print(f\"Final WCSS: {kmeans.inertia_:.2f}\")\n",
    "print(f\"Number of iterations: {kmeans.n_iter_}\")\n",
    "\n",
    "# Show cluster distribution\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(cluster_labels)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully segmented {len(df_clustered)} customers into {optimal_k} clusters!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Algorithm: K-Means Clustering\n",
      "========================================\n",
      "ALGORITHM EXPLANATION: K-Means Clustering\n",
      "â€¢ What it does: Groups data into k clusters based on similarity\n",
      "â€¢ How it works: Finds k cluster centers that minimize distances to points\n",
      "â€¢ Strengths: Fast, simple, works well with spherical clusters\n",
      "â€¢ Weaknesses: Assumes spherical clusters, sensitive to initialization\n",
      "â€¢ n_clusters: Number of clusters to create\n",
      "â€¢ random_state: Seed for reproducible results\n",
      "â€¢ n_init: Number of random initializations (best result is kept)\n",
      "\n",
      "ðŸŽ¯ Using k=3 clusters for clear customer segments\n",
      "\n",
      "ðŸ“Š Clustering Results:\n",
      "Number of clusters: 3\n",
      "Final WCSS: 4715.61\n",
      "Number of iterations: 29\n",
      "\n",
      "Cluster Distribution:\n",
      "Cluster 0: 526 customers (52.6%)\n",
      "Cluster 1: 225 customers (22.5%)\n",
      "Cluster 2: 249 customers (24.9%)\n",
      "\n",
      "âœ… Successfully segmented 1000 customers into 3 clusters!\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.778055Z",
     "start_time": "2026-01-21T22:41:35.752475Z"
    }
   },
   "source": [
    "# Analyze and interpret customer segments\n",
    "print(\"ðŸ” CUSTOMER SEGMENT ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate cluster centers in original scale for interpretation\n",
    "cluster_centers_scaled = kmeans.cluster_centers_\n",
    "cluster_centers_original = scaler_cluster.inverse_transform(cluster_centers_scaled)\n",
    "\n",
    "# Create cluster centers DataFrame\n",
    "cluster_centers_df = pd.DataFrame(\n",
    "    cluster_centers_original,\n",
    "    columns=clustering_features,\n",
    "    index=[f'Cluster_{i}' for i in range(optimal_k)]\n",
    ")\n",
    "\n",
    "print(\"Cluster Centers (Average Values):\")\n",
    "print(cluster_centers_df.round(2))\n",
    "\n",
    "# Detailed analysis by cluster\n",
    "print(\"\\nðŸ“‹ DETAILED CLUSTER PROFILES:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ CLUSTER {cluster_id} PROFILE ({len(cluster_data)} customers):\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Demographics\n",
    "    print(f\"Demographics:\")\n",
    "    print(f\"  Average Age: {cluster_data['age'].mean():.1f} years\")\n",
    "    print(f\"  Average Income: ${cluster_data['income'].mean():.0f}\")\n",
    "    print(f\"  Education: {cluster_data['education'].mode().iloc[0]} (most common)\")\n",
    "\n",
    "    # Behavior\n",
    "    print(f\"Behavior:\")\n",
    "    print(f\"  Average Purchases: {cluster_data['num_purchases'].mean():.1f}\")\n",
    "    print(f\"  Average Satisfaction: {cluster_data['satisfaction_score'].mean():.2f}/5.0\")\n",
    "    print(f\"  Average Experience: {cluster_data['experience_years'].mean():.1f} years\")\n",
    "\n",
    "    # Business metrics\n",
    "    print(f\"Business Value:\")\n",
    "    premium_rate = cluster_data['is_premium'].mean()\n",
    "    avg_clv = cluster_data['customer_lifetime_value'].mean()\n",
    "    print(f\"  Premium Rate: {premium_rate:.1%}\")\n",
    "    print(f\"  Average CLV: ${avg_clv:.0f}\")\n",
    "\n",
    "    # Region distribution\n",
    "    top_region = cluster_data['region'].mode().iloc[0]\n",
    "    region_pct = (cluster_data['region'] == top_region).mean()\n",
    "    print(f\"  Top Region: {top_region} ({region_pct:.1%})\")\n",
    "\n",
    "# Compare clusters side by side\n",
    "print(\"\\nðŸ“Š CLUSTER COMPARISON TABLE:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "comparison_metrics = []\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "\n",
    "    metrics = {\n",
    "        'Cluster': f'Cluster_{cluster_id}',\n",
    "        'Size': len(cluster_data),\n",
    "        'Avg_Age': cluster_data['age'].mean(),\n",
    "        'Avg_Income': cluster_data['income'].mean(),\n",
    "        'Avg_Satisfaction': cluster_data['satisfaction_score'].mean(),\n",
    "        'Premium_Rate': cluster_data['is_premium'].mean(),\n",
    "        'Avg_CLV': cluster_data['customer_lifetime_value'].mean()\n",
    "    }\n",
    "    comparison_metrics.append(metrics)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_metrics)\n",
    "print(comparison_df.round(2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” CUSTOMER SEGMENT ANALYSIS\n",
      "========================================\n",
      "Cluster Centers (Average Values):\n",
      "             age    income  education_encoded  experience_years  \\\n",
      "Cluster_0  34.64  21086.20               0.51              3.37   \n",
      "Cluster_1  32.82  39440.28               1.03              9.13   \n",
      "Cluster_2  36.69  22909.14               2.34              3.55   \n",
      "\n",
      "           num_purchases  satisfaction_score  \n",
      "Cluster_0           2.72                3.05  \n",
      "Cluster_1           3.67                2.88  \n",
      "Cluster_2           2.71                2.99  \n",
      "\n",
      "ðŸ“‹ DETAILED CLUSTER PROFILES:\n",
      "===================================\n",
      "\n",
      "ðŸŽ¯ CLUSTER 0 PROFILE (526 customers):\n",
      "--------------------------------------------------\n",
      "Demographics:\n",
      "  Average Age: 34.6 years\n",
      "  Average Income: $21086\n",
      "  Education: Bachelor (most common)\n",
      "Behavior:\n",
      "  Average Purchases: 2.7\n",
      "  Average Satisfaction: 3.05/5.0\n",
      "  Average Experience: 3.4 years\n",
      "Business Value:\n",
      "  Premium Rate: 29.7%\n",
      "  Average CLV: $1605\n",
      "  Top Region: South (27.2%)\n",
      "\n",
      "ðŸŽ¯ CLUSTER 1 PROFILE (225 customers):\n",
      "--------------------------------------------------\n",
      "Demographics:\n",
      "  Average Age: 32.8 years\n",
      "  Average Income: $39440\n",
      "  Education: Bachelor (most common)\n",
      "Behavior:\n",
      "  Average Purchases: 3.7\n",
      "  Average Satisfaction: 2.88/5.0\n",
      "  Average Experience: 9.1 years\n",
      "Business Value:\n",
      "  Premium Rate: 27.1%\n",
      "  Average CLV: $1674\n",
      "  Top Region: West (28.9%)\n",
      "\n",
      "ðŸŽ¯ CLUSTER 2 PROFILE (249 customers):\n",
      "--------------------------------------------------\n",
      "Demographics:\n",
      "  Average Age: 36.7 years\n",
      "  Average Income: $22909\n",
      "  Education: Master (most common)\n",
      "Behavior:\n",
      "  Average Purchases: 2.7\n",
      "  Average Satisfaction: 2.99/5.0\n",
      "  Average Experience: 3.6 years\n",
      "Business Value:\n",
      "  Premium Rate: 30.9%\n",
      "  Average CLV: $1845\n",
      "  Top Region: South (26.5%)\n",
      "\n",
      "ðŸ“Š CLUSTER COMPARISON TABLE:\n",
      "===================================\n",
      "     Cluster  Size  Avg_Age  Avg_Income  Avg_Satisfaction  Premium_Rate  \\\n",
      "0  Cluster_0   526    34.64    21086.20              3.05          0.30   \n",
      "1  Cluster_1   225    32.82    39440.28              2.88          0.27   \n",
      "2  Cluster_2   249    36.69    22909.14              2.99          0.31   \n",
      "\n",
      "   Avg_CLV  \n",
      "0  1605.42  \n",
      "1  1673.95  \n",
      "2  1844.80  \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:35.799931Z",
     "start_time": "2026-01-21T22:41:35.779262Z"
    }
   },
   "source": [
    "# Business interpretation and actionable insights\n",
    "print(\"ðŸ’¼ BUSINESS INTERPRETATION & MARKETING STRATEGY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Analyze each cluster for business insights\n",
    "cluster_insights = []\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == cluster_id]\n",
    "\n",
    "    # Calculate key metrics\n",
    "    avg_age = cluster_data['age'].mean()\n",
    "    avg_income = cluster_data['income'].mean()\n",
    "    avg_satisfaction = cluster_data['satisfaction_score'].mean()\n",
    "    premium_rate = cluster_data['is_premium'].mean()\n",
    "    avg_clv = cluster_data['customer_lifetime_value'].mean()\n",
    "    size = len(cluster_data)\n",
    "\n",
    "    # Generate business interpretation\n",
    "    if avg_clv > 2500 and premium_rate > 0.4:\n",
    "        segment_type = \"High-Value Customers\"\n",
    "        strategy = \"VIP treatment, loyalty programs, premium services\"\n",
    "        priority = \"HIGH\"\n",
    "    elif avg_clv > 1800 and avg_satisfaction > 3.5:\n",
    "        segment_type = \"Growth Potential\"\n",
    "        strategy = \"Upselling, premium conversion campaigns\"\n",
    "        priority = \"MEDIUM\"\n",
    "    else:\n",
    "        segment_type = \"Standard Customers\"\n",
    "        strategy = \"Retention programs, satisfaction improvement\"\n",
    "        priority = \"LOW\"\n",
    "\n",
    "    cluster_insights.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'segment_type': segment_type,\n",
    "        'strategy': strategy,\n",
    "        'priority': priority,\n",
    "        'size': size,\n",
    "        'avg_clv': avg_clv\n",
    "    })\n",
    "\n",
    "# Display business insights\n",
    "for insight in cluster_insights:\n",
    "    print(f\"\\nðŸŽ¯ CLUSTER {insight['cluster_id']}: {insight['segment_type']}\")\n",
    "    print(f\"   Size: {insight['size']} customers\")\n",
    "    print(f\"   Average CLV: ${insight['avg_clv']:.0f}\")\n",
    "    print(f\"   Priority: {insight['priority']}\")\n",
    "    print(f\"   Strategy: {insight['strategy']}\")\n",
    "\n",
    "# Calculate business impact\n",
    "print(f\"\\nðŸ’° BUSINESS IMPACT ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "total_clv = df_clustered['customer_lifetime_value'].sum()\n",
    "print(f\"Total Customer Value: ${total_clv:,.0f}\")\n",
    "\n",
    "for insight in cluster_insights:\n",
    "    cluster_data = df_clustered[df_clustered['Cluster'] == insight['cluster_id']]\n",
    "    cluster_clv = cluster_data['customer_lifetime_value'].sum()\n",
    "    clv_percentage = (cluster_clv / total_clv) * 100\n",
    "\n",
    "    print(f\"\\nCluster {insight['cluster_id']} ({insight['segment_type']}):\")\n",
    "    print(f\"  Total Value: ${cluster_clv:,.0f} ({clv_percentage:.1f}% of total)\")\n",
    "    print(f\"  Size: {insight['size']} customers ({insight['size'] / len(df_clustered) * 100:.1f}% of base)\")\n",
    "    print(f\"  Value per Customer: ${cluster_clv / insight['size']:.0f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ KEY INSIGHTS:\")\n",
    "print(\"â€¢ Customer segmentation reveals distinct behavioral patterns\")\n",
    "print(\"â€¢ High-value segments deserve premium marketing investment\")\n",
    "print(\"â€¢ Growth potential segments are prime for upselling campaigns\")\n",
    "print(\"â€¢ Targeted strategies can improve overall customer lifetime value\")\n",
    "print(\"â€¢ Regular re-segmentation helps track customer evolution\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¼ BUSINESS INTERPRETATION & MARKETING STRATEGY\n",
      "=======================================================\n",
      "\n",
      "ðŸŽ¯ CLUSTER 0: Standard Customers\n",
      "   Size: 526 customers\n",
      "   Average CLV: $1605\n",
      "   Priority: LOW\n",
      "   Strategy: Retention programs, satisfaction improvement\n",
      "\n",
      "ðŸŽ¯ CLUSTER 1: Standard Customers\n",
      "   Size: 225 customers\n",
      "   Average CLV: $1674\n",
      "   Priority: LOW\n",
      "   Strategy: Retention programs, satisfaction improvement\n",
      "\n",
      "ðŸŽ¯ CLUSTER 2: Standard Customers\n",
      "   Size: 249 customers\n",
      "   Average CLV: $1845\n",
      "   Priority: LOW\n",
      "   Strategy: Retention programs, satisfaction improvement\n",
      "\n",
      "ðŸ’° BUSINESS IMPACT ANALYSIS:\n",
      "------------------------------\n",
      "Total Customer Value: $1,680,444\n",
      "\n",
      "Cluster 0 (Standard Customers):\n",
      "  Total Value: $844,450 (50.3% of total)\n",
      "  Size: 526 customers (52.6% of base)\n",
      "  Value per Customer: $1605\n",
      "\n",
      "Cluster 1 (Standard Customers):\n",
      "  Total Value: $376,639 (22.4% of total)\n",
      "  Size: 225 customers (22.5% of base)\n",
      "  Value per Customer: $1674\n",
      "\n",
      "Cluster 2 (Standard Customers):\n",
      "  Total Value: $459,356 (27.3% of total)\n",
      "  Size: 249 customers (24.9% of base)\n",
      "  Value per Customer: $1845\n",
      "\n",
      "ðŸŽ¯ KEY INSIGHTS:\n",
      "â€¢ Customer segmentation reveals distinct behavioral patterns\n",
      "â€¢ High-value segments deserve premium marketing investment\n",
      "â€¢ Growth potential segments are prime for upselling campaigns\n",
      "â€¢ Targeted strategies can improve overall customer lifetime value\n",
      "â€¢ Regular re-segmentation helps track customer evolution\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation and Model Selection\n",
    "\n",
    "Let's dive deeper into cross-validation - a crucial technique for reliable model evaluation that every ML practitioner needs to master.\n",
    "\n",
    "**Why Cross-Validation Matters:**\n",
    "- Single train/test split might be lucky or unlucky\n",
    "- Small datasets need every sample for both training and testing\n",
    "- Provides more reliable performance estimates\n",
    "- Helps detect overfitting and model instability\n",
    "\n",
    "**When to Use Cross-Validation:**\n",
    "- Always for model selection and hyperparameter tuning\n",
    "- When you have limited data (< 10,000 samples)\n",
    "- To compare multiple algorithms fairly\n",
    "- Before deploying models to production"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:36.361514Z",
     "start_time": "2026-01-21T22:41:35.801475Z"
    }
   },
   "source": [
    "# Comprehensive Cross-Validation Analysis\n",
    "print(\"ðŸ”„ COMPREHENSIVE CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# PARAMETER EXPLANATION: Cross-validation types\n",
    "print(\"CROSS-VALIDATION TYPES EXPLAINED:\")\n",
    "print(\"â€¢ K-Fold CV: Split data into k equal parts, train on k-1, test on 1\")\n",
    "print(\"â€¢ Stratified K-Fold: Maintains class distribution in each fold\")\n",
    "print(\"â€¢ Leave-One-Out: Use each sample as test set once (for small datasets)\")\n",
    "print(\"â€¢ Time Series CV: Respects temporal order (for time-dependent data)\")\n",
    "print(\"â€¢ Common k values: 5 (fast), 10 (thorough), depends on dataset size\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Set up cross-validation\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Models to compare with cross-validation\n",
    "cv_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, min_samples_split=20, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Perform comprehensive cross-validation\n",
    "cv_results = {}\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "print(\"\\nðŸ“Š Cross-Validation Results (5-Fold Stratified):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, model in cv_models.items():\n",
    "    # Use appropriate data (scaled for algorithms that need it)\n",
    "    if model_name in ['Logistic Regression', 'K-Nearest Neighbors']:\n",
    "        X_cv = X_train_scaled\n",
    "    else:\n",
    "        X_cv = X_train\n",
    "\n",
    "    # Perform cross-validation with multiple metrics\n",
    "    cv_scores = cross_validate(model, X_cv, y_train, cv=cv_folds,\n",
    "                               scoring=scoring_metrics, return_train_score=True)\n",
    "\n",
    "    cv_results[model_name] = cv_scores\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric in scoring_metrics:\n",
    "        test_scores = cv_scores[f'test_{metric}']\n",
    "        train_scores = cv_scores[f'train_{metric}']\n",
    "\n",
    "        print(f\"  {metric.capitalize()}:\")\n",
    "        print(f\"    CV Test:  {test_scores.mean():.3f} Â± {test_scores.std():.3f}\")\n",
    "        print(f\"    CV Train: {train_scores.mean():.3f} Â± {train_scores.std():.3f}\")\n",
    "\n",
    "        # Check for overfitting\n",
    "        overfitting = train_scores.mean() - test_scores.mean()\n",
    "        if overfitting > 0.05:  # 5% gap indicates potential overfitting\n",
    "            print(f\"    âš ï¸ Potential overfitting: {overfitting:.3f} gap\")\n",
    "        else:\n",
    "            print(f\"    âœ… Good generalization: {overfitting:.3f} gap\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\nðŸ† MODEL RANKING BY CV ACCURACY:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "model_rankings = []\n",
    "for model_name, scores in cv_results.items():\n",
    "    accuracy_mean = scores['test_accuracy'].mean()\n",
    "    accuracy_std = scores['test_accuracy'].std()\n",
    "    model_rankings.append((model_name, accuracy_mean, accuracy_std))\n",
    "\n",
    "# Sort by accuracy\n",
    "model_rankings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (model_name, acc_mean, acc_std) in enumerate(model_rankings, 1):\n",
    "    print(f\"{i}. {model_name}: {acc_mean:.3f} Â± {acc_std:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Cross-Validation Insights:\")\n",
    "print(\"â€¢ Lower standard deviation = more consistent performance\")\n",
    "print(\"â€¢ Small train-test gap = good generalization\")\n",
    "print(\"â€¢ Use CV results for final model selection\")\n",
    "print(\"â€¢ Always validate on separate holdout set before deployment\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ COMPREHENSIVE CROSS-VALIDATION ANALYSIS\n",
      "=======================================================\n",
      "CROSS-VALIDATION TYPES EXPLAINED:\n",
      "â€¢ K-Fold CV: Split data into k equal parts, train on k-1, test on 1\n",
      "â€¢ Stratified K-Fold: Maintains class distribution in each fold\n",
      "â€¢ Leave-One-Out: Use each sample as test set once (for small datasets)\n",
      "â€¢ Time Series CV: Respects temporal order (for time-dependent data)\n",
      "â€¢ Common k values: 5 (fast), 10 (thorough), depends on dataset size\n",
      "\n",
      "ðŸ“Š Cross-Validation Results (5-Fold Stratified):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "  Accuracy:\n",
      "    CV Test:  0.705 Â± 0.003\n",
      "    CV Train: 0.706 Â± 0.001\n",
      "    âœ… Good generalization: 0.001 gap\n",
      "  Precision:\n",
      "    CV Test:  0.000 Â± 0.000\n",
      "    CV Train: 0.100 Â± 0.200\n",
      "    âš ï¸ Potential overfitting: 0.100 gap\n",
      "  Recall:\n",
      "    CV Test:  0.000 Â± 0.000\n",
      "    CV Train: 0.001 Â± 0.002\n",
      "    âœ… Good generalization: 0.001 gap\n",
      "  F1:\n",
      "    CV Test:  0.000 Â± 0.000\n",
      "    CV Train: 0.002 Â± 0.004\n",
      "    âœ… Good generalization: 0.002 gap\n",
      "\n",
      "Decision Tree:\n",
      "  Accuracy:\n",
      "    CV Test:  0.679 Â± 0.017\n",
      "    CV Train: 0.738 Â± 0.003\n",
      "    âš ï¸ Potential overfitting: 0.059 gap\n",
      "  Precision:\n",
      "    CV Test:  0.257 Â± 0.175\n",
      "    CV Train: 0.783 Â± 0.060\n",
      "    âš ï¸ Potential overfitting: 0.526 gap\n",
      "  Recall:\n",
      "    CV Test:  0.060 Â± 0.043\n",
      "    CV Train: 0.152 Â± 0.021\n",
      "    âš ï¸ Potential overfitting: 0.093 gap\n",
      "  F1:\n",
      "    CV Test:  0.096 Â± 0.069\n",
      "    CV Train: 0.253 Â± 0.027\n",
      "    âš ï¸ Potential overfitting: 0.157 gap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest:\n",
      "  Accuracy:\n",
      "    CV Test:  0.705 Â± 0.003\n",
      "    CV Train: 0.721 Â± 0.004\n",
      "    âœ… Good generalization: 0.016 gap\n",
      "  Precision:\n",
      "    CV Test:  0.000 Â± 0.000\n",
      "    CV Train: 1.000 Â± 0.000\n",
      "    âš ï¸ Potential overfitting: 1.000 gap\n",
      "  Recall:\n",
      "    CV Test:  0.000 Â± 0.000\n",
      "    CV Train: 0.050 Â± 0.012\n",
      "    âœ… Good generalization: 0.050 gap\n",
      "  F1:\n",
      "    CV Test:  0.000 Â± 0.000\n",
      "    CV Train: 0.095 Â± 0.022\n",
      "    âš ï¸ Potential overfitting: 0.095 gap\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "  Accuracy:\n",
      "    CV Test:  0.613 Â± 0.010\n",
      "    CV Train: 0.733 Â± 0.006\n",
      "    âš ï¸ Potential overfitting: 0.121 gap\n",
      "  Precision:\n",
      "    CV Test:  0.223 Â± 0.038\n",
      "    CV Train: 0.593 Â± 0.020\n",
      "    âš ï¸ Potential overfitting: 0.370 gap\n",
      "  Recall:\n",
      "    CV Test:  0.136 Â± 0.053\n",
      "    CV Train: 0.295 Â± 0.015\n",
      "    âš ï¸ Potential overfitting: 0.159 gap\n",
      "  F1:\n",
      "    CV Test:  0.167 Â± 0.051\n",
      "    CV Train: 0.394 Â± 0.017\n",
      "    âš ï¸ Potential overfitting: 0.226 gap\n",
      "\n",
      "ðŸ† MODEL RANKING BY CV ACCURACY:\n",
      "-----------------------------------\n",
      "1. Logistic Regression: 0.705 Â± 0.003\n",
      "2. Random Forest: 0.705 Â± 0.003\n",
      "3. Decision Tree: 0.679 Â± 0.017\n",
      "4. K-Nearest Neighbors: 0.613 Â± 0.010\n",
      "\n",
      "ðŸ’¡ Cross-Validation Insights:\n",
      "â€¢ Lower standard deviation = more consistent performance\n",
      "â€¢ Small train-test gap = good generalization\n",
      "â€¢ Use CV results for final model selection\n",
      "â€¢ Always validate on separate holdout set before deployment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karthik/Projects/Coursework/Machine-Learning-101/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are the settings you configure before training (like max_depth for trees). Tuning them can significantly improve model performance.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Parameters**: Learned during training (weights, coefficients)\n",
    "- **Hyperparameters**: Set before training (learning rate, tree depth)\n",
    "- **Grid Search**: Try all combinations of hyperparameter values\n",
    "- **Random Search**: Try random combinations (often more efficient)\n",
    "- **Validation**: Always use cross-validation for hyperparameter tuning\n",
    "\n",
    "**Common Hyperparameters by Algorithm:**\n",
    "- **Random Forest**: n_estimators, max_depth, min_samples_split\n",
    "- **Logistic Regression**: C (regularization), penalty type\n",
    "- **KNN**: n_neighbors, weights, distance metric"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:40.267646Z",
     "start_time": "2026-01-21T22:41:36.439172Z"
    }
   },
   "source": [
    "# Hyperparameter Tuning Example\n",
    "print(\"ðŸ”§ HYPERPARAMETER TUNING EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# PARAMETER EXPLANATION: GridSearchCV\n",
    "print(\"GRID SEARCH EXPLANATION:\")\n",
    "print(\"â€¢ What it does: Tries all combinations of hyperparameter values\")\n",
    "print(\"â€¢ How it works: Uses cross-validation to evaluate each combination\")\n",
    "print(\"â€¢ Pros: Thorough, finds optimal combination\")\n",
    "print(\"â€¢ Cons: Can be slow with many parameters\")\n",
    "print(\"â€¢ cv parameter: Number of cross-validation folds\")\n",
    "print(\"â€¢ scoring: Metric to optimize (accuracy, f1, etc.)\")\n",
    "\n",
    "# Example: Tune Random Forest hyperparameters\n",
    "print(\"\\nðŸŒ² Tuning Random Forest Hyperparameters:\")\n",
    "\n",
    "# Define hyperparameter grid (start small for demo)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [3, 5, 7, None],  # Tree depth\n",
    "    'min_samples_split': [10, 20, 50],  # Min samples to split\n",
    "    'min_samples_leaf': [5, 10, 20]  # Min samples in leaf\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Hyperparameter combinations to test: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])}\")\n",
    "\n",
    "# Perform grid search (using smaller grid for speed)\n",
    "small_param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'min_samples_split': [10, 20]\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"\\nActual combinations tested: {len(small_param_grid['n_estimators']) * len(small_param_grid['max_depth']) * len(small_param_grid['min_samples_split'])}\")\n",
    "\n",
    "# Create GridSearchCV\n",
    "rf_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=small_param_grid,\n",
    "    cv=3,  # 3-fold CV for speed\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit grid search\n",
    "print(\"\\nRunning grid search (this may take a moment...)\")\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(f\"\\nðŸ“Š Grid Search Results:\")\n",
    "print(f\"Best CV Score: {rf_grid_search.best_score_:.3f}\")\n",
    "print(f\"Best Parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# Compare with default model\n",
    "default_rf = RandomForestClassifier(random_state=42)\n",
    "default_scores = cross_val_score(default_rf, X_train, y_train, cv=3)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Improvement Analysis:\")\n",
    "print(f\"Default RF CV Score: {default_scores.mean():.3f} Â± {default_scores.std():.3f}\")\n",
    "print(f\"Tuned RF CV Score:   {rf_grid_search.best_score_:.3f}\")\n",
    "improvement = rf_grid_search.best_score_ - default_scores.mean()\n",
    "print(f\"Improvement: {improvement:.3f} ({improvement * 100:.1f} percentage points)\")\n",
    "\n",
    "# Test the tuned model\n",
    "tuned_predictions = rf_grid_search.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, tuned_predictions)\n",
    "print(f\"\\nTuned Model Test Accuracy: {tuned_accuracy:.3f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Hyperparameter Tuning Tips:\")\n",
    "print(\"â€¢ Start with wide ranges, then narrow down\")\n",
    "print(\"â€¢ Use RandomizedSearchCV for large parameter spaces\")\n",
    "print(\"â€¢ Always validate on separate test set\")\n",
    "print(\"â€¢ Consider computational cost vs. performance gain\")\n",
    "print(\"â€¢ Document your tuning process for reproducibility\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ HYPERPARAMETER TUNING EXAMPLE\n",
      "========================================\n",
      "GRID SEARCH EXPLANATION:\n",
      "â€¢ What it does: Tries all combinations of hyperparameter values\n",
      "â€¢ How it works: Uses cross-validation to evaluate each combination\n",
      "â€¢ Pros: Thorough, finds optimal combination\n",
      "â€¢ Cons: Can be slow with many parameters\n",
      "â€¢ cv parameter: Number of cross-validation folds\n",
      "â€¢ scoring: Metric to optimize (accuracy, f1, etc.)\n",
      "\n",
      "ðŸŒ² Tuning Random Forest Hyperparameters:\n",
      "Hyperparameter combinations to test: 108\n",
      "\n",
      "Actual combinations tested: 12\n",
      "\n",
      "Running grid search (this may take a moment...)\n",
      "\n",
      "ðŸ“Š Grid Search Results:\n",
      "Best CV Score: 0.705\n",
      "Best Parameters: {'max_depth': 3, 'min_samples_split': 20, 'n_estimators': 50}\n",
      "\n",
      "ðŸ“ˆ Improvement Analysis:\n",
      "Default RF CV Score: 0.681 Â± 0.028\n",
      "Tuned RF CV Score:   0.705\n",
      "Improvement: 0.024 (2.4 percentage points)\n",
      "\n",
      "Tuned Model Test Accuracy: 0.705\n",
      "\n",
      "ðŸŽ¯ Hyperparameter Tuning Tips:\n",
      "â€¢ Start with wide ranges, then narrow down\n",
      "â€¢ Use RandomizedSearchCV for large parameter spaces\n",
      "â€¢ Always validate on separate test set\n",
      "â€¢ Consider computational cost vs. performance gain\n",
      "â€¢ Document your tuning process for reproducibility\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Interpretation and Feature Importance\n",
    "\n",
    "Understanding what your models learned is crucial for trust, debugging, and business insights. Let's explore different ways to interpret ML models.\n",
    "\n",
    "**Why Model Interpretation Matters:**\n",
    "- **Trust**: Understand why the model makes certain predictions\n",
    "- **Debugging**: Identify if the model learned the right patterns\n",
    "- **Business Insights**: Discover which factors drive outcomes\n",
    "- **Compliance**: Some industries require explainable models\n",
    "- **Feature Selection**: Remove irrelevant or harmful features\n",
    "\n",
    "**Types of Interpretability:**\n",
    "- **Global**: How the model works overall\n",
    "- **Local**: Why a specific prediction was made\n",
    "- **Model-specific**: Built-in interpretability (tree rules, coefficients)\n",
    "- **Model-agnostic**: Works with any model (SHAP, LIME)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:41:40.457019Z",
     "start_time": "2026-01-21T22:41:40.322215Z"
    }
   },
   "source": [
    "# Comprehensive Feature Importance Analysis\n",
    "print(\"ðŸ” COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Use our best tuned model\n",
    "best_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# 1. Built-in Feature Importance\n",
    "print(\"1ï¸âƒ£ BUILT-IN FEATURE IMPORTANCE (Random Forest):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_model.feature_importances_,\n",
    "    'Importance_Pct': best_model.feature_importances_ * 100\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.round(4))\n",
    "\n",
    "# Visualize top features\n",
    "print(\"\\nðŸ“Š Top 5 Most Important Features:\")\n",
    "top_features = feature_importance_df.head(5)\n",
    "for idx, row in top_features.iterrows():\n",
    "    bar_length = int(row['Importance_Pct'] / 2)  # Scale for display\n",
    "    bar = 'â–ˆ' * bar_length\n",
    "    print(f\"{row['Feature']:<20} {bar} {row['Importance_Pct']:.1f}%\")\n",
    "\n",
    "# 2. Permutation Importance (more reliable)\n",
    "print(\"\\n2ï¸âƒ£ PERMUTATION IMPORTANCE (More Reliable):\")\n",
    "print(\"-\" * 45)\n",
    "print(\"â€¢ Shuffles each feature and measures performance drop\")\n",
    "print(\"â€¢ More reliable than built-in importance\")\n",
    "print(\"â€¢ Shows actual impact on model predictions\")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "perm_importance = permutation_importance(\n",
    "    best_model, X_test, y_test,\n",
    "    n_repeats=5, random_state=42, scoring='accuracy'\n",
    ")\n",
    "\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance_Mean': perm_importance.importances_mean,\n",
    "    'Importance_Std': perm_importance.importances_std\n",
    "}).sort_values('Importance_Mean', ascending=False)\n",
    "\n",
    "print(\"\\nPermutation Importance Results:\")\n",
    "print(perm_importance_df.round(4))\n",
    "\n",
    "# 3. Feature Importance Interpretation\n",
    "print(\"\\n3ï¸âƒ£ BUSINESS INTERPRETATION:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get top 3 features for interpretation\n",
    "top_3_features = perm_importance_df.head(3)['Feature'].tolist()\n",
    "\n",
    "feature_interpretations = {\n",
    "    'satisfaction_score': 'Customer satisfaction is the strongest predictor - happy customers become premium',\n",
    "    'income': 'Higher income customers are more likely to afford premium services',\n",
    "    'num_purchases': 'Purchase history indicates engagement and likelihood to upgrade',\n",
    "    'age': 'Age correlates with disposable income and service preferences',\n",
    "    'education_encoded': 'Education level affects income and technology adoption',\n",
    "    'experience_years': 'Experience indicates established relationship with company'\n",
    "}\n",
    "\n",
    "print(\"Key Business Insights:\")\n",
    "for i, feature in enumerate(top_3_features, 1):\n",
    "    interpretation = feature_interpretations.get(feature, 'Important predictor for premium status')\n",
    "    print(f\"{i}. {feature}: {interpretation}\")\n",
    "\n",
    "# 4. Feature Selection Recommendations\n",
    "print(\"\\n4ï¸âƒ£ FEATURE SELECTION RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Identify low-importance features\n",
    "low_importance = perm_importance_df[perm_importance_df['Importance_Mean'] < 0.01]\n",
    "\n",
    "if len(low_importance) > 0:\n",
    "    print(\"Features with very low importance (consider removing):\")\n",
    "    for feature in low_importance['Feature']:\n",
    "        print(f\"â€¢ {feature}\")\n",
    "    print(\"\\nBenefits of removing low-importance features:\")\n",
    "    print(\"â€¢ Faster training and prediction\")\n",
    "    print(\"â€¢ Reduced overfitting risk\")\n",
    "    print(\"â€¢ Simpler model maintenance\")\n",
    "    print(\"â€¢ Lower data collection costs\")\n",
    "else:\n",
    "    print(\"âœ… All features show meaningful importance - keep current feature set\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Model Interpretation Best Practices:\")\n",
    "print(\"â€¢ Use multiple interpretation methods for validation\")\n",
    "print(\"â€¢ Consider business context when interpreting features\")\n",
    "print(\"â€¢ Test feature removal impact before final decisions\")\n",
    "print(\"â€¢ Document interpretation findings for stakeholders\")\n",
    "print(\"â€¢ Regularly re-evaluate feature importance as data changes\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” COMPREHENSIVE FEATURE IMPORTANCE ANALYSIS\n",
      "=======================================================\n",
      "1ï¸âƒ£ BUILT-IN FEATURE IMPORTANCE (Random Forest):\n",
      "--------------------------------------------------\n",
      "              Feature  Importance  Importance_Pct\n",
      "1              income      0.2578         25.7834\n",
      "5  satisfaction_score      0.2001         20.0113\n",
      "3    experience_years      0.1935         19.3505\n",
      "0                 age      0.1384         13.8399\n",
      "4       num_purchases      0.1054         10.5364\n",
      "2   education_encoded      0.0451          4.5115\n",
      "6         region_East      0.0214          2.1426\n",
      "8        region_South      0.0189          1.8879\n",
      "9         region_West      0.0125          1.2474\n",
      "7        region_North      0.0069          0.6891\n",
      "\n",
      "ðŸ“Š Top 5 Most Important Features:\n",
      "income               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 25.8%\n",
      "satisfaction_score   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20.0%\n",
      "experience_years     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 19.4%\n",
      "age                  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 13.8%\n",
      "num_purchases        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10.5%\n",
      "\n",
      "2ï¸âƒ£ PERMUTATION IMPORTANCE (More Reliable):\n",
      "---------------------------------------------\n",
      "â€¢ Shuffles each feature and measures performance drop\n",
      "â€¢ More reliable than built-in importance\n",
      "â€¢ Shows actual impact on model predictions\n",
      "\n",
      "Permutation Importance Results:\n",
      "              Feature  Importance_Mean  Importance_Std\n",
      "0                 age              0.0             0.0\n",
      "1              income              0.0             0.0\n",
      "2   education_encoded              0.0             0.0\n",
      "3    experience_years              0.0             0.0\n",
      "4       num_purchases              0.0             0.0\n",
      "5  satisfaction_score              0.0             0.0\n",
      "6         region_East              0.0             0.0\n",
      "7        region_North              0.0             0.0\n",
      "8        region_South              0.0             0.0\n",
      "9         region_West              0.0             0.0\n",
      "\n",
      "3ï¸âƒ£ BUSINESS INTERPRETATION:\n",
      "------------------------------\n",
      "Key Business Insights:\n",
      "1. age: Age correlates with disposable income and service preferences\n",
      "2. income: Higher income customers are more likely to afford premium services\n",
      "3. education_encoded: Education level affects income and technology adoption\n",
      "\n",
      "4ï¸âƒ£ FEATURE SELECTION RECOMMENDATIONS:\n",
      "----------------------------------------\n",
      "Features with very low importance (consider removing):\n",
      "â€¢ age\n",
      "â€¢ income\n",
      "â€¢ education_encoded\n",
      "â€¢ experience_years\n",
      "â€¢ num_purchases\n",
      "â€¢ satisfaction_score\n",
      "â€¢ region_East\n",
      "â€¢ region_North\n",
      "â€¢ region_South\n",
      "â€¢ region_West\n",
      "\n",
      "Benefits of removing low-importance features:\n",
      "â€¢ Faster training and prediction\n",
      "â€¢ Reduced overfitting risk\n",
      "â€¢ Simpler model maintenance\n",
      "â€¢ Lower data collection costs\n",
      "\n",
      "ðŸ’¡ Model Interpretation Best Practices:\n",
      "â€¢ Use multiple interpretation methods for validation\n",
      "â€¢ Consider business context when interpreting features\n",
      "â€¢ Test feature removal impact before final decisions\n",
      "â€¢ Document interpretation findings for stakeholders\n",
      "â€¢ Regularly re-evaluate feature importance as data changes\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through machine learning with NumPy, Pandas, and scikit-learn. Let's summarize what you've accomplished and outline your path forward.\n",
    "\n",
    "## ðŸŽ‰ **What You've Mastered**\n",
    "\n",
    "### **NumPy Foundation**\n",
    "- âœ… Array creation, manipulation, and broadcasting\n",
    "- âœ… Linear algebra operations for ML\n",
    "- âœ… Performance optimization techniques\n",
    "- âœ… Integration with ML libraries\n",
    "\n",
    "### **Pandas Data Processing**\n",
    "- âœ… Real-world data cleaning and preprocessing\n",
    "- âœ… Feature engineering and categorical encoding\n",
    "- âœ… Handling missing values and outliers\n",
    "- âœ… Data splitting and validation preparation\n",
    "\n",
    "### **Scikit-learn Machine Learning**\n",
    "- âœ… **Classification**: Customer premium prediction (4 algorithms)\n",
    "- âœ… **Regression**: Customer lifetime value prediction (4 algorithms)\n",
    "- âœ… **Clustering**: Customer segmentation (unsupervised learning)\n",
    "- âœ… **Model Evaluation**: Comprehensive metrics and business impact\n",
    "- âœ… **Cross-Validation**: Reliable model selection techniques\n",
    "- âœ… **Hyperparameter Tuning**: Model optimization strategies\n",
    "- âœ… **Model Interpretation**: Understanding feature importance\n",
    "\n",
    "## ðŸš€ **You're Now Ready For**\n",
    "\n",
    "### **Immediate Applications**\n",
    "- Build ML models for real business problems\n",
    "- Participate in data science projects at work\n",
    "- Contribute to ML discussions with confidence\n",
    "- Start personal ML projects with your own data\n",
    "\n",
    "### **Advanced Topics to Explore Next**\n",
    "1. **Deep Learning**: Neural networks with TensorFlow/PyTorch\n",
    "2. **Specialized ML**: NLP, computer vision, time series\n",
    "3. **Ensemble Methods**: XGBoost, LightGBM, stacking\n",
    "4. **MLOps**: Model deployment, monitoring, and maintenance\n",
    "5. **Advanced Evaluation**: A/B testing, causal inference\n",
    "\n",
    "## ðŸ“š **Recommended Learning Path**\n",
    "\n",
    "### **Beginner â†’ Intermediate (Next 3-6 months)**\n",
    "1. **Practice Projects**: Apply these skills to 2-3 personal datasets\n",
    "2. **Kaggle Competitions**: Start with beginner-friendly competitions\n",
    "3. **Specialized Libraries**: Explore XGBoost, Seaborn for visualization\n",
    "4. **Advanced Pandas**: Time series analysis, advanced groupby operations\n",
    "\n",
    "### **Intermediate â†’ Advanced (6-12 months)**\n",
    "1. **Deep Learning**: Start with image classification or NLP projects\n",
    "2. **Production Skills**: Learn Docker, cloud deployment (AWS/GCP)\n",
    "3. **Advanced Statistics**: Bayesian methods, experimental design\n",
    "4. **Domain Specialization**: Choose an area (finance, healthcare, etc.)\n",
    "\n",
    "## ðŸŽ¯ **Key Takeaways for Success**\n",
    "\n",
    "### **Technical Skills**\n",
    "- **Always start with data exploration** before building models\n",
    "- **Cross-validation is essential** for reliable model evaluation\n",
    "- **Feature engineering often matters more** than algorithm choice\n",
    "- **Business context drives** technical decisions\n",
    "\n",
    "### **Best Practices**\n",
    "- **Document your process** for reproducibility\n",
    "- **Start simple** then add complexity gradually\n",
    "- **Validate assumptions** with data exploration\n",
    "- **Communicate results** in business terms\n",
    "\n",
    "### **Continuous Learning**\n",
    "- **Stay updated** with new libraries and techniques\n",
    "- **Practice regularly** with diverse datasets\n",
    "- **Join communities** (Reddit r/MachineLearning, Stack Overflow)\n",
    "- **Read research papers** to understand cutting-edge methods\n",
    "\n",
    "## ðŸŒŸ **Final Words**\n",
    "\n",
    "You've built a solid foundation in machine learning that will serve you well throughout your career. The combination of NumPy's computational power, Pandas' data manipulation capabilities, and scikit-learn's ML algorithms gives you the tools to tackle most real-world ML problems.\n",
    "\n",
    "Remember: **Machine learning is as much about asking the right questions as it is about building models.** Focus on understanding the business problem, exploring the data thoroughly, and communicating your findings clearly.\n",
    "\n",
    "**Keep practicing, stay curious, and happy machine learning!** ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*These notebooks represent your first step into the exciting world of machine learning. The skills you've learned here will grow and evolve as you tackle new challenges and explore advanced techniques. Welcome to the ML community!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
