{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas for Machine Learning (Beginner-friendly)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master DataFrame operations for real-world ML data preprocessing\n",
    "- Clean, transform, and engineer features from messy datasets\n",
    "- Handle missing values, outliers, and categorical data for ML pipelines\n",
    "- Prepare clean, ML-ready datasets and convert to NumPy arrays for modeling\n",
    "\n",
    "**Prerequisites:** Python basics, NumPy fundamentals (complete NumPy notebook first)\n",
    "\n",
    "**Estimated Time:** ~60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "Pandas is the essential library for working with structured data in Python and forms the data preprocessing backbone of most ML projects. This notebook focuses on practical, hands-on examples that prepare you for real-world ML workflows.\n",
    "\n",
    "**Why Pandas for ML?** Most real-world data comes as CSV files, databases, or APIs - not clean NumPy arrays. Pandas bridges this gap by:\n",
    "- Loading data from various sources (CSV, Excel, databases, APIs)\n",
    "- Handling mixed data types (numbers, text, dates) in a single structure\n",
    "- Providing powerful tools for data cleaning and transformation\n",
    "- Seamlessly converting to NumPy arrays for ML algorithms\n",
    "\n",
    "**Learning Path Connection:** This notebook builds on NumPy concepts (arrays, indexing, broadcasting) and prepares clean datasets for the scikit-learn notebook where you'll build ML models.\n",
    "\n",
    "**What You'll Build:** By the end, you'll transform a messy real-world dataset into a clean, ML-ready format - exactly what you do in every ML project!\n",
    "\n",
    "**ðŸŽ¯ Success Indicators:** By the end, you should be able to:\n",
    "- Load and explore real-world messy datasets\n",
    "- Handle missing values and outliers confidently\n",
    "- Engineer features that improve model performance\n",
    "- Convert DataFrames to NumPy arrays for ML models\n",
    "\n",
    "**ðŸ’¡ Beginner Tips:**\n",
    "- Data cleaning is 80% of ML work - embrace the messiness!\n",
    "- Always explore your data before cleaning (use .info(), .describe())\n",
    "- Make copies before modifying data (df_clean = df.copy())\n",
    "- Parameter explanations are your friend - refer back to them often!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.151978Z",
     "start_time": "2026-01-21T22:38:26.136035Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PARAMETER EXPLANATION: Random seed (same as NumPy notebook!)\n",
    "# â€¢ What it does: Makes random number generation predictable and reproducible\n",
    "# â€¢ Why seed=42: Reference to \"The Hitchhiker's Guide to the Galaxy\" (the answer to everything!)\n",
    "# â€¢ ML importance: Essential for reproducible experiments and consistent results\n",
    "# â€¢ Data science workflow: Always set seed when generating synthetic data\n",
    "# â€¢ Debugging: Helps isolate issues by ensuring same 'random' data every time\n",
    "# â€¢ Alternative values: Any integer works (0, 123, 2024, your birthday, etc.)\n",
    "np.random.seed(42)\n",
    "\n",
    "# PARAMETER EXPLANATION: Pandas display options\n",
    "# â€¢ max_columns: Maximum number of columns to display before truncating\n",
    "# â€¢ max_rows: Maximum number of rows to display before truncating  \n",
    "# â€¢ Why limit: Prevents overwhelming output with large DataFrames\n",
    "# â€¢ Jupyter tip: Keeps notebook output clean and readable\n",
    "# â€¢ Can also use: pd.set_option('display.width', 1000) for wider display\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Random seed set to 42 - our synthetic data will be reproducible!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.5\n",
      "Random seed set to 42 - our synthetic data will be reproducible!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataFrame Creation and Basic Operations\n",
    "\n",
    "Understanding how to create and manipulate DataFrames is fundamental to ML data preprocessing. While NumPy arrays are great for numerical computations, real-world data is messy and mixed - that's where Pandas shines!\n",
    "\n",
    "**Key Difference from NumPy:**\n",
    "- **NumPy arrays**: Homogeneous data (all same type), great for math operations\n",
    "- **Pandas DataFrames**: Heterogeneous data (mixed types), great for real-world datasets\n",
    "- **Under the hood**: DataFrames use NumPy arrays for each column!\n",
    "\n",
    "**ML Context:** Most ML projects start with loading CSV files, database tables, or API responses into DataFrames for cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.210592Z",
     "start_time": "2026-01-21T22:38:26.152955Z"
    }
   },
   "source": [
    "# CREATING A REALISTIC ML DATASET\n",
    "# This simulates what you'd get from a CSV file or database in a real ML project\n",
    "print(\"Building a Customer Analytics Dataset (typical ML scenario)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)  # For reproducible results (remember this from NumPy!)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic customer data with mixed data types (like real datasets)\n",
    "data = {\n",
    "    'customer_id': range(1, n_samples + 1),  # Integer IDs\n",
    "    'age': np.random.normal(35, 12, n_samples).astype(int),  # Numerical (continuous)\n",
    "    'income': np.random.lognormal(10, 0.5, n_samples),  # Numerical (skewed)\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'],\n",
    "                                  n_samples, p=[0.3, 0.4, 0.2, 0.1]),  # Categorical (ordinal)\n",
    "    'experience_years': np.random.exponential(5, n_samples),  # Numerical (continuous)\n",
    "    'num_purchases': np.random.poisson(3, n_samples),  # Numerical (count)\n",
    "    'satisfaction_score': np.random.uniform(1, 5, n_samples),  # Numerical (bounded)\n",
    "    'is_premium': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # Binary target\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),  # Categorical\n",
    "    'signup_date': pd.date_range('2020-01-01', periods=n_samples, freq='D')[:n_samples]  # Datetime\n",
    "}\n",
    "\n",
    "# Create DataFrame (this is where Pandas shines vs NumPy!)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce realistic data quality issues\n",
    "print(\"Adding realistic data quality issues (missing values, like real datasets)...\")\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices[:20], 'income'] = np.nan  # Missing income data\n",
    "df.loc[missing_indices[20:40], 'satisfaction_score'] = np.nan  # Missing satisfaction scores\n",
    "\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape} (samples, features)\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Data types: {len(df.dtypes.unique())} different types (mixed data!)\")\n",
    "\n",
    "print(\"\\nThis is exactly what real ML datasets look like:\")\n",
    "print(\"â€¢ Mixed data types (numbers, text, dates)\")\n",
    "print(\"â€¢ Missing values that need handling\")\n",
    "print(\"â€¢ Different scales and distributions\")\n",
    "print(\"â€¢ Categorical variables needing encoding\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Customer Analytics Dataset (typical ML scenario)\n",
      "============================================================\n",
      "Adding realistic data quality issues (missing values, like real datasets)...\n",
      "\n",
      "Dataset Overview:\n",
      "   customer_id  age        income    education  experience_years  \\\n",
      "0            1   40  44341.562353     Bachelor          1.800687   \n",
      "1            2   33  34972.483357  High School          4.143785   \n",
      "2            3   42  22693.077136     Bachelor          8.143228   \n",
      "3            4   53  15939.117886  High School          0.737563   \n",
      "4            5   32  31229.288168       Master          4.345832   \n",
      "\n",
      "   num_purchases  satisfaction_score  is_premium region signup_date  \n",
      "0              3            1.991730           0   East  2020-01-01  \n",
      "1              3            4.711166           0   East  2020-01-02  \n",
      "2              7            4.728536           1  North  2020-01-03  \n",
      "3              3            3.882346           1   East  2020-01-04  \n",
      "4              3            4.063053           0   East  2020-01-05  \n",
      "\n",
      "Dataset shape: (1000, 10) (samples, features)\n",
      "Memory usage: 186.23 KB\n",
      "Data types: 4 different types (mixed data!)\n",
      "\n",
      "This is exactly what real ML datasets look like:\n",
      "â€¢ Mixed data types (numbers, text, dates)\n",
      "â€¢ Missing values that need handling\n",
      "â€¢ Different scales and distributions\n",
      "â€¢ Categorical variables needing encoding\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.236152Z",
     "start_time": "2026-01-21T22:38:26.212009Z"
    }
   },
   "source": [
    "# Basic DataFrame information (essential for ML)\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   customer_id         1000 non-null   int64         \n",
      " 1   age                 1000 non-null   int64         \n",
      " 2   income              980 non-null    float64       \n",
      " 3   education           1000 non-null   object        \n",
      " 4   experience_years    1000 non-null   float64       \n",
      " 5   num_purchases       1000 non-null   int64         \n",
      " 6   satisfaction_score  980 non-null    float64       \n",
      " 7   is_premium          1000 non-null   int64         \n",
      " 8   region              1000 non-null   object        \n",
      " 9   signup_date         1000 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(3), int64(4), object(2)\n",
      "memory usage: 78.3+ KB\n",
      "None\n",
      "\n",
      "Data Types:\n",
      "customer_id                    int64\n",
      "age                            int64\n",
      "income                       float64\n",
      "education                     object\n",
      "experience_years             float64\n",
      "num_purchases                  int64\n",
      "satisfaction_score           float64\n",
      "is_premium                     int64\n",
      "region                        object\n",
      "signup_date           datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      "customer_id            0\n",
      "age                    0\n",
      "income                20\n",
      "education              0\n",
      "experience_years       0\n",
      "num_purchases          0\n",
      "satisfaction_score    20\n",
      "is_premium             0\n",
      "region                 0\n",
      "signup_date            0\n",
      "dtype: int64\n",
      "\n",
      "Basic Statistics:\n",
      "       customer_id          age         income  experience_years  \\\n",
      "count  1000.000000  1000.000000     980.000000       1000.000000   \n",
      "mean    500.500000    34.743000   25729.730374          4.712565   \n",
      "min       1.000000    -3.000000    5063.461821          0.000154   \n",
      "25%     250.750000    27.000000   16266.763557          1.354730   \n",
      "50%     500.500000    35.000000   22732.225225          3.295243   \n",
      "75%     750.250000    42.000000   31711.722205          6.413300   \n",
      "max    1000.000000    81.000000  105754.353809         38.617648   \n",
      "std     288.819436    11.748233   13254.373746          4.769008   \n",
      "\n",
      "       num_purchases  satisfaction_score  is_premium          signup_date  \n",
      "count    1000.000000          980.000000  1000.00000                 1000  \n",
      "mean        2.933000            2.998455     0.29400  2021-05-14 12:00:00  \n",
      "min         0.000000            1.000211     0.00000  2020-01-01 00:00:00  \n",
      "25%         2.000000            2.078501     0.00000  2020-09-06 18:00:00  \n",
      "50%         3.000000            2.989607     0.00000  2021-05-14 12:00:00  \n",
      "75%         4.000000            3.929172     1.00000  2022-01-19 06:00:00  \n",
      "max         9.000000            4.994469     1.00000  2022-09-26 00:00:00  \n",
      "std         1.578291            1.117896     0.45582                  NaN  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Analysis\n",
    "\n",
    "Understanding your data is crucial before building ML models. This is where you discover patterns, identify problems, and make decisions about preprocessing. In the NumPy notebook, we worked with clean arrays - here we deal with real-world messiness!\n",
    "\n",
    "**ML Exploration Checklist:**\n",
    "1. **Data types**: What kind of features do we have?\n",
    "2. **Missing values**: How much data is missing and why?\n",
    "3. **Distributions**: Are features normally distributed or skewed?\n",
    "4. **Correlations**: Which features relate to our target variable?\n",
    "5. **Outliers**: Are there extreme values that need handling?\n",
    "\n",
    "**Connection to NumPy:** Remember the statistical operations from NumPy? Pandas makes them easier with labeled data!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.248383Z",
     "start_time": "2026-01-21T22:38:26.237265Z"
    }
   },
   "source": [
    "# Categorical data analysis\n",
    "print(\"Categorical Data Analysis:\")\n",
    "\n",
    "# Value counts for categorical features\n",
    "print(\"Education distribution:\")\n",
    "print(df['education'].value_counts())\n",
    "\n",
    "# PARAMETER EXPLANATION: normalize=True/False in value_counts()\n",
    "print(\"\\nPARAMETER EXPLANATION: normalize parameter\")\n",
    "print(\"â€¢ What it does: Controls whether to return counts or proportions\")\n",
    "print(\"â€¢ normalize=False (default): Returns raw counts\")\n",
    "print(\"â€¢ normalize=True: Returns proportions (values sum to 1.0)\")\n",
    "print(\"â€¢ ML usage: Proportions help understand class balance and distribution\")\n",
    "print(\"â€¢ Tip: Multiply by 100 to get percentages\")\n",
    "print(\"â€¢ Why useful: Easier to compare distributions across different sized datasets\")\n",
    "\n",
    "print(\"\\nEducation percentages:\")\n",
    "proportions = df['education'].value_counts(normalize=True)\n",
    "percentages = proportions * 100\n",
    "print(f\"Raw proportions (sum={proportions.sum():.1f}):\")\n",
    "print(proportions)\n",
    "print(f\"\\nAs percentages:\")\n",
    "print(percentages)\n",
    "\n",
    "print(\"\\nRegion distribution:\")\n",
    "print(df['region'].value_counts())\n",
    "\n",
    "print(\"\\nPremium customers:\")\n",
    "print(df['is_premium'].value_counts())\n",
    "print(f\"Premium rate: {df['is_premium'].mean():.2%}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Data Analysis:\n",
      "Education distribution:\n",
      "education\n",
      "Bachelor       382\n",
      "High School    315\n",
      "Master         209\n",
      "PhD             94\n",
      "Name: count, dtype: int64\n",
      "\n",
      "PARAMETER EXPLANATION: normalize parameter\n",
      "â€¢ What it does: Controls whether to return counts or proportions\n",
      "â€¢ normalize=False (default): Returns raw counts\n",
      "â€¢ normalize=True: Returns proportions (values sum to 1.0)\n",
      "â€¢ ML usage: Proportions help understand class balance and distribution\n",
      "â€¢ Tip: Multiply by 100 to get percentages\n",
      "â€¢ Why useful: Easier to compare distributions across different sized datasets\n",
      "\n",
      "Education percentages:\n",
      "Raw proportions (sum=1.0):\n",
      "education\n",
      "Bachelor       0.382\n",
      "High School    0.315\n",
      "Master         0.209\n",
      "PhD            0.094\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "As percentages:\n",
      "education\n",
      "Bachelor       38.2\n",
      "High School    31.5\n",
      "Master         20.9\n",
      "PhD             9.4\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Region distribution:\n",
      "region\n",
      "South    271\n",
      "West     260\n",
      "North    235\n",
      "East     234\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Premium customers:\n",
      "is_premium\n",
      "0    706\n",
      "1    294\n",
      "Name: count, dtype: int64\n",
      "Premium rate: 29.40%\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.276116Z",
     "start_time": "2026-01-21T22:38:26.258026Z"
    }
   },
   "source": [
    "# Numerical data analysis\n",
    "print(\"Numerical Data Analysis:\")\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Numerical columns: {list(numerical_cols)}\")\n",
    "\n",
    "# Correlation analysis (important for feature selection)\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "print(\"\\nCorrelation with target (is_premium):\")\n",
    "target_corr = correlation_matrix['is_premium'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# Identify highly correlated features (multicollinearity)\n",
    "print(\"\\nHighly correlated feature pairs (|correlation| > 0.5):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.5:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_val))\n",
    "\n",
    "for col1, col2, corr in high_corr_pairs:\n",
    "    print(f\"{col1} - {col2}: {corr:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Data Analysis:\n",
      "Numerical columns: ['customer_id', 'age', 'income', 'experience_years', 'num_purchases', 'satisfaction_score', 'is_premium']\n",
      "\n",
      "Correlation with target (is_premium):\n",
      "is_premium            1.000000\n",
      "customer_id           0.058631\n",
      "income                0.022244\n",
      "age                   0.008329\n",
      "satisfaction_score   -0.006930\n",
      "experience_years     -0.036799\n",
      "num_purchases        -0.042162\n",
      "Name: is_premium, dtype: float64\n",
      "\n",
      "Highly correlated feature pairs (|correlation| > 0.5):\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.302249Z",
     "start_time": "2026-01-21T22:38:26.278085Z"
    }
   },
   "source": [
    "# Groupby analysis (understanding patterns)\n",
    "print(\"Group Analysis:\")\n",
    "\n",
    "# Analyze by education level\n",
    "education_analysis = df.groupby('education').agg({\n",
    "    'age': ['mean', 'std'],\n",
    "    'income': ['mean', 'median'],\n",
    "    'satisfaction_score': 'mean',\n",
    "    'is_premium': 'mean',\n",
    "    'customer_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "print(\"Analysis by Education Level:\")\n",
    "print(education_analysis)\n",
    "\n",
    "# Analyze by region\n",
    "print(\"\\nAnalysis by Region:\")\n",
    "region_analysis = df.groupby('region').agg({\n",
    "    'income': 'mean',\n",
    "    'is_premium': 'mean',\n",
    "    'satisfaction_score': 'mean'\n",
    "}).round(2)\n",
    "print(region_analysis)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Analysis:\n",
      "Analysis by Education Level:\n",
      "               age           income           satisfaction_score is_premium  \\\n",
      "              mean    std      mean    median               mean       mean   \n",
      "education                                                                     \n",
      "Bachelor     34.49  11.28  25722.33  22188.83               3.02       0.29   \n",
      "High School  33.90  12.14  25138.59  22588.63               3.00       0.28   \n",
      "Master       36.34  11.76  27224.65  23229.79               3.02       0.32   \n",
      "PhD          35.02  12.08  24331.07  22946.36               2.86       0.29   \n",
      "\n",
      "            customer_id  \n",
      "                  count  \n",
      "education                \n",
      "Bachelor            382  \n",
      "High School         315  \n",
      "Master              209  \n",
      "PhD                  94  \n",
      "\n",
      "Analysis by Region:\n",
      "          income  is_premium  satisfaction_score\n",
      "region                                          \n",
      "East    26469.21        0.27                2.90\n",
      "North   25249.80        0.28                2.97\n",
      "South   25172.40        0.32                3.01\n",
      "West    26063.05        0.30                3.10\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Essential steps before feeding data to ML models. This is where Pandas really shines - handling the messy, real-world data issues that would be painful with pure NumPy arrays.\n",
    "\n",
    "**Why Cleaning Matters for ML:**\n",
    "- **Garbage in, garbage out**: Poor data quality leads to poor model performance\n",
    "- **Algorithm assumptions**: Many ML algorithms assume clean, complete data\n",
    "- **Feature consistency**: Models need consistent data types and scales\n",
    "- **Missing data**: Can break training or lead to biased predictions\n",
    "\n",
    "**NumPy Connection:** Remember how we had to be careful about array shapes and dtypes? Pandas makes this easier with automatic type inference and flexible operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.334908Z",
     "start_time": "2026-01-21T22:38:26.303818Z"
    }
   },
   "source": [
    "# Handle missing values\n",
    "print(\"Handling Missing Values:\")\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# PARAMETER DEEP DIVE: inplace parameter\n",
    "print(\"\\nPARAMETER EXPLANATION: inplace=True/False\")\n",
    "print(\"â€¢ What it does: Controls whether to modify the original DataFrame or return a new one\")\n",
    "print(\"â€¢ inplace=True: Modifies the original DataFrame directly (no return value)\")\n",
    "print(\"â€¢ inplace=False (default): Returns a new DataFrame, leaves original unchanged\")\n",
    "print(\"â€¢ Memory impact: inplace=True saves memory by not creating copies\")\n",
    "print(\"â€¢ Safety: inplace=False is safer - you can always go back to original data\")\n",
    "print(\"â€¢ ML workflow: Often use inplace=True after confirming operations are correct\")\n",
    "print(\"â€¢ Common mistake: Forgetting to assign result when inplace=False\")\n",
    "\n",
    "# Demonstrate the difference\n",
    "print(\"\\nDemonstration:\")\n",
    "demo_series = pd.Series([1, None, 3, None, 5])\n",
    "print(f\"Original: {demo_series.tolist()}\")\n",
    "\n",
    "# Method 1: inplace=False (default)\n",
    "filled_copy = demo_series.fillna(999)  # Returns new Series\n",
    "print(f\"After fillna(999): Original still {demo_series.tolist()}\")\n",
    "print(f\"New series: {filled_copy.tolist()}\")\n",
    "\n",
    "# Method 2: inplace=True\n",
    "demo_series.fillna(999, inplace=True)  # Modifies original\n",
    "print(f\"After fillna(999, inplace=True): Original now {demo_series.tolist()}\")\n",
    "\n",
    "# Strategy 1: Fill numerical missing values with median\n",
    "df_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n",
    "df_clean['satisfaction_score'].fillna(df_clean['satisfaction_score'].mean(), inplace=True)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Alternative strategies\n",
    "print(\"\\nAlternative missing value strategies:\")\n",
    "print(\"1. Forward fill: df.fillna(method='ffill')\")\n",
    "print(\"2. Backward fill: df.fillna(method='bfill')\")\n",
    "print(\"3. Interpolation: df.interpolate()\")\n",
    "print(\"4. Drop rows: df.dropna()\")\n",
    "print(\"5. Drop columns: df.dropna(axis=1)\")\n",
    "\n",
    "# PARAMETER DEEP DIVE: axis parameter in Pandas\n",
    "print(\"\\nPARAMETER EXPLANATION: axis parameter\")\n",
    "print(\"â€¢ What it controls: Which dimension to operate along\")\n",
    "print(\"â€¢ axis=0 or axis='index': Operate along ROWS (down the DataFrame)\")\n",
    "print(\"â€¢ axis=1 or axis='columns': Operate along COLUMNS (across the DataFrame)\")\n",
    "print(\"â€¢ Memory tip: axis=0 affects rows, axis=1 affects columns\")\n",
    "print(\"â€¢ ML context: axis=0 for sample-wise operations, axis=1 for feature-wise\")\n",
    "print(\"â€¢ Connection to NumPy: Same concept as NumPy axis parameter!\")\n",
    "\n",
    "# Demonstrate axis parameter\n",
    "demo_df = pd.DataFrame({\n",
    "    'A': [1, 2, None],\n",
    "    'B': [None, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "print(\"\\nDemo DataFrame:\")\n",
    "print(demo_df)\n",
    "print(f\"\\nMissing values per column (axis=0): \\n{demo_df.isnull().sum(axis=0)}\")\n",
    "print(f\"\\nMissing values per row (axis=1): \\n{demo_df.isnull().sum(axis=1)}\")\n",
    "print(f\"\\nDrop rows with any NaN (axis=0): \\n{demo_df.dropna(axis=0)}\")\n",
    "print(f\"\\nDrop columns with any NaN (axis=1): \\n{demo_df.dropna(axis=1)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Missing Values:\n",
      "Missing values before cleaning:\n",
      "customer_id            0\n",
      "age                    0\n",
      "income                20\n",
      "education              0\n",
      "experience_years       0\n",
      "num_purchases          0\n",
      "satisfaction_score    20\n",
      "is_premium             0\n",
      "region                 0\n",
      "signup_date            0\n",
      "dtype: int64\n",
      "\n",
      "PARAMETER EXPLANATION: inplace=True/False\n",
      "â€¢ What it does: Controls whether to modify the original DataFrame or return a new one\n",
      "â€¢ inplace=True: Modifies the original DataFrame directly (no return value)\n",
      "â€¢ inplace=False (default): Returns a new DataFrame, leaves original unchanged\n",
      "â€¢ Memory impact: inplace=True saves memory by not creating copies\n",
      "â€¢ Safety: inplace=False is safer - you can always go back to original data\n",
      "â€¢ ML workflow: Often use inplace=True after confirming operations are correct\n",
      "â€¢ Common mistake: Forgetting to assign result when inplace=False\n",
      "\n",
      "Demonstration:\n",
      "Original: [1.0, nan, 3.0, nan, 5.0]\n",
      "After fillna(999): Original still [1.0, nan, 3.0, nan, 5.0]\n",
      "New series: [1.0, 999.0, 3.0, 999.0, 5.0]\n",
      "After fillna(999, inplace=True): Original now [1.0, 999.0, 3.0, 999.0, 5.0]\n",
      "\n",
      "Missing values after cleaning:\n",
      "customer_id           0\n",
      "age                   0\n",
      "income                0\n",
      "education             0\n",
      "experience_years      0\n",
      "num_purchases         0\n",
      "satisfaction_score    0\n",
      "is_premium            0\n",
      "region                0\n",
      "signup_date           0\n",
      "dtype: int64\n",
      "\n",
      "Alternative missing value strategies:\n",
      "1. Forward fill: df.fillna(method='ffill')\n",
      "2. Backward fill: df.fillna(method='bfill')\n",
      "3. Interpolation: df.interpolate()\n",
      "4. Drop rows: df.dropna()\n",
      "5. Drop columns: df.dropna(axis=1)\n",
      "\n",
      "PARAMETER EXPLANATION: axis parameter\n",
      "â€¢ What it controls: Which dimension to operate along\n",
      "â€¢ axis=0 or axis='index': Operate along ROWS (down the DataFrame)\n",
      "â€¢ axis=1 or axis='columns': Operate along COLUMNS (across the DataFrame)\n",
      "â€¢ Memory tip: axis=0 affects rows, axis=1 affects columns\n",
      "â€¢ ML context: axis=0 for sample-wise operations, axis=1 for feature-wise\n",
      "â€¢ Connection to NumPy: Same concept as NumPy axis parameter!\n",
      "\n",
      "Demo DataFrame:\n",
      "     A    B  C\n",
      "0  1.0  NaN  7\n",
      "1  2.0  5.0  8\n",
      "2  NaN  6.0  9\n",
      "\n",
      "Missing values per column (axis=0): \n",
      "A    1\n",
      "B    1\n",
      "C    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values per row (axis=1): \n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "dtype: int64\n",
      "\n",
      "Drop rows with any NaN (axis=0): \n",
      "     A    B  C\n",
      "1  2.0  5.0  8\n",
      "\n",
      "Drop columns with any NaN (axis=1): \n",
      "   C\n",
      "0  7\n",
      "1  8\n",
      "2  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_46539/108240732.py:34: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['income'].fillna(df_clean['income'].median(), inplace=True)\n",
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_46539/108240732.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_clean['satisfaction_score'].fillna(df_clean['satisfaction_score'].mean(), inplace=True)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.377324Z",
     "start_time": "2026-01-21T22:38:26.341363Z"
    }
   },
   "source": [
    "# Handle outliers\n",
    "print(\"Outlier Detection and Handling:\")\n",
    "\n",
    "# PARAMETER EXPLANATION: quantile() parameter\n",
    "print(\"\\nPARAMETER EXPLANATION: quantile parameter\")\n",
    "print(\"â€¢ What it does: Returns the value at a specific percentile of the data\")\n",
    "print(\"â€¢ Range: 0.0 to 1.0 (0% to 100%)\")\n",
    "print(\"â€¢ Common values:\")\n",
    "print(\"  - 0.25 = 25th percentile (Q1, first quartile)\")\n",
    "print(\"  - 0.50 = 50th percentile (median, Q2)\")\n",
    "print(\"  - 0.75 = 75th percentile (Q3, third quartile)\")\n",
    "print(\"  - 0.95 = 95th percentile (common outlier threshold)\")\n",
    "print(\"â€¢ ML usage: Outlier detection, data capping, understanding distributions\")\n",
    "print(\"â€¢ IQR method: Uses Q1 and Q3 to identify outliers\")\n",
    "print(\"â€¢ Why 0.95: Keeps 95% of data, removes extreme 5%\")\n",
    "\n",
    "# Demonstrate quantiles\n",
    "demo_data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 100])  # Note the outlier 100\n",
    "print(\"\\nDemo with data [1,2,3,4,5,6,7,8,9,100]:\")\n",
    "print(f\"25th percentile (Q1): {demo_data.quantile(0.25)}\")\n",
    "print(f\"50th percentile (median): {demo_data.quantile(0.50)}\")\n",
    "print(f\"75th percentile (Q3): {demo_data.quantile(0.75)}\")\n",
    "print(f\"95th percentile: {demo_data.quantile(0.95)}\")\n",
    "print(f\"Note: 95th percentile (9.5) would cap the outlier 100\")\n",
    "\n",
    "\n",
    "# Identify outliers using IQR method\n",
    "def detect_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "\n",
    "# Check for outliers in income\n",
    "income_outliers = detect_outliers_iqr(df_clean['income'])\n",
    "print(f\"\\nIncome outliers: {income_outliers.sum()} ({income_outliers.mean():.1%})\")\n",
    "\n",
    "# Visualize outliers\n",
    "print(\"Income statistics:\")\n",
    "print(f\"Mean: ${df_clean['income'].mean():.0f}\")\n",
    "print(f\"Median: ${df_clean['income'].median():.0f}\")\n",
    "print(f\"95th percentile: ${df_clean['income'].quantile(0.95):.0f}\")\n",
    "print(f\"99th percentile: ${df_clean['income'].quantile(0.99):.0f}\")\n",
    "print(f\"Max: ${df_clean['income'].max():.0f}\")\n",
    "\n",
    "# Handle outliers (cap at 95th percentile)\n",
    "income_cap = df_clean['income'].quantile(0.95)\n",
    "\n",
    "# PARAMETER EXPLANATION: clip() upper and lower parameters\n",
    "print(\"\\nPARAMETER EXPLANATION: clip() function\")\n",
    "print(\"â€¢ What it does: Limits values to a specified range\")\n",
    "print(\"â€¢ upper parameter: Sets maximum allowed value (clips values above this)\")\n",
    "print(\"â€¢ lower parameter: Sets minimum allowed value (clips values below this)\")\n",
    "print(\"â€¢ ML usage: Handle outliers without removing data points\")\n",
    "print(\"â€¢ Alternative to: Removing outliers entirely (preserves sample size)\")\n",
    "print(\"â€¢ Example: clip(lower=0, upper=100) keeps values between 0 and 100\")\n",
    "\n",
    "# Demonstrate clip function\n",
    "demo_values = pd.Series([-5, 2, 8, 15, 25, 150])\n",
    "print(f\"\\nDemo values: {demo_values.tolist()}\")\n",
    "print(f\"After clip(upper=20): {demo_values.clip(upper=20).tolist()}\")\n",
    "print(f\"After clip(lower=0, upper=20): {demo_values.clip(lower=0, upper=20).tolist()}\")\n",
    "\n",
    "df_clean['income_capped'] = df_clean['income'].clip(upper=income_cap)\n",
    "\n",
    "print(\"\\nAfter capping at 95th percentile:\")\n",
    "print(f\"Max income: ${df_clean['income_capped'].max():.0f}\")\n",
    "print(f\"Values capped: {(df_clean['income'] > income_cap).sum()} out of {len(df_clean)}\")\n",
    "\n",
    "print(\"\\n\" + \"ðŸŽ¯\" + \" TRY IT YOURSELF - Outlier Detection\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Challenge: Apply outlier detection to the 'age' column\")\n",
    "print(\"\\nYour task:\")\n",
    "print(\"1. Calculate Q1, Q3, and IQR for the 'age' column\")\n",
    "print(\"2. Identify outliers using the 1.5*IQR rule\")\n",
    "print(\"3. Count how many outliers you found\")\n",
    "print(\"4. Cap outliers at the 95th percentile\")\n",
    "print(\"5. Compare before/after statistics\")\n",
    "print(\"\\nHint: Use the detect_outliers_iqr() function we defined above\")\n",
    "print(\"Bonus: Try different percentile thresholds (90th, 99th)\")\n",
    "print(\"\\n# Uncomment and complete the code below:\")\n",
    "print(\"# age_outliers = detect_outliers_iqr(df_clean['age'])\")\n",
    "print(\"# print(f'Age outliers: {age_outliers.sum()}')\")\n",
    "print(\"# age_cap = df_clean['age'].quantile(?)\")\n",
    "print(\"# df_clean['age_capped'] = df_clean['age'].clip(upper=?)\")\n",
    "print(\"# print('Before:', df_clean['age'].describe())\")\n",
    "print(\"# print('After:', df_clean['age_capped'].describe())\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier Detection and Handling:\n",
      "\n",
      "PARAMETER EXPLANATION: quantile parameter\n",
      "â€¢ What it does: Returns the value at a specific percentile of the data\n",
      "â€¢ Range: 0.0 to 1.0 (0% to 100%)\n",
      "â€¢ Common values:\n",
      "  - 0.25 = 25th percentile (Q1, first quartile)\n",
      "  - 0.50 = 50th percentile (median, Q2)\n",
      "  - 0.75 = 75th percentile (Q3, third quartile)\n",
      "  - 0.95 = 95th percentile (common outlier threshold)\n",
      "â€¢ ML usage: Outlier detection, data capping, understanding distributions\n",
      "â€¢ IQR method: Uses Q1 and Q3 to identify outliers\n",
      "â€¢ Why 0.95: Keeps 95% of data, removes extreme 5%\n",
      "\n",
      "Demo with data [1,2,3,4,5,6,7,8,9,100]:\n",
      "25th percentile (Q1): 3.25\n",
      "50th percentile (median): 5.5\n",
      "75th percentile (Q3): 7.75\n",
      "95th percentile: 59.049999999999905\n",
      "Note: 95th percentile (9.5) would cap the outlier 100\n",
      "\n",
      "Income outliers: 37 (3.7%)\n",
      "Income statistics:\n",
      "Mean: $25670\n",
      "Median: $22732\n",
      "95th percentile: $50839\n",
      "99th percentile: $69259\n",
      "Max: $105754\n",
      "\n",
      "PARAMETER EXPLANATION: clip() function\n",
      "â€¢ What it does: Limits values to a specified range\n",
      "â€¢ upper parameter: Sets maximum allowed value (clips values above this)\n",
      "â€¢ lower parameter: Sets minimum allowed value (clips values below this)\n",
      "â€¢ ML usage: Handle outliers without removing data points\n",
      "â€¢ Alternative to: Removing outliers entirely (preserves sample size)\n",
      "â€¢ Example: clip(lower=0, upper=100) keeps values between 0 and 100\n",
      "\n",
      "Demo values: [-5, 2, 8, 15, 25, 150]\n",
      "After clip(upper=20): [-5, 2, 8, 15, 20, 20]\n",
      "After clip(lower=0, upper=20): [0, 2, 8, 15, 20, 20]\n",
      "\n",
      "After capping at 95th percentile:\n",
      "Max income: $50839\n",
      "Values capped: 50 out of 1000\n",
      "\n",
      "ðŸŽ¯ TRY IT YOURSELF - Outlier Detection\n",
      "==================================================\n",
      "Challenge: Apply outlier detection to the 'age' column\n",
      "\n",
      "Your task:\n",
      "1. Calculate Q1, Q3, and IQR for the 'age' column\n",
      "2. Identify outliers using the 1.5*IQR rule\n",
      "3. Count how many outliers you found\n",
      "4. Cap outliers at the 95th percentile\n",
      "5. Compare before/after statistics\n",
      "\n",
      "Hint: Use the detect_outliers_iqr() function we defined above\n",
      "Bonus: Try different percentile thresholds (90th, 99th)\n",
      "\n",
      "# Uncomment and complete the code below:\n",
      "# age_outliers = detect_outliers_iqr(df_clean['age'])\n",
      "# print(f'Age outliers: {age_outliers.sum()}')\n",
      "# age_cap = df_clean['age'].quantile(?)\n",
      "# df_clean['age_capped'] = df_clean['age'].clip(upper=?)\n",
      "# print('Before:', df_clean['age'].describe())\n",
      "# print('After:', df_clean['age_capped'].describe())\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.394797Z",
     "start_time": "2026-01-21T22:38:26.378475Z"
    }
   },
   "source": [
    "# Data type optimization (important for large datasets)\n",
    "print(\"Data Type Optimization:\")\n",
    "print(f\"Memory usage before optimization: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Optimize integer columns\n",
    "int_cols = df_clean.select_dtypes(include=['int64']).columns\n",
    "for col in int_cols:\n",
    "    if col != 'customer_id':  # Keep ID as int64\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], downcast='integer')\n",
    "\n",
    "# Optimize float columns\n",
    "float_cols = df_clean.select_dtypes(include=['float64']).columns\n",
    "for col in float_cols:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], downcast='float')\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "categorical_cols = ['education', 'region']\n",
    "for col in categorical_cols:\n",
    "    df_clean[col] = df_clean[col].astype('category')\n",
    "\n",
    "print(f\"Memory usage after optimization: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"Memory reduction: {(1 - df_clean.memory_usage(deep=True).sum() / df.memory_usage(deep=True).sum()) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nOptimized data types:\")\n",
    "print(df_clean.dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type Optimization:\n",
      "Memory usage before optimization: 194.04 KB\n",
      "Memory usage after optimization: 44.90 KB\n",
      "Memory reduction: 75.9%\n",
      "\n",
      "Optimized data types:\n",
      "customer_id                    int64\n",
      "age                             int8\n",
      "income                       float64\n",
      "education                   category\n",
      "experience_years             float32\n",
      "                           ...      \n",
      "satisfaction_score           float32\n",
      "is_premium                      int8\n",
      "region                      category\n",
      "signup_date           datetime64[ns]\n",
      "income_capped                float64\n",
      "Length: 11, dtype: object\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Creating new features that can improve ML model performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.459001Z",
     "start_time": "2026-01-21T22:38:26.397527Z"
    }
   },
   "source": [
    "# Feature engineering examples\n",
    "print(\"Feature Engineering:\")\n",
    "\n",
    "# 1. Binning continuous variables\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'],\n",
    "                               bins=[0, 25, 35, 50, 100],\n",
    "                               labels=['Young', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "df_clean['income_tier'] = pd.qcut(df_clean['income_capped'],\n",
    "                                  q=4,\n",
    "                                  labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Age group distribution:\")\n",
    "print(df_clean['age_group'].value_counts())\n",
    "\n",
    "print(\"\\nIncome tier distribution:\")\n",
    "print(df_clean['income_tier'].value_counts())\n",
    "\n",
    "# 2. Mathematical transformations\n",
    "# PARAMETER EXPLANATION: np.log1p() function\n",
    "# â€¢ What it does: Calculates log(1 + x) more accurately than np.log(1 + x)\n",
    "# â€¢ Why log1p: Avoids numerical precision issues when x is close to zero\n",
    "# â€¢ ML usage: Log transforms reduce skewness in income/price data\n",
    "# â€¢ Why +1: Handles zero values (log(0) is undefined, but log(1+0) = 0)\n",
    "# â€¢ Alternative: np.log(df_clean['income_capped'] + 1) but less precise\n",
    "df_clean['log_income'] = np.log1p(df_clean['income_capped'])  # log(1+x) to handle zeros\n",
    "\n",
    "# PARAMETER EXPLANATION: Adding +1 in denominators\n",
    "# â€¢ Why +1: Prevents division by zero errors\n",
    "# â€¢ Example: If num_purchases=0, division by (0+1)=1 instead of crashing\n",
    "# â€¢ ML safety: Ensures robust feature engineering without runtime errors\n",
    "# â€¢ Alternative: Use np.where() to handle zero cases explicitly\n",
    "df_clean['income_per_purchase'] = df_clean['income_capped'] / (df_clean['num_purchases'] + 1)\n",
    "df_clean['satisfaction_squared'] = df_clean['satisfaction_score'] ** 2\n",
    "\n",
    "# 3. Date-based features\n",
    "# PARAMETER EXPLANATION: .dt accessor\n",
    "# â€¢ What it is: Special accessor for datetime operations on Series\n",
    "# â€¢ Usage: Only works on datetime-type Series (not regular objects)\n",
    "# â€¢ Common attributes: .year, .month, .day, .dayofweek, .quarter\n",
    "# â€¢ ML usage: Extract time-based features from timestamps\n",
    "# â€¢ dayofweek: 0=Monday, 1=Tuesday, ..., 6=Sunday\n",
    "# â€¢ Why useful: Captures seasonal patterns and cyclical behavior\n",
    "# â€¢ Error prevention: Will fail if Series is not datetime type\n",
    "df_clean['signup_year'] = df_clean['signup_date'].dt.year\n",
    "df_clean['signup_month'] = df_clean['signup_date'].dt.month\n",
    "df_clean['signup_dayofweek'] = df_clean['signup_date'].dt.dayofweek\n",
    "df_clean['days_since_signup'] = (datetime.now() - df_clean['signup_date']).dt.days\n",
    "\n",
    "print(\"\\nDatetime feature examples:\")\n",
    "print(f\"Sample signup_date: {df_clean['signup_date'].iloc[0]}\")\n",
    "print(f\"Extracted year: {df_clean['signup_year'].iloc[0]}\")\n",
    "print(f\"Extracted month: {df_clean['signup_month'].iloc[0]}\")\n",
    "print(f\"Day of week (0=Mon): {df_clean['signup_dayofweek'].iloc[0]}\")\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = ['age_group', 'income_tier', 'log_income', 'income_per_purchase',\n",
    "                'satisfaction_squared', 'signup_year', 'signup_month', 'days_since_signup']\n",
    "print(df_clean[new_features].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering:\n",
      "Age group distribution:\n",
      "age_group\n",
      "Middle-aged    374\n",
      "Adult          309\n",
      "Young          218\n",
      "Senior          98\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Income tier distribution:\n",
      "income_tier\n",
      "Medium       260\n",
      "Low          250\n",
      "Very High    250\n",
      "High         240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Datetime feature examples:\n",
      "Sample signup_date: 2020-01-01 00:00:00\n",
      "Extracted year: 2020\n",
      "Extracted month: 1\n",
      "Day of week (0=Mon): 2\n",
      "\n",
      "New features created:\n",
      "     age_group income_tier  log_income  income_per_purchase  \\\n",
      "0  Middle-aged   Very High   10.699700         11085.390588   \n",
      "1        Adult   Very High   10.462345          8743.120839   \n",
      "2  Middle-aged      Medium   10.029859          2836.634642   \n",
      "3       Senior         Low    9.676594          3984.779471   \n",
      "4        Adult        High   10.349144          7807.322042   \n",
      "\n",
      "   satisfaction_squared  signup_year  signup_month  days_since_signup  \n",
      "0              3.966988         2020             1               2212  \n",
      "1             22.195084         2020             1               2211  \n",
      "2             22.359049         2020             1               2210  \n",
      "3             15.072608         2020             1               2209  \n",
      "4             16.508400         2020             1               2208  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.486792Z",
     "start_time": "2026-01-21T22:38:26.459854Z"
    }
   },
   "source": [
    "# Interaction features\n",
    "print(\"Interaction Features:\")\n",
    "\n",
    "# Create interaction between important features\n",
    "df_clean['age_income_interaction'] = df_clean['age'] * df_clean['log_income']\n",
    "df_clean['experience_satisfaction'] = df_clean['experience_years'] * df_clean['satisfaction_score']\n",
    "\n",
    "# Boolean combinations\n",
    "df_clean['high_income_high_satisfaction'] = (\n",
    "        (df_clean['income_tier'] == 'Very High') &\n",
    "        (df_clean['satisfaction_score'] > 4)\n",
    ").astype(int)\n",
    "\n",
    "df_clean['experienced_premium'] = (\n",
    "        (df_clean['experience_years'] > 5) &\n",
    "        (df_clean['is_premium'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "print(\"Interaction features:\")\n",
    "interaction_features = ['age_income_interaction', 'experience_satisfaction',\n",
    "                        'high_income_high_satisfaction', 'experienced_premium']\n",
    "print(df_clean[interaction_features].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction Features:\n",
      "Interaction features:\n",
      "       age_income_interaction  experience_satisfaction  \\\n",
      "count             1000.000000              1000.000000   \n",
      "mean               348.022457                13.925125   \n",
      "std                118.560771                15.730806   \n",
      "min                -30.808238                 0.000356   \n",
      "25%                267.392329                 3.523085   \n",
      "50%                348.322690                 9.498555   \n",
      "75%                425.520200                18.350727   \n",
      "max                877.751173               175.326508   \n",
      "\n",
      "       high_income_high_satisfaction  experienced_premium  \n",
      "count                    1000.000000          1000.000000  \n",
      "mean                        0.067000             0.084000  \n",
      "std                         0.250147             0.277527  \n",
      "min                         0.000000             0.000000  \n",
      "25%                         0.000000             0.000000  \n",
      "50%                         0.000000             0.000000  \n",
      "75%                         0.000000             0.000000  \n",
      "max                         1.000000             1.000000  \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.521258Z",
     "start_time": "2026-01-21T22:38:26.495056Z"
    }
   },
   "source": [
    "# Aggregation features (useful for time series or grouped data)\n",
    "print(\"Aggregation Features:\")\n",
    "\n",
    "# Features based on region\n",
    "region_stats = df_clean.groupby('region').agg({\n",
    "    'income_capped': ['mean', 'std'],\n",
    "    'satisfaction_score': 'mean',\n",
    "    'is_premium': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "# Flatten column names\n",
    "region_stats.columns = ['_'.join(col).strip() for col in region_stats.columns]\n",
    "region_stats = region_stats.add_prefix('region_')\n",
    "\n",
    "# Merge back to main dataframe\n",
    "df_clean = df_clean.merge(region_stats, left_on='region', right_index=True, how='left')\n",
    "\n",
    "print(\"Region-based features:\")\n",
    "region_features = [col for col in df_clean.columns if col.startswith('region_')]\n",
    "print(df_clean[['region'] + region_features].head())\n",
    "\n",
    "# Relative features (compare individual to group)\n",
    "df_clean['income_vs_region_mean'] = df_clean['income_capped'] / df_clean['region_income_capped_mean']\n",
    "df_clean['satisfaction_vs_region_mean'] = df_clean['satisfaction_score'] / df_clean['region_satisfaction_score_mean']\n",
    "\n",
    "print(\"\\nRelative features:\")\n",
    "print(df_clean[['income_vs_region_mean', 'satisfaction_vs_region_mean']].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation Features:\n",
      "Region-based features:\n",
      "  region  region_income_capped_mean  region_income_capped_std  \\\n",
      "0   East                  25677.381                 10974.508   \n",
      "1   East                  25677.381                 10974.508   \n",
      "2  North                  24758.733                 11104.631   \n",
      "3   East                  25677.381                 10974.508   \n",
      "4   East                  25677.381                 10974.508   \n",
      "\n",
      "   region_satisfaction_score_mean  region_is_premium_mean  \n",
      "0                           2.901                   0.269  \n",
      "1                           2.901                   0.269  \n",
      "2                           2.971                   0.281  \n",
      "3                           2.901                   0.269  \n",
      "4                           2.901                   0.269  \n",
      "\n",
      "Relative features:\n",
      "       income_vs_region_mean  satisfaction_vs_region_mean\n",
      "count            1000.000000                  1000.000000\n",
      "mean                1.000000                     0.999992\n",
      "std                 0.452976                     0.368513\n",
      "min                 0.202645                     0.328418\n",
      "25%                 0.654898                     0.696641\n",
      "50%                 0.904276                     0.995834\n",
      "75%                 1.251632                     1.306528\n",
      "max                 2.085304                     1.721367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_46539/830367533.py:5: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  region_stats = df_clean.groupby('region').agg({\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding\n",
    "\n",
    "Converting categorical variables to numerical format for ML models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.549161Z",
     "start_time": "2026-01-21T22:38:26.526560Z"
    }
   },
   "source": [
    "# One-hot encoding\n",
    "print(\"One-Hot Encoding:\")\n",
    "\n",
    "# Select categorical columns for encoding\n",
    "categorical_cols = ['education', 'region', 'age_group', 'income_tier']\n",
    "\n",
    "# PARAMETER EXPLANATION: drop_first=True/False in get_dummies()\n",
    "print(\"\\nPARAMETER EXPLANATION: drop_first parameter\")\n",
    "print(\"â€¢ What it does: Controls whether to drop the first dummy variable\")\n",
    "print(\"â€¢ drop_first=False (default): Creates dummy for every category\")\n",
    "print(\"â€¢ drop_first=True: Drops first category to avoid multicollinearity\")\n",
    "print(\"â€¢ Why drop_first=True: Prevents 'dummy variable trap' in linear models\")\n",
    "print(\"â€¢ ML impact: Linear regression can fail with perfect multicollinearity\")\n",
    "print(\"â€¢ Example: For [A, B, C], creates only B and C columns (A is implied when both are 0)\")\n",
    "print(\"â€¢ Trade-off: Saves memory and prevents multicollinearity vs. interpretability\")\n",
    "\n",
    "# Demonstrate the difference\n",
    "demo_data = pd.DataFrame({'category': ['A', 'B', 'C', 'A', 'B']})\n",
    "print(\"\\nDemo with small dataset:\")\n",
    "print(\"Original:\", demo_data['category'].tolist())\n",
    "print(\"\\nWith drop_first=False:\")\n",
    "print(pd.get_dummies(demo_data['category'], drop_first=False))\n",
    "print(\"\\nWith drop_first=True:\")\n",
    "print(pd.get_dummies(demo_data['category'], drop_first=True))\n",
    "print(\"Note: Category 'A' is implied when both B and C are 0\")\n",
    "\n",
    "# One-hot encode\n",
    "df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"\\nShape before encoding: {df_clean.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "\n",
    "# Show new columns\n",
    "new_cols = [col for col in df_encoded.columns if any(cat in col for cat in categorical_cols)]\n",
    "print(f\"\\nNew encoded columns ({len(new_cols)}):\")\n",
    "for col in new_cols[:10]:  # Show first 10\n",
    "    print(f\"  {col}\")\n",
    "if len(new_cols) > 10:\n",
    "    print(f\"  ... and {len(new_cols) - 10} more\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding:\n",
      "\n",
      "PARAMETER EXPLANATION: drop_first parameter\n",
      "â€¢ What it does: Controls whether to drop the first dummy variable\n",
      "â€¢ drop_first=False (default): Creates dummy for every category\n",
      "â€¢ drop_first=True: Drops first category to avoid multicollinearity\n",
      "â€¢ Why drop_first=True: Prevents 'dummy variable trap' in linear models\n",
      "â€¢ ML impact: Linear regression can fail with perfect multicollinearity\n",
      "â€¢ Example: For [A, B, C], creates only B and C columns (A is implied when both are 0)\n",
      "â€¢ Trade-off: Saves memory and prevents multicollinearity vs. interpretability\n",
      "\n",
      "Demo with small dataset:\n",
      "Original: ['A', 'B', 'C', 'A', 'B']\n",
      "\n",
      "With drop_first=False:\n",
      "       A      B      C\n",
      "0   True  False  False\n",
      "1  False   True  False\n",
      "2  False  False   True\n",
      "3   True  False  False\n",
      "4  False   True  False\n",
      "\n",
      "With drop_first=True:\n",
      "       B      C\n",
      "0  False  False\n",
      "1   True  False\n",
      "2  False   True\n",
      "3  False  False\n",
      "4   True  False\n",
      "Note: Category 'A' is implied when both B and C are 0\n",
      "\n",
      "Shape before encoding: (1000, 30)\n",
      "Shape after encoding: (1000, 38)\n",
      "\n",
      "New encoded columns (18):\n",
      "  region_income_capped_mean\n",
      "  region_income_capped_std\n",
      "  region_satisfaction_score_mean\n",
      "  region_is_premium_mean\n",
      "  income_vs_region_mean\n",
      "  satisfaction_vs_region_mean\n",
      "  education_High School\n",
      "  education_Master\n",
      "  education_PhD\n",
      "  region_North\n",
      "  ... and 8 more\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.734474Z",
     "start_time": "2026-01-21T22:38:26.550682Z"
    }
   },
   "source": [
    "# Label encoding (for ordinal variables)\n",
    "print(\"Label Encoding:\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy for label encoding\n",
    "df_label_encoded = df_clean.copy()\n",
    "\n",
    "# Education has natural ordering\n",
    "education_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_label_encoded['education_encoded'] = df_label_encoded['education'].map(education_order)\n",
    "\n",
    "# For non-ordinal categories, use LabelEncoder\n",
    "le_region = LabelEncoder()\n",
    "df_label_encoded['region_encoded'] = le_region.fit_transform(df_label_encoded['region'])\n",
    "\n",
    "print(\"Education encoding:\")\n",
    "print(df_label_encoded[['education', 'education_encoded']].drop_duplicates().sort_values('education_encoded'))\n",
    "\n",
    "print(\"\\nRegion encoding:\")\n",
    "print(df_label_encoded[['region', 'region_encoded']].drop_duplicates().sort_values('region_encoded'))\n",
    "\n",
    "# Show encoding mapping\n",
    "print(\"\\nRegion encoding mapping:\")\n",
    "for i, region in enumerate(le_region.classes_):\n",
    "    print(f\"  {region}: {i}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding:\n",
      "Education encoding:\n",
      "     education education_encoded\n",
      "0     Bachelor                 1\n",
      "1  High School                 0\n",
      "4       Master                 2\n",
      "5          PhD                 3\n",
      "\n",
      "Region encoding:\n",
      "  region  region_encoded\n",
      "0   East               0\n",
      "2  North               1\n",
      "6  South               2\n",
      "9   West               3\n",
      "\n",
      "Region encoding mapping:\n",
      "  East: 0\n",
      "  North: 1\n",
      "  South: 2\n",
      "  West: 3\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.754655Z",
     "start_time": "2026-01-21T22:38:26.734915Z"
    }
   },
   "source": [
    "# Target encoding (advanced technique)\n",
    "print(\"Target Encoding:\")\n",
    "\n",
    "\n",
    "# Calculate mean target value for each category\n",
    "def target_encode(df, categorical_col, target_col, smoothing=1):\n",
    "    \"\"\"\n",
    "    Target encoding with smoothing to prevent overfitting\n",
    "    \"\"\"\n",
    "    # Calculate global mean\n",
    "    global_mean = df[target_col].mean()\n",
    "\n",
    "    # Calculate category means and counts\n",
    "    category_stats = df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n",
    "\n",
    "    # Apply smoothing\n",
    "    smoothed_means = (\n",
    "            (category_stats['mean'] * category_stats['count'] + global_mean * smoothing) /\n",
    "            (category_stats['count'] + smoothing)\n",
    "    )\n",
    "\n",
    "    return smoothed_means\n",
    "\n",
    "\n",
    "# Target encode education based on premium rate\n",
    "education_target_encoding = target_encode(df_clean, 'education', 'is_premium')\n",
    "df_clean['education_target_encoded'] = df_clean['education'].map(education_target_encoding)\n",
    "\n",
    "print(\"Education target encoding (premium rate):\")\n",
    "print(education_target_encoding.sort_values(ascending=False))\n",
    "\n",
    "# Target encode region\n",
    "region_target_encoding = target_encode(df_clean, 'region', 'is_premium')\n",
    "df_clean['region_target_encoded'] = df_clean['region'].map(region_target_encoding)\n",
    "\n",
    "print(\"\\nRegion target encoding (premium rate):\")\n",
    "print(region_target_encoding.sort_values(ascending=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding:\n",
      "Education target encoding (premium rate):\n",
      "education\n",
      "Master         0.320448\n",
      "Bachelor       0.293196\n",
      "PhD            0.287305\n",
      "High School    0.279411\n",
      "dtype: float64\n",
      "\n",
      "Region target encoding (premium rate):\n",
      "region\n",
      "South    0.317257\n",
      "West     0.303808\n",
      "North    0.280907\n",
      "East     0.269336\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_46539/1258410269.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_stats = df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n",
      "/var/folders/_0/vj0h1w7s5rz2zncs6_jg3bp00000gn/T/ipykernel_46539/1258410269.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  category_stats = df.groupby(categorical_col)[target_col].agg(['mean', 'count'])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling and Normalization\n",
    "\n",
    "Preparing numerical features for ML algorithms that are sensitive to scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.793165Z",
     "start_time": "2026-01-21T22:38:26.764551Z"
    }
   },
   "source": [
    "# Feature scaling\n",
    "print(\"Feature Scaling:\")\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "# Select numerical features for scaling\n",
    "numerical_features = ['age', 'income_capped', 'experience_years', 'satisfaction_score',\n",
    "                      'log_income', 'days_since_signup']\n",
    "\n",
    "print(\"Original feature statistics:\")\n",
    "print(df_clean[numerical_features].describe())\n",
    "\n",
    "# Standard scaling (z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_standard_scaled = df_clean.copy()\n",
    "df_standard_scaled[numerical_features] = scaler_standard.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"\\nAfter Standard Scaling (mean=0, std=1):\")\n",
    "print(df_standard_scaled[numerical_features].describe())\n",
    "\n",
    "# Min-Max scaling (0-1 range)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_minmax_scaled = df_clean.copy()\n",
    "df_minmax_scaled[numerical_features] = scaler_minmax.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"\\nAfter Min-Max Scaling (range 0-1):\")\n",
    "print(df_minmax_scaled[numerical_features].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Scaling:\n",
      "Original feature statistics:\n",
      "               age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1000.000000    1000.000000       1000.000000         1000.000000   \n",
      "mean     34.743000   25075.212815          4.712564            2.998455   \n",
      "std      11.748233   11361.041618          4.769008            1.106650   \n",
      "min      -3.000000    5063.461821          0.000154            1.000211   \n",
      "25%      27.000000   16372.665420          1.354730            2.094302   \n",
      "50%      35.000000   22732.225225          3.295244            2.998455   \n",
      "75%      42.000000   31391.594681          6.413300            3.903685   \n",
      "max      81.000000   50838.771470         38.617649            4.994469   \n",
      "\n",
      "        log_income  days_since_signup  \n",
      "count  1000.000000        1000.000000  \n",
      "mean     10.024256        1712.500000  \n",
      "std       0.471203         288.819436  \n",
      "min       8.530003        1213.000000  \n",
      "25%       9.703429        1462.750000  \n",
      "50%      10.031583        1712.500000  \n",
      "75%      10.354327        1962.250000  \n",
      "max      10.836434        2212.000000  \n",
      "\n",
      "After Standard Scaling (mean=0, std=1):\n",
      "                age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1.000000e+03   1.000000e+03      1.000000e+03        1.000000e+03   \n",
      "mean  -1.687539e-16  -3.552714e-18      2.486900e-17        1.421085e-17   \n",
      "std    1.000500e+00   1.000500e+00      1.000500e+00        1.000500e+00   \n",
      "min   -3.214261e+00  -1.762318e+00     -9.886267e-01       -1.806573e+00   \n",
      "25%   -6.594076e-01  -7.663824e-01     -7.044473e-01       -8.174271e-01   \n",
      "50%    2.188658e-02  -2.063332e-01     -2.973428e-01        1.110080e-08   \n",
      "75%    6.180190e-01   5.562467e-01      3.568009e-01        8.184002e-01   \n",
      "max    3.939328e+00   2.268846e+00      7.113020e+00        1.804557e+00   \n",
      "\n",
      "         log_income  days_since_signup  \n",
      "count  1.000000e+03         1000.00000  \n",
      "mean   3.417711e-15            0.00000  \n",
      "std    1.000500e+00            1.00050  \n",
      "min   -3.172729e+00           -1.73032  \n",
      "25%   -6.812079e-01           -0.86516  \n",
      "50%    1.555627e-02            0.00000  \n",
      "75%    7.008356e-01            0.86516  \n",
      "max    1.724487e+00            1.73032  \n",
      "\n",
      "After Min-Max Scaling (range 0-1):\n",
      "               age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1000.000000    1000.000000       1000.000000         1000.000000   \n",
      "mean      0.449321       0.437173          0.122028            0.500279   \n",
      "std       0.139860       0.248191          0.123493            0.277060   \n",
      "min       0.000000       0.000000          0.000000            0.000000   \n",
      "25%       0.357143       0.247059          0.035077            0.273916   \n",
      "50%       0.452381       0.385989          0.085326            0.500279   \n",
      "75%       0.535714       0.575160          0.166068            0.726912   \n",
      "max       1.000000       1.000000          1.000000            1.000000   \n",
      "\n",
      "        log_income  days_since_signup  \n",
      "count  1000.000000        1000.000000  \n",
      "mean      0.647864           0.500000  \n",
      "std       0.204300           0.289109  \n",
      "min       0.000000           0.000000  \n",
      "25%       0.508763           0.250000  \n",
      "50%       0.651040           0.500000  \n",
      "75%       0.790973           0.750000  \n",
      "max       1.000000           1.000000  \n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:26.817221Z",
     "start_time": "2026-01-21T22:38:26.793862Z"
    }
   },
   "source": [
    "# Robust scaling (less sensitive to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust_scaled = df_clean.copy()\n",
    "df_robust_scaled[numerical_features] = scaler_robust.fit_transform(df_clean[numerical_features])\n",
    "\n",
    "print(\"After Robust Scaling (median=0, IQR=1):\")\n",
    "print(df_robust_scaled[numerical_features].describe())\n",
    "\n",
    "# Compare scaling methods visually\n",
    "print(\"\\nScaling Comparison for 'income_capped':\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': df_clean['income_capped'],\n",
    "    'Standard': df_standard_scaled['income_capped'],\n",
    "    'MinMax': df_minmax_scaled['income_capped'],\n",
    "    'Robust': df_robust_scaled['income_capped']\n",
    "})\n",
    "\n",
    "print(comparison_df.describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Robust Scaling (median=0, IQR=1):\n",
      "               age  income_capped  experience_years  satisfaction_score  \\\n",
      "count  1000.000000    1000.000000       1000.000000        1.000000e+03   \n",
      "mean     -0.017133       0.156002          0.280182       -6.786049e-09   \n",
      "std       0.783216       0.756448          0.942758        6.116174e-01   \n",
      "min      -2.533333      -1.176433         -0.651388       -1.104379e+00   \n",
      "25%      -0.533333      -0.423436         -0.383609       -4.997026e-01   \n",
      "50%       0.000000       0.000000          0.000000        0.000000e+00   \n",
      "75%       0.466667       0.576564          0.616391        5.002974e-01   \n",
      "max       3.066667       1.871408          6.982686        1.103147e+00   \n",
      "\n",
      "        log_income  days_since_signup  \n",
      "count  1000.000000       1.000000e+03  \n",
      "mean     -0.011256      -2.842171e-17  \n",
      "std       0.723928       5.782171e-01  \n",
      "min      -2.306936      -1.000000e+00  \n",
      "25%      -0.504155      -5.000000e-01  \n",
      "50%       0.000000       0.000000e+00  \n",
      "75%       0.495845       5.000000e-01  \n",
      "max       1.236525       1.000000e+00  \n",
      "\n",
      "Scaling Comparison for 'income_capped':\n",
      "           Original      Standard       MinMax       Robust\n",
      "count   1000.000000  1.000000e+03  1000.000000  1000.000000\n",
      "mean   25075.212815 -3.552714e-18     0.437173     0.156002\n",
      "std    11361.041618  1.000500e+00     0.248191     0.756448\n",
      "min     5063.461821 -1.762318e+00     0.000000    -1.176433\n",
      "25%    16372.665420 -7.663824e-01     0.247059    -0.423436\n",
      "50%    22732.225225 -2.063332e-01     0.385989     0.000000\n",
      "75%    31391.594681  5.562467e-01     0.575160     0.576564\n",
      "max    50838.771470  2.268846e+00     1.000000     1.871408\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Splitting and Sampling\n",
    "\n",
    "Preparing data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:27.010124Z",
     "start_time": "2026-01-21T22:38:26.818291Z"
    }
   },
   "source": [
    "# Train-validation-test split\n",
    "print(\"Data Splitting:\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = [col for col in df_encoded.columns\n",
    "                   if col not in ['customer_id', 'is_premium', 'signup_date', 'education', 'region']]\n",
    "\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['is_premium']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# First split: separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation (80% of remaining)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp  # 0.25 * 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(f\"Train: {X_train.shape[0]} ({X_train.shape[0] / len(X):.1%})\")\n",
    "print(f\"Validation: {X_val.shape[0]} ({X_val.shape[0] / len(X):.1%})\")\n",
    "print(f\"Test: {X_test.shape[0]} ({X_test.shape[0] / len(X):.1%})\")\n",
    "\n",
    "# Check target distribution in each split\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(f\"Train: {y_train.mean():.3f}\")\n",
    "print(f\"Validation: {y_val.mean():.3f}\")\n",
    "print(f\"Test: {y_test.mean():.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Splitting:\n",
      "Features shape: (1000, 35)\n",
      "Target shape: (1000,)\n",
      "Target distribution: {0: 706, 1: 294}\n",
      "\n",
      "Split sizes:\n",
      "Train: 600 (60.0%)\n",
      "Validation: 200 (20.0%)\n",
      "Test: 200 (20.0%)\n",
      "\n",
      "Target distribution:\n",
      "Train: 0.293\n",
      "Validation: 0.295\n",
      "Test: 0.295\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:27.027536Z",
     "start_time": "2026-01-21T22:38:27.010544Z"
    }
   },
   "source": [
    "# Handling imbalanced data\n",
    "print(\"Handling Imbalanced Data:\")\n",
    "\n",
    "# Check class imbalance\n",
    "class_counts = y_train.value_counts()\n",
    "imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Class distribution: {class_counts.to_dict()}\")\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 2:  # If significantly imbalanced\n",
    "    print(\"\\nDataset is imbalanced. Strategies to consider:\")\n",
    "\n",
    "    # 1. Undersampling majority class\n",
    "    majority_class = y_train.value_counts().index[0]\n",
    "    minority_class = y_train.value_counts().index[1]\n",
    "\n",
    "    majority_indices = y_train[y_train == majority_class].index\n",
    "    minority_indices = y_train[y_train == minority_class].index\n",
    "\n",
    "    # Random undersample majority class\n",
    "    undersampled_majority = np.random.choice(majority_indices, size=len(minority_indices), replace=False)\n",
    "    balanced_indices = np.concatenate([undersampled_majority, minority_indices])\n",
    "\n",
    "    X_train_balanced = X_train.loc[balanced_indices]\n",
    "    y_train_balanced = y_train.loc[balanced_indices]\n",
    "\n",
    "    print(f\"1. Undersampling - New size: {len(X_train_balanced)}\")\n",
    "    print(f\"   New distribution: {y_train_balanced.value_counts().to_dict()}\")\n",
    "\n",
    "    # 2. Class weights (for algorithms that support it)\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights, strict=False))\n",
    "\n",
    "    print(f\"2. Class weights: {class_weight_dict}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dataset is reasonably balanced.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Imbalanced Data:\n",
      "Class distribution: {0: 424, 1: 176}\n",
      "Imbalance ratio: 2.41:1\n",
      "\n",
      "Dataset is imbalanced. Strategies to consider:\n",
      "1. Undersampling - New size: 352\n",
      "   New distribution: {0: 176, 1: 176}\n",
      "2. Class weights: {np.int8(0): np.float64(0.7075471698113207), np.int8(1): np.float64(1.7045454545454546)}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Converting cleaned data for other tools (framework-neutral)\n",
    "\n",
    "After cleaning and encoding, you will often convert DataFrame features to NumPy arrays. The guidance below is framework-neutral and focuses on checks and formats most tools expect.\n",
    "\n",
    "- Use numeric dtypes for features (float32 is common for inputs)\n",
    "- Use integer dtypes for labels (int32/int64 depending on the tool)\n",
    "- Verify shapes: (n_samples, n_features) for feature matrices\n",
    "- Check for NaNs and infinite values before exporting\n",
    "\n",
    "Example: convert train/val/test splits to NumPy and save them for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:27.046502Z",
     "start_time": "2026-01-21T22:38:27.028999Z"
    }
   },
   "source": [
    "print(\"Converting DataFrame to NumPy arrays (neutral):\")\n",
    "\n",
    "# Convert only numeric columns to NumPy arrays (ensure appropriate dtype)\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "X_train_np = X_train[numeric_columns].values.astype(np.float32)\n",
    "y_train_np = y_train.values.astype(np.int64)\n",
    "\n",
    "X_val_np = X_val[numeric_columns].values.astype(np.float32)\n",
    "y_val_np = y_val.values.astype(np.int64)\n",
    "\n",
    "X_test_np = X_test[numeric_columns].values.astype(np.float32)\n",
    "y_test_np = y_test.values.astype(np.int64)\n",
    "\n",
    "print(f\"Training data shape: {X_train_np.shape}\")\n",
    "print(f\"Training data dtype: {X_train_np.dtype}\")\n",
    "print(f\"Training labels dtype: {y_train_np.dtype}\")\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\" - Save these NumPy arrays to disk for reproducibility and to reuse in other tools\")\n",
    "print(\" - Save preprocessing objects (scalers, encoders) so the same transforms are applied in production\")\n",
    "\n",
    "# Example saving (commented out so the notebook can run without writing files)\n",
    "# np.save('data/processed/X_train.npy', X_train_np)\n",
    "# np.save('data/processed/y_train.npy', y_train_np)\n",
    "# np.save('data/processed/X_val.npy', X_val_np)\n",
    "# np.save('data/processed/y_val.npy', y_val_np)\n",
    "# np.save('data/processed/X_test.npy', X_test_np)\n",
    "# np.save('data/processed/y_test.npy', y_test_np)\n",
    "\n",
    "# Save feature names and preprocessing info\n",
    "# import pickle\n",
    "# with open('data/processed/preprocessing_info.pkl', 'wb') as f:\n",
    "#     pickle.dump(preprocessing_summary, f)\n",
    "\n",
    "# Save scalers for future use\n",
    "# with open('data/processed/scaler.pkl', 'wb') as f:\n",
    "#     pickle.dump(scaler_standard, f)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting DataFrame to NumPy arrays (neutral):\n",
      "Training data shape: (600, 23)\n",
      "Training data dtype: float32\n",
      "Training labels dtype: int64\n",
      "\n",
      "Notes:\n",
      " - Save these NumPy arrays to disk for reproducibility and to reuse in other tools\n",
      " - Save preprocessing objects (scalers, encoders) so the same transforms are applied in production\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Errors and Debugging\n",
    "\n",
    "Before we wrap up, let's cover the most common mistakes beginners make with Pandas and how to fix them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T22:38:27.065898Z",
     "start_time": "2026-01-21T22:38:27.048858Z"
    }
   },
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸš¨ COMMON BEGINNER ERRORS AND SOLUTIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. KEYERROR - COLUMN NOT FOUND:\")\n",
    "print(\"   Error: 'KeyError: column_name'\")\n",
    "print(\"   Solution: Check column names with df.columns.tolist()\")\n",
    "print(\"   Tip: Column names are case-sensitive and include spaces!\")\n",
    "\n",
    "print(\"\\n2. INPLACE CONFUSION:\")\n",
    "print(\"   Problem: Changes not saved or unexpected None returned\")\n",
    "print(\"   Solution: Either use inplace=True OR assign result: df = df.fillna(0)\")\n",
    "print(\"   Remember: inplace=True modifies original, inplace=False returns new DataFrame\")\n",
    "\n",
    "print(\"\\n3. AXIS PARAMETER CONFUSION:\")\n",
    "print(\"   Problem: Operation applied to wrong dimension\")\n",
    "print(\"   Solution: axis=0 for rows (down), axis=1 for columns (across)\")\n",
    "print(\"   Memory trick: axis=0 affects row count, axis=1 affects column count\")\n",
    "\n",
    "print(\"\\n4. DTYPE ISSUES:\")\n",
    "print(\"   Problem: Unexpected behavior with mixed data types\")\n",
    "print(\"   Solution: Check dtypes with df.dtypes, convert with .astype()\")\n",
    "print(\"   Tip: Use pd.to_numeric() for safe numeric conversion\")\n",
    "\n",
    "print(\"\\n5. MISSING VALUE CONFUSION:\")\n",
    "print(\"   Problem: Operations fail due to NaN values\")\n",
    "print(\"   Solution: Always check for missing values with df.isnull().sum()\")\n",
    "print(\"   Tip: Handle missing values BEFORE other operations\")\n",
    "\n",
    "print(\"\\nðŸ’¡ DEBUGGING TIPS:\")\n",
    "print(\"â€¢ Start every analysis with df.info() and df.describe()\")\n",
    "print(\"â€¢ Use df.head() to see actual data, not just summaries\")\n",
    "print(\"â€¢ Check shapes after every major operation: print(df.shape)\")\n",
    "print(\"â€¢ Make copies before modifying: df_clean = df.copy()\")\n",
    "print(\"â€¢ Test operations on small samples first: df.head(10).operation()\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš¨ COMMON BEGINNER ERRORS AND SOLUTIONS\n",
      "============================================================\n",
      "\n",
      "1. KEYERROR - COLUMN NOT FOUND:\n",
      "   Error: 'KeyError: column_name'\n",
      "   Solution: Check column names with df.columns.tolist()\n",
      "   Tip: Column names are case-sensitive and include spaces!\n",
      "\n",
      "2. INPLACE CONFUSION:\n",
      "   Problem: Changes not saved or unexpected None returned\n",
      "   Solution: Either use inplace=True OR assign result: df = df.fillna(0)\n",
      "   Remember: inplace=True modifies original, inplace=False returns new DataFrame\n",
      "\n",
      "3. AXIS PARAMETER CONFUSION:\n",
      "   Problem: Operation applied to wrong dimension\n",
      "   Solution: axis=0 for rows (down), axis=1 for columns (across)\n",
      "   Memory trick: axis=0 affects row count, axis=1 affects column count\n",
      "\n",
      "4. DTYPE ISSUES:\n",
      "   Problem: Unexpected behavior with mixed data types\n",
      "   Solution: Check dtypes with df.dtypes, convert with .astype()\n",
      "   Tip: Use pd.to_numeric() for safe numeric conversion\n",
      "\n",
      "5. MISSING VALUE CONFUSION:\n",
      "   Problem: Operations fail due to NaN values\n",
      "   Solution: Always check for missing values with df.isnull().sum()\n",
      "   Tip: Handle missing values BEFORE other operations\n",
      "\n",
      "ðŸ’¡ DEBUGGING TIPS:\n",
      "â€¢ Start every analysis with df.info() and df.describe()\n",
      "â€¢ Use df.head() to see actual data, not just summaries\n",
      "â€¢ Check shapes after every major operation: print(df.shape)\n",
      "â€¢ Make copies before modifying: df_clean = df.copy()\n",
      "â€¢ Test operations on small samples first: df.head(10).operation()\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "**What we've learned (short):**\n",
    "- Explore data with .info(), .describe(), and .value_counts()\n",
    "- Handle missing values and outliers with straightforward strategies\n",
    "- Create new features and encode categorical variables\n",
    "- Scale numeric features and split data into train/val/test\n",
    "- Convert cleaned DataFrames to NumPy arrays for downstream tools\n",
    "\n",
    "**Connection to NumPy:** You've seen how Pandas builds on NumPy concepts:\n",
    "- DataFrames use NumPy arrays under the hood\n",
    "- Statistical operations mirror NumPy functions\n",
    "- Broadcasting works the same way\n",
    "- Final output is clean NumPy arrays ready for ML\n",
    "\n",
    "**Next Steps in Your ML Journey:**\n",
    "\n",
    "You're now ready for the **Scikit-learn Notebook** where you'll:\n",
    "\n",
    "1. **Use Your Clean Data**: Take the preprocessed datasets from this notebook and build actual ML models\n",
    "2. **Apply ML Algorithms**: See how classification, regression, and clustering work with real data\n",
    "3. **Evaluate Models**: Learn to measure and improve model performance\n",
    "4. **Complete ML Pipeline**: Combine data preprocessing (Pandas) + algorithms (scikit-learn) + numerical operations (NumPy)\n",
    "\n",
    "**What to Remember:**\n",
    "- Save your preprocessing steps (scalers, encoders) for future use\n",
    "- The data cleaning workflow you learned applies to every ML project\n",
    "- Clean data is the foundation of successful ML models\n",
    "\n",
    "**You're Ready!** You now have the data preprocessing skills that every ML engineer needs. Time to build some models! ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
